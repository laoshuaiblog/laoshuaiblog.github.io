<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【视觉语言大模型&#43;LLaVA1.0】大语言模型视觉助手（视觉指令调优）GPT4-Vision丐版 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/ac8b8ac1cb969aacbf27ddecdd2b4e80/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="【视觉语言大模型&#43;LLaVA1.0】大语言模型视觉助手（视觉指令调优）GPT4-Vision丐版">
  <meta property="og:description" content="官方资源汇总： 项目主页 || https://huggingface.co/liuhaotian
23.04.LLaVA1.论文: Large Language and Vision Assistant（Visual Instruction Tuning)
23.10 LLaVA-1.5论文: Improved Baselines with Visual Instruction Tuning
23.11 LLaVA-Plus项目：LLaVA-Plus: Large Language and Vision Assistants that Plug and Learn to Use Skills
24.01 LLaVA-1.6 博客(论文还未出): LLaVA-NeXT: Improved reasoning, OCR, and world knowledge
本地部署参考：https://blog.csdn.net/zhzxlcc/article/details/133773891
其他: 23.02.大语言模型LLaMA项目: Open and Efficient Foundation Language Models
相关博文： 【LLaVA所用的预训练大预言模型LLMs】23.03.Vicuna: 类似GPT4的开源聊天机器人（ 90%* ChatGPT Quality）
【通用baseline】23.10.LLaVA-1.5改善后视觉语言大模型
【医学图像】23.06 LLaVA-Med（医学图片视觉助手）: Training a Large Language-and-Vision Assistant for Biomedicine">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-21T15:26:18+08:00">
    <meta property="article:modified_time" content="2024-03-21T15:26:18+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【视觉语言大模型&#43;LLaVA1.0】大语言模型视觉助手（视觉指令调优）GPT4-Vision丐版</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><code>官方资源汇总</code>： <a href="https://llava-vl.github.io/" rel="nofollow">项目主页</a> || <a href="https://huggingface.co/liuhaotian" rel="nofollow">https://huggingface.co/liuhaotian</a><br> 23.04.LLaVA1.论文: <a href="https://arxiv.org/abs/2304.08485" rel="nofollow">Large Language and Vision Assistant（Visual Instruction Tuning)</a><br> 23.10 LLaVA-1.5论文: <a href="https://arxiv.org/pdf/2310.03744.pdf" rel="nofollow">Improved Baselines with Visual Instruction Tuning</a><br> 23.11 LLaVA-Plus项目：<a href="https://llava-vl.github.io/llava-plus/" rel="nofollow">LLaVA-Plus: Large Language and Vision Assistants that Plug and Learn to Use Skills</a><br> 24.01 LLaVA-1.6 博客(论文还未出): <a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/" rel="nofollow">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</a><br> <code>本地部署</code>参考：<a href="https://blog.csdn.net/zhzxlcc/article/details/133773891">https://blog.csdn.net/zhzxlcc/article/details/133773891</a><br> 其他: 23.02.大语言模型LLaMA项目: <a href="https://github.com/facebookresearch/llama">Open and Efficient Foundation Language Models</a></p> 
<h3><a id="_8"></a><code>相关博文</code>：</h3> 
<blockquote> 
 <p>【LLaVA所用的预训练大预言模型LLMs】<a href="http://t.csdnimg.cn/an2zH" rel="nofollow">23.03.Vicuna: 类似GPT4的开源聊天机器人（ 90%* ChatGPT Quality）</a><br> 【通用baseline】<a href="https://blog.csdn.net/imwaters/article/details/136896668">23.10.LLaVA-1.5改善后视觉语言大模型</a><br> 【医学图像】<a href="https://blog.csdn.net/imwaters/article/details/136907312">23.06 LLaVA-Med（医学图片视觉助手）: Training a Large Language-and-Vision Assistant for Biomedicine</a></p> 
</blockquote> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#_8" rel="nofollow">`相关博文`：</a></li></ul> 
  </li><li><a href="#_16" rel="nofollow">一、简介</a></li><li><ul><li><a href="#10_LLaVA__17" rel="nofollow">1.0 为什么要学习LLaVA？ （开源+性能好+多模态+持续更新）</a></li><li><a href="#11__22" rel="nofollow">1.1 基本术语</a></li><li><ul><li><a href="#111_LLaVA____23" rel="nofollow">1.1.1 什么是LLaVA ? （大语言模型视觉助手）</a></li><li><a href="#112___32" rel="nofollow">1.1.2 语言视觉特征对齐？ （将图片视觉特征映射到文本特征层）</a></li></ul> 
   </li><li><a href="#12_GPT4Vision_37" rel="nofollow">*1.2 能干什么？（实现部分-GPT4Vision的能力）</a></li><li><ul><li><a href="#121__html__js_2_38" rel="nofollow">1.2.1 `草图生成网页`相关的 html / js代码 （原图2）（同时能识别字符的能力）</a></li><li><a href="#122__44" rel="nofollow">1.2.2 无需提示内容，能够识别用户意图（根据冰箱内图片提供菜谱）</a></li><li><a href="#123__4_60" rel="nofollow">1.2.3 继承大语言模型的知识 （图4）</a></li><li><a href="#124__5_62" rel="nofollow">1.2.4 能识别名画 (原图5)</a></li><li><a href="#125LLaVa_CLIP_6_67" rel="nofollow">1.2.5-LLaVa 本身没有训练马斯克的图片，继承了CLIP的先验学习过 （原图6）</a></li><li><a href="#126___72" rel="nofollow">1.2.6 图片中字符的识别与读取能力</a></li></ul> 
   </li><li><a href="#13__LLaVA10____2304_Visual_Instruction_Tuning_78" rel="nofollow">1.3 LLaVA1.0 原文摘要 : 23.04 Visual Instruction Tuning</a></li><li><ul><li><a href="#131__86" rel="nofollow">1.3.1 **贡献**：</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_95" rel="nofollow">二、如何`训练`？</a></li><li><ul><li><a href="#20____99" rel="nofollow">2.0 数据集</a></li><li><ul><li><a href="#201__102" rel="nofollow">2.0.1 指令跟踪数据的一个示例</a></li></ul> 
   </li><li><a href="#21___109" rel="nofollow">2.1 对齐图像和语言特征 （将图片特征映射到语言模型的特征层）</a></li><li><ul><li><a href="#211___111" rel="nofollow">2.1.1 数据集相关</a></li><li><a href="#212__114" rel="nofollow">2.1.2 训练基础模型+时间</a></li><li><a href="#213__121" rel="nofollow">2.1.3 训练脚本</a></li></ul> 
   </li><li><a href="#22__Visual_Instruction_Tuning_174" rel="nofollow">2.2 视觉指令微调 （Visual Instruction Tuning）</a></li><li><ul><li><a href="#221___LLaVAInstruct150KhttpshuggingfacecodatasetsliuhaotianLLaVAInstruct150Ktreemain____COCO_train2017_imageshttpscocodatasetorgdownload__176" rel="nofollow">2.2.1 准备数据集 （[LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/tree/main) + [COCO train2017 images](https://cocodataset.org/#download) ）</a></li><li><a href="#222_3epoch_schedule_on_the_LLaVAInstruct158K_datasehttpsgithubcomhaotianliuLLaVAblobmainscriptsfinetune_full_schedulesh_180" rel="nofollow">2.2.2 原始的训练脚本：[3-epoch schedule on the LLaVA-Instruct-158K datase](https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_full_schedule.sh)</a></li></ul> 
  </li></ul> 
  </li><li><a href="#_linuxGPU13G__222" rel="nofollow">三、本地部署 （linux服务器，未压缩时GPU约13G显存 ）</a></li><li><ul><li><a href="#31__cuda_228" rel="nofollow">3.1 环境相关 （需要安装好cuda驱动）</a></li><li><ul><li><a href="#311___pip_238" rel="nofollow">3.1.1 pip安装其他依赖</a></li></ul> 
   </li><li><a href="#32__273" rel="nofollow">3.2 下载模型</a></li><li><ul><li><a href="#321_llavav15_282" rel="nofollow">3.2.1下载 llavav1.5模型</a></li><li><a href="#322__289" rel="nofollow">3.2.2 下载视觉编码器</a></li></ul> 
   </li><li><a href="#33___ui_294" rel="nofollow">3.3 运行 服务后台端、模型端、ui界面</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_16"></a>一、简介</h2> 
<h3><a id="10_LLaVA__17"></a>1.0 为什么要学习LLaVA？ （开源+性能好+多模态+持续更新）</h3> 
<p>它对标的是 <strong>GPT4-Vison</strong>模型，使聊天助手，具备了<strong>解析图片</strong>的能力<br> 生态好，一直在更新！<br> 开源了模型、数据、论文也写的贼好！ <strong><a href="https://blog.csdn.net/imwaters/article/details/136370035">图from 多模态综述</a></strong><br> <img src="https://images2.imgbox.com/f3/3c/ZtapgZFq_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="11__22"></a>1.1 基本术语</h3> 
<h4><a id="111_LLaVA____23"></a>1.1.1 什么是LLaVA ? （大语言模型视觉助手）</h4> 
<p>LLaVA: <code>L</code>arge <code>L</code>anguage <code>a</code>nd <code>V</code>ision <code>A</code>ssistant</p> 
<p>一个端到端训练的大型多模态模型，将<strong>视觉编码器</strong>(vision encoder)和<strong>LLM</strong>(large language model ,大语言模型)连接起来，<br> 用于<strong>通用的视觉和语言理解</strong> （general-purpose visual and language understanding）<br> 就是将图片的视觉特征，映射到了预训练的LLMs中embdding向量，通过2轮的训练，实现对图片理解，并继承大模型能力。<br> <img src="https://images2.imgbox.com/0d/22/JzyhFrLK_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="112___32"></a>1.1.2 语言视觉特征对齐？ （将图片视觉特征映射到文本特征层）</h4> 
<p>特征对齐是一种在多模态模型中训练<strong>视觉和语言编码器</strong>以便在共享空间中表示图像和文本的方法。<br> 在视觉指令调整（Visual Instruction Tuning）中，特征对齐是通过预训练的<code>视觉编码器</code>（如CLIP的ViT-L/14）提取图像特征，并使用<code>简单的线性层</code>将这些特征连接到语言模型的<strong>词嵌入空间</strong>来实现的。<br> 这样，图像特征可以与预训练语言模型（LLM）的词嵌入对齐。这个阶段可以理解为训练一个与冻结LLM兼容的视觉分词器。</p> 
<h3><a id="12_GPT4Vision_37"></a>*1.2 能干什么？（实现部分-GPT4Vision的能力）</h3> 
<h4><a id="121__html__js_2_38"></a>1.2.1 <code>草图生成网页</code>相关的 html / js代码 （原图2）（同时能识别字符的能力）</h4> 
<blockquote> 
 <p>Turn this mock-up into an interactive website using html/js, replacethe joke with a real joke.<br> 将此模型转换为使用 html/js 的交互网站，用真正的笑话替换文本joke (点击后)。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/1e/a9/7KOdU3ui_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="122__44"></a>1.2.2 无需提示内容，能够识别用户意图（根据冰箱内图片提供菜谱）</h4> 
<p>提问：图片+文字：我能用这些做什么饭？<br> <img src="https://images2.imgbox.com/cf/b7/ZhATmdIl_o.png" alt="在这里插入图片描述"><br> 回应原文</p> 
<blockquote> 
 <p>Certainly! Here’s a simple fruit salad recipe using the items in your refrigerator:<br> Ingredients:</p> 
 <ul><li>2 cups fresh strawberries, sliced</li><li>1 cup fresh blueberries</li><li>1 large carrot, grated</li><li>Juice of half a lemon</li><li>A handful of chopped fresh parsley or mint- A few ice cubes</li><li>Optional: honey or maple syrup (adjust to taste)</li></ul> 
 <p>Instructions:1. In a large bowl, combine the sliced strawberries, blueberries, and carrot. Gently toss them together to mix.2. Drizzle the juice of half a lemon over the fruit and vegetables to prevent them from oxidizing and add a refreshing citrus flavor.3. Sprinkle the chopped fresh parsley or mint over the fruit and gently toss once more.4. If desired, add a drizzle of honey or maple syrup for a touch of sweetness. Adjust the amount according to your taste preferences.5. Cover the bowl and refrigerate the fruit salad for at least 30 minutes or up to a few hours. This allows the flavors to meld and creates amore refreshing salad.6. Serve the fruit salad in cups or bowls, garnished with a sprinkle of fresh parsley or mint, if desired.This fruit salad recipe is perfect for a healthy snack or side dish, and with the variety of ingredients available in your refrigerator, it should beeasy to put together quickly.</p> 
</blockquote> 
<h4><a id="123__4_60"></a>1.2.3 继承大语言模型的知识 （图4）</h4> 
<p><img src="https://images2.imgbox.com/e6/03/yUk85BDd_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="124__5_62"></a>1.2.4 能识别名画 (原图5)</h4> 
<p><img src="https://images2.imgbox.com/ea/ed/Ax0BuMgs_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="125LLaVa_CLIP_6_67"></a>1.2.5-LLaVa 本身没有训练马斯克的图片，继承了CLIP的先验学习过 （原图6）</h4> 
<blockquote> 
 <p>An interesting emergent behavior of LLaVA is its ability to recognize Elon Musk both in a headshot and in a humorous meme where he is dressed as a doge. This implies that the pre-trained CLIP vision encoder may have seen images of Elon Musk. However, it is still surprising because Elon Musk <code>never appears</code> in the <strong>training data</strong> for either the <strong>visual feature alignment</strong> or <strong>visual instruction tuning</strong> stages of LLaVA, which indicates that the base language model generalizes to <code>unseen visual concepts</code> (识别没有对齐过的图像).</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/dd/8e/BsD8UYfH_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="126___72"></a>1.2.6 图片中字符的识别与读取能力</h4> 
<p>输入图片<br> <img src="https://images2.imgbox.com/1f/42/ZLzKaxBP_o.png" alt="在这里插入图片描述"><br> 原文是英语（翻译为中文了）<br> <img src="https://images2.imgbox.com/b8/06/EbaMdTLC_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="13__LLaVA10____2304_Visual_Instruction_Tuning_78"></a>1.3 LLaVA1.0 原文摘要 : 23.04 Visual Instruction Tuning</h3> 
<p>论文: <a href="https://arxiv.org/abs/2304.08485" rel="nofollow">Large Language and Vision Assistant（Visual Instruction Tuning)</a></p> 
<p>具有<strong>多模态聊天能力</strong>（multimodal chat abilities），有时在看不见的图像/指令上表现出多模态<code>GPT-4类似能力</code>，与GPT-4在合成的多模态指令遵循数据集上相比，产生了85.1%的相对分数。<br> 当在Science QA上进行微调时，LLaVA和GPT-4的协同作用（synergy）达到了92.53%的最新水平.</p> 
<p>在1.0中，我们提出了<code>**视觉指令调整**</code>（visual instruction-tuning），这是将指令微调（instruction-tuning）扩展到语言-图像多模态空间的首次尝试，为构建通用视觉助手铺平了道路。<br> 做出了以下</p> 
<h4><a id="131__86"></a>1.3.1 <strong>贡献</strong>：</h4> 
<ol><li><strong>多模态遵循指令数据</strong>（Multimodal <strong>instruction-following</strong> data）。一项关键挑战是缺乏视觉语言指令跟踪数据。我们提出了一种数据重组视角和方法（pipeline），使用 ChatGPT/GPT-4, <strong><code>将图像文本对转换为适当的指令遵循格式</code></strong> (instruction-following format)。</li><li>大型多模态模型（Large multimodal models）。我们通过将 CLIP 的开放集视觉编码器与语言解码器Vicuna 连接起来，开发了一个大型多模态模型（LMM），并对我们生成的教学视觉语言数据进行端到端微调。我们的实证研究验证了使用生成的数据进行 LMM 指令调整的有效性，并提出了构建通用指令跟踪视觉代理的实用技巧。当与 GPT-4 集成时，我们的方法在 Science QA [34] 多模态推理数据集上实现了 SoTA。</li><li>多模式指令遵循基准。我们向 LLaVA-Bench 提供了两个具有挑战性的基准，以及多种配对图像、说明和详细的标注。</li><li><strong>开源</strong>。我们向公众发布以下资源：生成的多模式指令数据、代码库、可视化聊天的示例交互程序。</li></ol> 
<h2><a id="_95"></a>二、如何<code>训练</code>？</h2> 
<p>训练文档：<a href="https://github.com/haotian-liu/LLaVA/tree/v1.0.1?tab=readme-ov-file#train">https://github.com/haotian-liu/LLaVA/tree/v1.0.1?tab=readme-ov-file#train</a><br> 二阶段训练 （Feature Alignment + 视觉微调）</p> 
<h3><a id="20____99"></a>2.0 数据集</h3> 
<p>** GPT-4 扩展**<br> 输入收集到的图片简单本文（caption）,以及图片中主体的位置（box），输入到纯文本的gpt4模型中（不输人图片）</p> 
<h4><a id="201__102"></a>2.0.1 指令跟踪数据的一个示例</h4> 
<p>顶部块显示了用于提示 GPT 的字幕和框等上下文，<br> 底部块显示了三种类型的响应。<br> 请注意，视觉图像不用于提示 GPT，我们仅将其此处显示为参考。<br> <img src="https://images2.imgbox.com/03/54/oAko4ll9_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>Table 1: One example to illustrate the instruction-following data. The top block shows the contexts such as captions and boxes used to prompt GPT, and the bottom block shows the three types of responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.</p> 
</blockquote> 
<h3><a id="21___109"></a>2.1 对齐图像和语言特征 （将图片特征映射到语言模型的特征层）</h3> 
<h4><a id="211___111"></a>2.1.1 数据集相关</h4> 
<p>过滤 **<code>CC3M</code>**数据集选出 <strong>595K</strong> image-text 对 ： <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K" rel="nofollow">下载地址-<strong>LLaVA-CC3M-Pretrain-595K</strong></a></p> 
<h4><a id="212__114"></a>2.1.2 训练基础模型+时间</h4> 
<p>在基础大语言模型（LLMs） Vicuna 上进行训练，推荐使用<a href="https://github.com/haotian-liu/LLaVA/blob/main/scripts/pretrain.sh">DeepSpeed</a>框架训练</p> 
<blockquote> 
 <p><strong>在8x A100（80G）上，LLAVA-13B大约需要4个小时。 7B检查点大约需要2个小时</strong><br> 1x A100 (80G): LLaVA-13B,. Time: ~33 hours. (也可以)</p> 
</blockquote> 
<h4><a id="213__121"></a>2.1.3 训练脚本</h4> 
<p><a href="https://github.com/haotian-liu/LLaVA/blob/main/scripts/pretrain.sh">https://github.com/haotian-liu/LLaVA/blob/main/scripts/pretrain.sh</a></p> 
<pre><code class="prism language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token comment"># IMPORTANT: this is the training script for the original LLaVA, NOT FOR LLaVA V1.5!</span>

<span class="token comment"># Uncomment and set the following variables correspondingly to run this script:</span>

<span class="token comment"># MODEL_VERSION=vicuna-v1-3-7b</span>
<span class="token comment"># MODEL_VERSION=llama-2-7b-chat</span>

<span class="token comment">########### DO NOT CHANGE ###########</span>
<span class="token comment">########### USE THIS FOR BOTH ###########</span>
<span class="token assign-left variable">PROMPT_VERSION</span><span class="token operator">=</span>plain
<span class="token comment">########### DO NOT CHANGE ###########</span>

deepspeed llava/train/train_mem.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--deepspeed</span> ./scripts/zero2.json <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> ./checkpoints/<span class="token variable">$MODEL_VERSION</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--version</span> <span class="token variable">$PROMPT_VERSION</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--data_path</span> /path/to/pretrain_data.json <span class="token punctuation">\</span>
    <span class="token parameter variable">--image_folder</span> /path/to/images <span class="token punctuation">\</span>
    <span class="token parameter variable">--vision_tower</span> openai/clip-vit-large-patch14 <span class="token punctuation">\</span>
    <span class="token parameter variable">--tune_mm_mlp_adapter</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--mm_vision_select_layer</span> <span class="token parameter variable">-2</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mm_use_im_start_end</span> False <span class="token punctuation">\</span>
    <span class="token parameter variable">--mm_use_im_patch_token</span> False <span class="token punctuation">\</span>
    <span class="token parameter variable">--bf16</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_dir</span> ./checkpoints/llava-<span class="token variable">$MODEL_VERSION</span>-pretrain <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_train_epochs</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">16</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--per_device_eval_batch_size</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--evaluation_strategy</span> <span class="token string">"no"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_strategy</span> <span class="token string">"steps"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_steps</span> <span class="token number">24000</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_total_limit</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> 2e-3 <span class="token punctuation">\</span>
    <span class="token parameter variable">--weight_decay</span> <span class="token number">0</span>. <span class="token punctuation">\</span>
    <span class="token parameter variable">--warmup_ratio</span> <span class="token number">0.03</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--lr_scheduler_type</span> <span class="token string">"cosine"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--logging_steps</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--tf32</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_max_length</span> <span class="token number">2048</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--gradient_checkpointing</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--dataloader_num_workers</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--lazy_preprocess</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--report_to</span> wandb
</code></pre> 
<h3><a id="22__Visual_Instruction_Tuning_174"></a>2.2 视觉指令微调 （Visual Instruction Tuning）</h3> 
<p>总是保持视觉编码器的权重冻结，并继续更新LLaVA中投影层(特征对齐)和LLM的预训练模型权重;即(3)中的可训练参数为θ = {W, φ}。我们考虑两种特定的用例场景：</p> 
<h4><a id="221___LLaVAInstruct150KhttpshuggingfacecodatasetsliuhaotianLLaVAInstruct150Ktreemain____COCO_train2017_imageshttpscocodatasetorgdownload__176"></a>2.2.1 准备数据集 （<a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/tree/main" rel="nofollow">LLaVA-Instruct-150K</a> + <a href="https://cocodataset.org/#download" rel="nofollow">COCO train2017 images</a> ）</h4> 
<p>总共收集了 158K 个<strong>遵循指令的数据</strong>（instruction-following data）语言-图像：<br> 包括关于图像内容的对话中的 58K、详细描述 23K 复杂推理 77k。</p> 
<h4><a id="222_3epoch_schedule_on_the_LLaVAInstruct158K_datasehttpsgithubcomhaotianliuLLaVAblobmainscriptsfinetune_full_schedulesh_180"></a>2.2.2 原始的训练脚本：<a href="https://github.com/haotian-liu/LLaVA/blob/main/scripts/finetune_full_schedule.sh">3-epoch schedule on the LLaVA-Instruct-158K datase</a></h4> 
<pre><code class="prism language-bash">deepspeed llava/train/train_mem.py <span class="token punctuation">\</span>
    <span class="token parameter variable">--deepspeed</span> ./scripts/zero2.json <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_name_or_path</span> ./checkpoints/<span class="token variable">$MODEL_VERSION</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--version</span> <span class="token variable">$PROMPT_VERSION</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--data_path</span> ./playground/data/llava_instruct_158k.json <span class="token punctuation">\</span>
    <span class="token parameter variable">--image_folder</span> /path/to/coco/train2017 <span class="token punctuation">\</span>
    <span class="token parameter variable">--vision_tower</span> openai/clip-vit-large-patch14 <span class="token punctuation">\</span>
    <span class="token parameter variable">--pretrain_mm_mlp_adapter</span> ./checkpoints/llava-<span class="token variable">$MODEL_VERSION</span>-pretrain/mm_projector.bin <span class="token punctuation">\</span>
    <span class="token parameter variable">--mm_vision_select_layer</span> <span class="token parameter variable">-2</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--mm_use_im_start_end</span> False <span class="token punctuation">\</span>
    <span class="token parameter variable">--mm_use_im_patch_token</span> False <span class="token punctuation">\</span>
    <span class="token parameter variable">--bf16</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--output_dir</span> ./checkpoints/llava-<span class="token variable">$MODEL_VERSION</span>-finetune <span class="token punctuation">\</span>
    <span class="token parameter variable">--num_train_epochs</span> <span class="token number">3</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">16</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--per_device_eval_batch_size</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--evaluation_strategy</span> <span class="token string">"no"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_strategy</span> <span class="token string">"steps"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_steps</span> <span class="token number">50000</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--save_total_limit</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--learning_rate</span> 2e-5 <span class="token punctuation">\</span>
    <span class="token parameter variable">--weight_decay</span> <span class="token number">0</span>. <span class="token punctuation">\</span>
    <span class="token parameter variable">--warmup_ratio</span> <span class="token number">0.03</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--lr_scheduler_type</span> <span class="token string">"cosine"</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--logging_steps</span> <span class="token number">1</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--tf32</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--model_max_length</span> <span class="token number">2048</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--gradient_checkpointing</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--dataloader_num_workers</span> <span class="token number">4</span> <span class="token punctuation">\</span>
    <span class="token parameter variable">--lazy_preprocess</span> True <span class="token punctuation">\</span>
    <span class="token parameter variable">--report_to</span> wandb
</code></pre> 
<h2><a id="_linuxGPU13G__222"></a>三、本地部署 （linux服务器，未压缩时GPU约13G显存 ）</h2> 
<p>官网参考：<a href="https://github.com/haotian-liu/LLaVA">https://github.com/haotian-liu/LLaVA</a><br> 可参考：<a href="https://blog.csdn.net/zhzxlcc/article/details/133773891">https://blog.csdn.net/zhzxlcc/article/details/133773891</a></p> 
<p><img src="https://images2.imgbox.com/b0/b2/uTxssq4L_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="31__cuda_228"></a>3.1 环境相关 （需要安装好cuda驱动）</h3> 
<p>根据自己显卡驱动，在pytorch官方选择对应的版本: <a href="https://pytorch.org/get-started/previous-versions/" rel="nofollow">https://pytorch.org/get-started/previous-versions/</a></p> 
<pre><code class="prism language-bash">conda create <span class="token parameter variable">-n</span>  llava <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.10</span>  <span class="token parameter variable">-y</span>

conda activate llava 
conda <span class="token function">install</span> <span class="token assign-left variable">pytorch</span><span class="token operator">==</span><span class="token number">2.1</span>.2 <span class="token assign-left variable">torchvision</span><span class="token operator">==</span><span class="token number">0.16</span>.2 <span class="token assign-left variable">torchaudio</span><span class="token operator">==</span><span class="token number">2.1</span>.2 pytorch-cuda<span class="token operator">=</span><span class="token number">11.8</span> <span class="token parameter variable">-c</span> pytorch <span class="token parameter variable">-c</span> nvidia

</code></pre> 
<h4><a id="311___pip_238"></a>3.1.1 pip安装其他依赖</h4> 
<p>根据 LLaVA/pyproject.toml 建立其他依赖环境</p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token parameter variable">-r</span> install.txt
</code></pre> 
<p><strong>install.txt的内容</strong> （自己新建）</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">torch</span><span class="token operator">==</span><span class="token number">2.1</span>.2
<span class="token assign-left variable">torchvision</span><span class="token operator">==</span><span class="token number">0.16</span>.2
<span class="token assign-left variable">transformers</span><span class="token operator">==</span><span class="token number">4.37</span>.2
<span class="token assign-left variable">tokenizers</span><span class="token operator">==</span><span class="token number">0.15</span>.1
<span class="token assign-left variable">sentencepiece</span><span class="token operator">==</span><span class="token number">0.1</span>.99
shortuuid
<span class="token assign-left variable">accelerate</span><span class="token operator">==</span><span class="token number">0.21</span>.0
peft
bitsandbytes
pydantic
markdown2
numpy
scikit-learn<span class="token operator">==</span><span class="token number">1.2</span>.2
<span class="token assign-left variable">gradio</span><span class="token operator">==</span><span class="token number">4.16</span>.0
<span class="token assign-left variable">gradio_client</span><span class="token operator">==</span><span class="token number">0.8</span>.1
requests
<span class="token assign-left variable">httpx</span><span class="token operator">==</span><span class="token number">0.24</span>.0
uvicorn
fastapi
<span class="token assign-left variable">einops</span><span class="token operator">==</span><span class="token number">0.6</span>.1
einops-exts<span class="token operator">==</span><span class="token number">0.0</span>.4
<span class="token assign-left variable">timm</span><span class="token operator">==</span><span class="token number">0.6</span>.13
<span class="token assign-left variable">deepspeed</span><span class="token operator">==</span><span class="token number">0.12</span>.6
ninja
wandb
</code></pre> 
<h3><a id="32__273"></a>3.2 下载模型</h3> 
<p>配置官方下载工具包</p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token parameter variable">-U</span> huggingface_hub  hf-transfer
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com  <span class="token comment"># linux</span>
<span class="token comment"># $env:HF_ENDPOINT = "https://hf-mirror.com"  # windows</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_HUB_ENABLE_HF_TRANSFER</span><span class="token operator">=</span><span class="token number">1</span>  <span class="token comment"># 官方加速报错，就关了</span>

</code></pre> 
<h4><a id="321_llavav15_282"></a>3.2.1下载 llavav1.5模型</h4> 
<pre><code class="prism language-bash">huggingface-cli download --resume-download liuhaotian/llava-v1.5-7b  --local-dir  ./weights/llava-v1.5-7b

huggingface-cli download --resume-download liuhaotian/llava-v1.5-13b  --local-dir ./weights/llava-v1.5-13b
</code></pre> 
<h4><a id="322__289"></a>3.2.2 下载视觉编码器</h4> 
<pre><code class="prism language-bash">huggingface-cli download --resume-download openai/clip-vit-large-patch14-336  --local-dir ./models/clip-vit-large-patch14-336  
</code></pre> 
<h3><a id="33___ui_294"></a>3.3 运行 服务后台端、模型端、ui界面</h3> 
<p>其中 CUDA_VISIBLE_DEVICES=0 表示选择哪个显卡进行运行，值可为0，1,2</p> 
<pre><code class="prism language-bash"><span class="token comment"># service</span>
conda activate llava
<span class="token builtin class-name">cd</span> ~/code/LLaVA  <span class="token comment"># 你的llava项目位置</span>
<span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span>  python <span class="token parameter variable">-m</span> llava.serve.controller <span class="token parameter variable">--host</span> <span class="token number">0.0</span>.0.0 <span class="token parameter variable">--port</span> <span class="token number">10000</span>

<span class="token comment"># model</span>
<span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python <span class="token parameter variable">-m</span> llava.serve.model_worker <span class="token parameter variable">--host</span> <span class="token number">0.0</span>.0.0 <span class="token parameter variable">--controller</span> http://localhost:10000 <span class="token parameter variable">--port</span> <span class="token number">2006</span> <span class="token parameter variable">--worker</span> http://localhost:2006 --model-path /data/zengxingyu/code/LLaVA/weights/llava-v1.5-7b/
<span class="token comment"># </span>
<span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0</span> python <span class="token parameter variable">-m</span> llava.serve.gradio_web_server <span class="token parameter variable">--controller</span> http://localhost:10000 --model-list-mode reload
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a9e94a77e314fb0631fa440663872f81/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C&#43;&#43; list详解及模拟实现</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/023028aa0a1a7b016bfa02811470731c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">package.json详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>