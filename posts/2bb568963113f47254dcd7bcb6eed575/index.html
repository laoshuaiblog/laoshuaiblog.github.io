<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Convolution&#43;BatchNorm&#43;Scale模块参数设置 &amp;  融合BatchNorm&#43;Scale层到Convolution层 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/2bb568963113f47254dcd7bcb6eed575/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="Convolution&#43;BatchNorm&#43;Scale模块参数设置 &amp;  融合BatchNorm&#43;Scale层到Convolution层">
  <meta property="og:description" content="Convolution&#43;BatchNorm&#43;Scale&#43;Relu的组合模块在卷积后进行归一化，然后在放出非线性Relu层，可以加速训练收敛。但在推理时BatchNorm非常耗时，可以将训练时学习到的BatchNorm&#43;Scale的线性变换参数融合到卷积层，替换原来的Convolution层中weight和bias，实现在不影响准确度的前提下加速预测时间。
一、Convolution&#43;BatchNorm&#43;Scale层在caffe中参数设置示例： layer { name: &#34;Conv1&#34; type: &#34;Convolution&#34; bottom: &#34;Conv1&#34; top: &#34;Conv2&#34; convolution_param { num_output: 64 kernel_h:1 kernel_w:3 pad_h: 0 pad_w: 1 stride: 1 weight_filler { type: &#34;msra&#34; } bias_term: false } } layer { name: &#34;Conv2/bn&#34; type: &#34;BatchNorm&#34; bottom: &#34;Conv2&#34; top: &#34;Conv2&#34; batch_norm_param { use_global_stats: false eps:1e-03 } param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } include { phase: TRAIN } } layer { name: &#34;">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2018-08-28T13:20:26+08:00">
    <meta property="article:modified_time" content="2018-08-28T13:20:26+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Convolution&#43;BatchNorm&#43;Scale模块参数设置 &amp;  融合BatchNorm&#43;Scale层到Convolution层</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>Convolution+BatchNorm+Scale+Relu的组合模块在卷积后进行归一化，然后在放出非线性Relu层，可以加速训练收敛。但在推理时BatchNorm非常耗时，可以将训练时学习到的BatchNorm+Scale的线性变换参数融合到卷积层，替换原来的Convolution层中weight和bias，实现在不影响准确度的前提下加速预测时间。</p> 
<h2>一、Convolution+BatchNorm+Scale层在caffe中参数设置示例：</h2> 
<pre class="has"><code>layer {
  name:  "Conv1"
  type: "Convolution"
  bottom: "Conv1"
  top:  "Conv2"
  convolution_param {
    num_output: 64
    kernel_h:1
    kernel_w:3
    pad_h: 0
    pad_w: 1
    stride: 1
    weight_filler {
        type: "msra"
      }
    bias_term: false
  }
}

layer {
  name: "Conv2/bn"
  type: "BatchNorm"
  bottom:  "Conv2"
  top:  "Conv2"
  batch_norm_param {
    use_global_stats: false
	  eps:1e-03
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TRAIN
  }
}

layer {
  name: "Conv2/bn"
  type: "BatchNorm"
  bottom:  "Conv2"
  top:  "Conv2"
  batch_norm_param {
    use_global_stats: true
	  eps:1e-03
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  include {
    phase: TEST
  }
}

layer {
  name: "Conv2/bn/scale"
  type: "Scale"
  bottom:  "Conv2"
  top:  "Conv2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}</code></pre> 
<h2>二、解释说明：</h2> 
<h3>1、Convolution层的设置：</h3> 
<ul><li>权重: 强烈建议手动选择weight_filler初始化方式！对于Convolution+BatchNorm+Scale的组合模块，建议使用msra模式初始化。 (msra: short for  Microsoft Research Asia or He-initialization, ref:Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification)。其他情况xavier初始化比较常用, xavier初始化可以根据输入和输出神经元的个数，自动地决定初始化值的大小。caffe中默认的weight_filler是constant, value:0, 这种默认的初始化方式得到的结果一般不好，我就遇到过使用了默认的weight_filler后导致图像语义分割错误地得到同色图片的情况。</li><li>因为BatchNorm层会减去平均值，所以convolution层的bias_term设置并没有意义。bias_term默认值为true, 默认的bias_filler也是constant, value:0。所以手动设置bias_term: false。</li><li>学习率和权重衰减系数，因为只有weight，没有bias，使用如下默认值（无需手动设置） </li></ul> 
<pre class="has"><code>param {
    lr_mult: 1
    decay_mult: 1
  }</code></pre> 
<h4>2、BatchNorm层设置：</h4> 
<p>BatchNorm是进行归一化，计算</p> 
<p>                                                                    <img alt="\frac{x-mean}{\sqrt{var+\varepsilon }}" class="mathcode" src="https://images2.imgbox.com/db/a2/pt18CqTw_o.gif">                                                （1）</p> 
<ul><li>use_global_stats在训练时设为false，神经网络只针对每一个batch的数据进行归一化操作，并每一步都利用滑移平均计算更新全局的mean和variance，但是并没有使用这两个全局的统计量；在测试时use_global_stats设为true，直接使用训练时计算好的全局mean和variance进行归一化；</li><li>eps是在分母的var项上加一个小值，防止方差为0时出错；</li><li>moving_average_fraction默认值为0.999，值越小表示较新迭代计算的mean和var权重更大，之前的迭代影响越小；</li><li> <p>BatchNorm的三个参数mean，variance，moving_average_fraction。其中mean，variance是由输入数据计算直接计算得到的，moving_average_fraction是指定的，因此都与学习率和衰减率无关，设定</p> </li></ul> 
<pre class="has"><code>param {
lr_mult: 0
decay_mult: 0
}
param {
lr_mult: 0
decay_mult: 0
}
param {
lr_mult: 0
decay_mult: 0
}</code></pre> 
<p>参考资料：</p> 
<p><a href="https://github.com/hujie-frank/SENet/issues/26">https://github.com/hujie-frank/SENet/issues/26</a> </p> 
<p><a href="https://gist.github.com/ducha-aiki/c0d1325f0cebe0b05c36" rel="nofollow">https://gist.github.com/ducha-aiki/c0d1325f0cebe0b05c36</a></p> 
<p> </p> 
<h4>3、Scale层设置：</h4> 
<p>Scale是将BatchNorm得到的数据做线性变换：</p> 
<p>                                                                   <img alt="\gamma (\frac{x-mean}{\sqrt{var+\varepsilon }})+\beta" class="mathcode" src="https://images2.imgbox.com/06/8c/QRsJt2Yj_o.gif">                               （2）</p> 
<p>bias_term在这里打开！学习率系数设定为1即可，无需decay_mult。</p> 
<pre class="has"><code>param {
     lr_mult: 1
     decay_mult: 0
}
param {
     lr_mult: 1
     decay_mult: 0
}
scale_param {
     bias_term: true
}</code></pre> 
<h2>三、融合BatchNorm+Scale层到卷积层进行推理加速</h2> 
<p>在推理时BatchNorm非常耗时，可以将训练时学习到的BatchNorm+Scale的线性变换参数融合到卷积层。具体方法为：</p> 
<p>1、识别prototxt文件中的Batch和Scale层，直接删除（<strong>这要求BatchNorm和Scale层中top和bottom名保持一致，才能保证删除BatchNorm和Scale层后数据仍然能够正确传输，如上文中示例所用的方式</strong>）</p> 
<p>2、修改caffemodel的weight，将convolution的bias_term修改为true，并用Convolution+BatchNorm+Scale的等效weight和bias去替换原来的weight和新打开的bias_term。convolution输出值为wx+b,作为BatchNorm层的输入，带入公式（1）（2）中，得到公式如下：</p> 
<p>                                                   <img alt="W_{new}=\frac{W_{old}\cdot \gamma }{\sqrt{var+\epsilon} }" class="mathcode" src="https://images2.imgbox.com/b9/c2/vycsccyM_o.gif">                                             （3）</p> 
<p>                                                   <img alt="b_{new}=\frac{b_{old}\cdot \gamma }{\sqrt{var+\epsilon} }+\frac{mean\cdot \gamma }{\sqrt{var+\epsilon} }+\beta" class="mathcode" src="https://images2.imgbox.com/1b/2a/jucZDEPH_o.gif">                  （4）</p> 
<p>其中各参数的含义和定义同上文公式（1）（2）</p> 
<p>3、另外，有些网络使用了dropout层防止过拟合，在预测时可直接删除dropout层，并不影响预测时数据的传输。</p> 
<p>代码可以参考<a href="https://download.csdn.net/download/cxiazaiyu/10657938">https://download.csdn.net/download/cxiazaiyu/10657938</a></p> 
<p>代码中识别type类型符合BatchNorm, Scale和dropout的层对神经网络配置文件prototxt和权重文件caffemodel做相应修改。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/823a765f30043b5c08a905aede1562a7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">京东数据库智能运维平台建设之路</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/10725ed5a1f9aef3d51e54316d16dbba/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">matlab —— cftool曲线拟合工具箱的使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>