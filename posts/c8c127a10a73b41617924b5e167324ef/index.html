<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>logstash 出现的问题 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/c8c127a10a73b41617924b5e167324ef/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="logstash 出现的问题">
  <meta property="og:description" content="以下所列问题，都是本人在使用Logstash做异库同步业务中，所遇到的问题，记录一下，供大家参考。很多问题解决后，想想感觉很崩溃，尤其是第二个问题。
一、
[FATAL][logstash.runner ] An unexpected error occurred! {:error=&amp;gt;#&amp;lt;NoMethodError: undefined method `[]&#39; for nil:NilClass&amp;gt;, :backtrace=&amp;gt;[&#34;/export/servers/logstash-6.2.2/vendor/bundle/jruby/2.3.0/gems/stud-0.0.23/lib/stud/buffer.rb:187:in `buffer_flush&#39;&#34;, &#34;/export/servers/logstash-6.2.2/vendor/bundle/jruby/2.3.0/gems/stud-0.0.23/lib/stud/buffer.rb:112:in `block in buffer_initialize&#39;&#34;, &#34;org/jruby/RubyKernel.java:1292:in `loop&#39;&#34;, &#34;/export/servers/logstash-6.2.2/vendor/bundle/jruby/2.3.0/gems/stud-0.0.23/lib/stud/buffer.rb:110:in `block in buffer_initialize&#39;&#34;]} [INFO ][logstash.pipeline ] Pipeline started succesfully {:pipeline_id=&amp;gt;&#34;main&#34;, :thread=&amp;gt;&#34;#&amp;lt;Thread:0x1b2da3f0 run&amp;gt;&#34;} [INFO ][logstash.agent ] Pipelines running {:count=&amp;gt;1, :pipelines=&amp;gt;[&#34;main&#34;]} [ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: org.jruby.exceptions.RaiseException: (SystemExit) exit 这个问题很隐晦。当我们使用-t对logstash的配置文件进行测试的时候，没问题，但是当运行后，就会出现这样的错误。在我多次的测试中，发现这其实还是配置文件的问题，只是logstash架构自己不能检测出来。所以遇到这样的问题，别慌。细心检查自己的配置文件。
二、
[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items {:outgoing_count=&amp;gt;1, :exception=&amp;gt;&#34;LogStash::Error&#34;, :backtrace=&amp;gt;[&#34;org/logstash/ext/JrubyEventExtLibrary.java:202:in `sprintf&#39;&#34;, &#34;/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-3.0.4/lib/logstash/outputs/webhdfs.rb:194:in `flush&#39;&#34;, &#34;org/jruby/RubyArray.java:2409:in `collect&#39;&#34;, &#34;/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-3.0.4/lib/logstash/outputs/webhdfs.rb:189:in `flush&#39;&#34;, &#34;/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/stud-0.0.23/lib/stud/buffer.rb:219:in `buffer_flush&#39;&#34;">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2018-05-26T15:25:18+08:00">
    <meta property="article:modified_time" content="2018-05-26T15:25:18+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">logstash 出现的问题</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>以下所列问题，都是本人在使用Logstash做异库同步业务中，所遇到的问题，记录一下，供大家参考。很多问题解决后，想想感觉很崩溃，尤其是第二个问题。</p> 
<p>一、</p> 
<pre><code class="language-java">[FATAL][logstash.runner          ] An unexpected error occurred! {:error=&gt;#&lt;NoMethodError: undefined method `[]' for nil:NilClass&gt;, :backtrace=&gt;["/export/servers/logstash-6.2.2/vendor/bundle/jruby/2.3.0/gems/stud-0.0.23/lib/stud/buffer.rb:187:in `buffer_flush'", "/export/servers/logstash-6.2.2/vendor/bundle/jruby/2.3.0/gems/stud-0.0.23/lib/stud/buffer.rb:112:in `block in buffer_initialize'", "org/jruby/RubyKernel.java:1292:in `loop'", "/export/servers/logstash-6.2.2/vendor/bundle/jruby/2.3.0/gems/stud-0.0.23/lib/stud/buffer.rb:110:in `block in buffer_initialize'"]}

[INFO ][logstash.pipeline        ] Pipeline started succesfully {:pipeline_id=&gt;"main", :thread=&gt;"#&lt;Thread:0x1b2da3f0 run&gt;"}
[INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :pipelines=&gt;["main"]}
[ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: org.jruby.exceptions.RaiseException: (SystemExit) exit</code></pre> 
<p>这个问题很隐晦。当我们使用-t对logstash的配置文件进行测试的时候，没问题，但是当运行后，就会出现这样的错误。在我多次的测试中，发现这其实还是配置文件的问题，只是logstash架构自己不能检测出来。所以遇到这样的问题，别慌。细心检查自己的配置文件。</p> 
<p>二、</p> 
<pre><code class="language-java">[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items {:outgoing_count=&gt;1, :exception=&gt;"LogStash::Error", :backtrace=&gt;["org/logstash/ext/JrubyEventExtLibrary.java:202:in `sprintf'", "/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-3.0.4/lib/logstash/outputs/webhdfs.rb:194:in `flush'", "org/jruby/RubyArray.java:2409:in `collect'", "/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-3.0.4/lib/logstash/outputs/webhdfs.rb:189:in `flush'", "/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/stud-0.0.23/lib/stud/buffer.rb:219:in `buffer_flush'", "org/jruby/RubyHash.java:1342:in `each'", "/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/stud-0.0.23/lib/stud/buffer.rb:216:in `buffer_flush'", "/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/stud-0.0.23/lib/stud/buffer.rb:193:in `buffer_flush'", "/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/stud-0.0.23/lib/stud/buffer.rb:159:in `buffer_receive'", "/export/servers/logstash-5.5.3/vendor/bundle/jruby/1.9/gems/logstash-output-webhdfs-3.0.4/lib/logstash/outputs/webhdfs.rb:182:in `receive'", "/export/servers/logstash-5.5.3/logstash-core/lib/logstash/outputs/base.rb:92:in `multi_receive'", "org/jruby/RubyArray.java:1613:in `each'", "/export/servers/logstash-5.5.3/logstash-core/lib/logstash/outputs/base.rb:92:in `multi_receive'", "/export/servers/logstash-5.5.3/logstash-core/lib/logstash/output_delegator_strategies/legacy.rb:22:in `multi_receive'", "/export/servers/logstash-5.5.3/logstash-core/lib/logstash/output_delegator.rb:47:in `multi_receive'", "/export/servers/logstash-5.5.3/logstash-core/lib/logstash/pipeline.rb:420:in `output_batch'", "org/jruby/RubyHash.java:1342:in `each'", "/export/servers/logstash-5.5.3/logstash-core/lib/logstash/pipeline.rb:419:in `output_batch'", "/export/servers/logstash-5.5.3/logstash-core/lib/logstash/pipeline.rb:365:in `worker_loop'", "/export/servers/logstash-5.5.3/logstash-core/lib/logstash/pipeline.rb:330:in `start_workers'"]}</code></pre> 
<p>这个问题，其实是我花了很长时间都没弄懂的。最终在亚洲和涛哥的帮助下才解决的，我觉得不是我能力不够，而是真的太出乎意料了。</p> 
<p>原因是我的输出路径。我们先看一下<a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html#plugins-outputs-webhdfs-compression" rel="nofollow">官网</a>对path的举例：<span style="color:rgb(68,68,68);font-family:'Open Sans', helvetica;background-color:rgb(255,255,255);"> e.g.: </span><code class="literal" style="font-family:Consolas, Menlo, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console';font-size:.9em;padding:0px 3px;color:rgb(85,85,85);background:rgb(248,248,248);vertical-align:middle;width:auto;">/user/logstash/dt=%{+YYYY-MM-dd}/%{@source_host}-%{+HH}.log。同样的写法我用在了我的配置文件中，只是我把log替换为了TXT。最开始的测试中，完全没问题，所以在出现问题后，我就根本没想过居然是这的问题。最终我替换成了/hive/logs/output.txt，最终解决了这个问题。其实后面我有试过改成<span style="color:rgb(85,85,85);font-family:Consolas, Menlo, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console';font-size:14.4px;background-color:rgb(248,248,248);">dt=%{+YYYY-MM-dd}/%{+HH}，几次测试又没问题了，在接近上线的时候，又出现为了上面的问题。最终保险决定不使用这种格式了。这个问题我已经提交官网，后续我也会关注其解决情况。</span></code></p> 
<p><code class="literal" style="font-family:Consolas, Menlo, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console';font-size:.9em;padding:0px 3px;color:rgb(85,85,85);background:rgb(248,248,248);vertical-align:middle;width:auto;"><span style="color:rgb(85,85,85);font-family:Consolas, Menlo, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console';font-size:14.4px;background-color:rgb(248,248,248);">三、</span></code></p> 
<pre><code class="language-java">[WARN ][logstash.outputs.webhdfs ] webhdfs write caused an exception: 
{"RemoteException":{"exception":"AlreadyBeingCreatedException",
"javaClassName":"org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException",
"message":"Failed to APPEND_FILE /output for DFSClient_NONMAPREDUCE_-688998419_40 on 10.66.90.167 
because this file lease is currently owned by DFSClient_NONMAPREDUCE_-380528477_38 on 10.66.90.167\n\tat 
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2932)\n\tat 
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2683)\n\tat 
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:2982)\n\tat 
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2950)\n\tat 
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:654)\n\tat 
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:421)\n\tat 
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat 
org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat 
org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat 
org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat 
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1727)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n"}}. 
Maybe you should increase retry_interval or reduce number of workers. Retrying...</code></pre>这个问题就很简单了，在我的输出配置中，我设置了： 
<pre><code class="language-java">retry_interval =&gt; 30  # 间隔多久向HDFS尝试重写</code></pre> 
<p>如果上述问题不能很好的解决，其实最终会引发下面的问题：</p> 
<pre><code class="language-java">[ERROR][logstash.outputs.webhdfs ] Max write retries reached. Events will be discarded. Exception: {"RemoteException":{"exception":"IOException","javaClassName":"java.io.IOException","message":"append: lastBlock=blk_1073768614_56374 of src=/output is not sufficiently replicated yet.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2690)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:2982)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2950)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:654)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:421)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1727)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n"}}</code></pre>最终导致写入HDFS数据请求失败。 
<br> 
<p>我当时配置的时间很短，当写入的数据比较多的时候，就会警告。虽然不影响使用，但是看着不舒服，就增大了这个配置。</p> 
<p>后续我会不断更新此文章，如果大家在开发遇到有意思的问题，也欢迎共同探讨。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d2d0260ecb40f3f86ff667c2ba604297/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">jQuery动态添加input，name值自增</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cc7f36808070bf5334836b3e229b495d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Kotlin——初级篇（八）：关于字符串（String）常用操作汇总</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>