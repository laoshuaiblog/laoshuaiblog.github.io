<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>让Stable Diffusion一秒出图！清华硕士加速神器爆火，已有公司接入 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/d1630f6c8d1c3443b383595365223f54/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="让Stable Diffusion一秒出图！清华硕士加速神器爆火，已有公司接入">
  <meta property="og:description" content="克雷西 发自 凹非寺 量子位 | 公众号 QbitAI AI图像生成，已经进入了秒速级别，只要4步推理就能完成绘制，最快更是能在1秒之内完成。
现在，清华大学联合HuggingFace的研究人员，推出了全新的绘图模型加速模块。
作者给出的体验版当中，点击生成按钮后，模型只用了几秒钟就绘制出了4张清晰的图像。
这个加速模块叫做LCM-LoRA，发布后不久就斩获了2k&#43;次GitHub星标。
它不仅加速能力强、泛化性能好，适配的模型也很广泛，SD系和LoRA模型都能用它来加速。
团队基于LCM-LoRA自行优化的文生图模型已在HuggingFace上开放体验，图生图模型也推出了CoLab等版本。
AI绘图工具迅速接入 LCM-LoRA开源后不久，就有AI绘图工具厂商Scenario宣布将基于它推出“实时绘图”功能。
Scenario的CEO还在𝕏上亲自展示了即将上线的实时绘图功能DEMO。
只见一边在绘制草图，另一边AI就把相应的画作绘制好了，时间上几乎同步。
调整提示词和有关参数，模型响应得也是干脆利落。
这些DEMO发布后，引发了众人的一致赞叹。
那么，LCM-LoRA这个加速模块到底有多强，又是怎样实现的呢？
“跳步”降低内存开销 LCM-LoRA将LoRA引入潜在一致性模型（LCM）的蒸馏过程，显著减少了训练内存开销，从而提高性能。
而LCM是从潜扩散模型（LDM）中蒸馏出来的，“蒸馏”的过程也可以看做是对扩散模型的微调。
它的核心思想是在图像的隐变量空间中学习一致性映射函数，该函数可以直接将扩散过程中的任意点映射到终点，即微分方程的解。
通过这种一致性映射，LCM可以跳过迭代采样过程，直接进行少步甚至一步采样，从而极大地加速了图像的生成。
而隐变量空间操作相比基于像素空间的方法，计算复杂度和内存需求也更低。
结合LoRA后，只需要训练低秩分解矩阵，可训练参数量和内存开销进一步减少，应用范围也从单纯的文生图扩展到了图生图和视频生成。
最直观体现的就是我们看到的秒速出图，而训练时间上，LCM-LoRA优化后的模型在A100上训练只需32个GPU时。
训练时间缩短的背后，也于训练参数量大幅减少密切相关：
SD-V1.5全量参数为9.8亿，使用LoRA后可训练参数减少到6750万，约减少了93.1%。
SSD-1B参数从13亿减少到1.05亿，约减少了91.9%。
SDXL参数从35亿减少到1.97亿，约减少了94.3%。
不仅是训练消耗的降低，推理过程中的步数也大幅减少，一般只需要4步推理就能绘制出质量不错的图像。
有时甚至只要一步就能完成，用时还不到1秒，FID分数（越低越好）在50以下。
不仅加速性能优异，LCM-LoRA的适配性也十分广泛。
LCM-LoRA训练得到的LoRA参数又称为加速向量，可以数据集上微调得到的LoRA参数直接线性组合，不需要额外训练。
这种组合方式使得LCM-LoRA成为一个可直接插接到各种微调模型中的通用图像生成加速模块。
作者简介 LCM和LCM-LoRA论文的两位主要作者是来自清华大学交叉信息研究院的研究生骆思勉（Simian Luo）和谭亦钦（Yiqin Tan）。
清华叉院的黄隆波副教授、李建副教授和赵行助理教授也参与了这两项研究。
在LCM-LoRA的工作中，来自HuggingFace的研究人员亦有贡献。
论文地址：
[1]https://arxiv.org/abs/2310.04378
[2]https://arxiv.org/abs/2311.05556
关注公众号【机器学习与AI生成创作】，更多精彩等你来读
卧剿，6万字！30个方向130篇！CVPR 2023 最全 AIGC 论文！一口气读完
深入浅出stable diffusion：AI作画技术背后的潜在扩散模型论文解读
深入浅出ControlNet，一种可控生成的AIGC绘画生成算法！ 经典GAN不得不读：StyleGAN
戳我，查看GAN的系列专辑~！
一杯奶茶，成为AIGC&#43;CV视觉的前沿弄潮儿！
最新最全100篇汇总！生成扩散模型Diffusion Models
ECCV2022 | 生成对抗网络GAN部分论文汇总
CVPR 2022 | 25&#43;方向、最新50篇GAN论文
ICCV 2021 | 35个主题GAN论文汇总">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-11-20T18:31:06+08:00">
    <meta property="article:modified_time" content="2023-11-20T18:31:06+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">让Stable Diffusion一秒出图！清华硕士加速神器爆火，已有公司接入</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h6>克雷西 发自 凹非寺  量子位 | 公众号 QbitAI</h6> 
 <p style="text-align:left;">AI图像生成，已经进入了秒速级别，只要4步推理就能完成绘制，最快更是能在1秒之内完成。</p> 
 <p style="text-align:left;">现在，清华大学联合HuggingFace的研究人员，推出了全新的绘图模型加速模块。</p> 
 <p style="text-align:left;">作者给出的体验版当中，点击生成按钮后，模型只用了几秒钟就绘制出了4张清晰的图像。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/1a/95/bFgDqw5a_o.gif" alt="f151cea7abc69bb2982d1bddb6bc9c64.gif"></p> 
 <p style="text-align:left;">这个加速模块叫做LCM-LoRA，发布后不久就斩获了2k+次GitHub星标。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c1/69/4lTbJTM0_o.png" alt="7cc3813c9ff706b7717ad185cd72742f.png"></p> 
 <p style="text-align:left;">它不仅加速能力强、泛化性能好，适配的模型也很广泛，SD系和LoRA模型都能用它来加速。</p> 
 <p style="text-align:left;">团队基于LCM-LoRA自行优化的文生图模型已在HuggingFace上开放体验，图生图模型也推出了CoLab等版本。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/23/d2/VWikgpFB_o.png" alt="b81ba9103538dc3da0b4641670d146c1.png"></p> 
 <h3>AI绘图工具迅速接入</h3> 
 <p style="text-align:left;">LCM-LoRA开源后不久，就有AI绘图工具厂商Scenario宣布将基于它推出“实时绘图”功能。</p> 
 <p style="text-align:left;">Scenario的CEO还在𝕏上亲自展示了即将上线的实时绘图功能DEMO。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/08/95/jCcpkP81_o.png" alt="dedfeeaf48c743d253a489b22ce26a9d.png"></p> 
 <p style="text-align:left;">只见一边在绘制草图，另一边AI就把相应的画作绘制好了，时间上几乎同步。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/dc/65/6Xs0U4um_o.gif" alt="d6ecf44eaa2ebab9385c0cf40389721a.gif"></p> 
 <p style="text-align:left;">调整提示词和有关参数，模型响应得也是干脆利落。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/85/78/XUtpcDI9_o.gif" alt="039fc245f8019dba04bc284de80b29f9.gif"></p> 
 <p style="text-align:left;">这些DEMO发布后，引发了众人的一致赞叹。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b9/c2/YlgGxBg3_o.png" alt="cc8de1a3878c7687a940c4e82998dc8d.png"></p> 
 <p style="text-align:left;">那么，LCM-LoRA这个加速模块到底有多强，又是怎样实现的呢？</p> 
 <h3>“跳步”降低内存开销</h3> 
 <p style="text-align:left;">LCM-LoRA将LoRA引入潜在一致性模型（LCM）的蒸馏过程，显著减少了训练内存开销，从而提高性能。</p> 
 <p style="text-align:left;">而LCM是从潜扩散模型（LDM）中蒸馏出来的，“蒸馏”的过程也可以看做是对扩散模型的微调。</p> 
 <p style="text-align:left;">它的核心思想是在图像的<strong>隐变量空间中</strong>学习一致性映射函数，该函数可以直接将扩散过程中的任意点映射到终点，即微分方程的解。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/85/e8/ldYrT8AY_o.png" alt="927af6cb0c3f050ac6880aa485ed9a1c.png"></p> 
 <p style="text-align:left;">通过这种一致性映射，LCM可以<strong>跳过迭代采样过程</strong>，直接进行少步甚至一步采样，从而极大地加速了图像的生成。</p> 
 <p style="text-align:left;">而隐变量空间操作相比基于像素空间的方法，计算复杂度和内存需求也更低。</p> 
 <p style="text-align:left;">结合LoRA后，只需要训练低秩分解矩阵，可训练参数量和内存开销进一步减少，应用范围也从单纯的文生图扩展到了图生图和视频生成。</p> 
 <p style="text-align:left;">最直观体现的就是我们看到的秒速出图，而训练时间上，LCM-LoRA优化后的模型在A100上训练只需32个GPU时。</p> 
 <p style="text-align:left;">训练时间缩短的背后，也于训练参数量大幅减少密切相关：</p> 
 <ul><li><p>SD-V1.5全量参数为9.8亿，使用LoRA后可训练参数减少到6750万，约减少了93.1%。</p></li><li><p>SSD-1B参数从13亿减少到1.05亿，约减少了91.9%。</p></li><li><p>SDXL参数从35亿减少到1.97亿，约减少了94.3%。</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e6/15/bBOrxZjX_o.png" alt="d2535e67840f73ffa79ea92b6f2c8793.png"></p> 
 <p style="text-align:left;">不仅是训练消耗的降低，推理过程中的步数也大幅减少，一般只需要4步推理就能绘制出质量不错的图像。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/14/b9/B1lyBzH7_o.png" alt="da3cf83d2718ef6c7d5814ded419946a.png"></p> 
 <p style="text-align:left;">有时甚至只要一步就能完成，用时还不到1秒，FID分数（越低越好）在50以下。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c0/66/zb1gDk56_o.png" alt="d98c48777b417378cbc1f53d23802319.png"></p> 
 <p style="text-align:left;">不仅加速性能优异，LCM-LoRA的适配性也十分广泛。</p> 
 <p style="text-align:left;">LCM-LoRA训练得到的LoRA参数又称为加速向量，可以数据集上微调得到的LoRA参数<strong>直接线性组合</strong>，不需要额外训练。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b1/81/2NqUSAjY_o.png" alt="776238c371555d97c6855118877fba01.png"></p> 
 <p style="text-align:left;">这种组合方式使得LCM-LoRA成为一个可直接插接到各种微调模型中的通用图像生成加速模块。</p> 
 <h3>作者简介</h3> 
 <p style="text-align:left;">LCM和LCM-LoRA论文的两位主要作者是来自清华大学交叉信息研究院的研究生骆思勉（Simian Luo）和谭亦钦（Yiqin Tan）。</p> 
 <p style="text-align:left;">清华叉院的黄隆波副教授、李建副教授和赵行助理教授也参与了这两项研究。</p> 
 <p style="text-align:left;">在LCM-LoRA的工作中，来自HuggingFace的研究人员亦有贡献。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e3/3a/DvgEpBKs_o.png" alt="9ca800f83c37137c75313da29950a672.png"></p> 
 <p style="text-align:left;">论文地址：<br>[1]https://arxiv.org/abs/2310.04378<br>[2]https://arxiv.org/abs/2311.05556</p> 
 <p style="text-align:center;"><strong>关注公众号【机器学习与AI生成创作】，更多精彩等你来读</strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">卧剿，6万字！30个方向130篇！CVPR 2023 最全 AIGC 论文！一口气读完</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">深入浅出stable diffusion：AI作画技术背后的潜在扩散模型论文解读</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">深入浅出ControlNet，一种可控生成的AIGC绘画生成算法！</a> <br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">经典GAN不得不读：StyleGAN</a><br></strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b4/e4/7Lv1QCxH_o.png" alt="017bcaa8b9ec77c3ad4af9c0e84a169b.png"> <strong><a href="" rel="nofollow">戳我，查看GAN的系列专辑~！</a></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">一杯奶茶，成为AIGC+CV视觉的前沿弄潮儿！</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">最新最全100篇汇总！生成扩散模型Diffusion Models</a></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">ECCV2022 | 生成对抗网络GAN部分论文汇总</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">CVPR 2022 | 25+方向、最新50篇GAN论文</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow"> ICCV 2021 | 35个主题GAN论文汇总</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">超110篇！CVPR 2021最全GAN论文梳理</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><strong>超100篇！CVPR 2020最全GAN论文梳理</strong></a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">拆解组新的GAN：解耦表征MixNMatch</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">StarGAN第2版：多域多样性图像生成</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 | 《可解释的机器学习》中文版</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 |《TensorFlow 2.0 深度学习算法实战》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 |《计算机视觉中的数学方法》分享</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《基于深度学习的表面缺陷检测方法综述》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《零样本图像分类综述: 十年进展》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《基于深度神经网络的少样本学习综述》</a></p> 
 <blockquote> 
  <p>《礼记·学记》有云：独学而无友，则孤陋而寡闻</p> 
 </blockquote> 
 <p><strong>点击</strong><em><strong><a href="" rel="nofollow">一杯奶茶，成为AIGC+CV视觉的前沿弄潮儿！</a></strong></em><strong>，加入 </strong><strong>AI生成创作与计算机视觉</strong><strong> 知识星球！</strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e88d61b43533fdc5e18f1b1c885a62e4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JavaScript 运行机制</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3e34e6aa0fb8c556657dd7187140a64c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">P1036 [NOIP2002 普及组] 选数 (判断素数，dfs）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>