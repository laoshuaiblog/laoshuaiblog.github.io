<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python实现股票数据接口 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/148b0fd4f0c7b7e3ffc36e69209b8d91/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="Python实现股票数据接口">
  <meta property="og:description" content="资源下载地址：https://download.csdn.net/download/sheziqiong/85984124
资源下载地址：https://download.csdn.net/download/sheziqiong/85984124
这篇文章主要介绍如何使用 Python 股票数据和实现数据接口。
1.定时抓取和解析数据
2.存储数据到 MongoDB
3.缓存数据到 Redis
4.配置 Nginx 和数据接口
1.定时抓和解析数据 按照链接的格式，我们拼接好股票代码、年份和季度：
url = &#34;http://quotes.money.163.com/trade/lsjysj_&#34; &#43; key &#43; &#34;.html?year=&#34; &#43; year &#43; &#34;&amp;amp;season=&#34; &#43; season 拼接好链接后，使用 requests 库获取页面的内容：
requests.get(url) self.parse_pager(content.content, item[&#34;code&#34;]) 考虑到网络请求可能会失败，我们在请求失败时设置多次重新请求(最多 8 次)，如果多次请求后仍然失败，则将请求的相关内容存储到 error_logs 中：
# 请求失败后重新请求(最多8次) max_try = 8 for tries in range(max_try): try: content = requests.get(url) self.parse_pager(content.content, item[&#34;code&#34;]) break except Exception: if tries &amp;lt; (max_try - 1): sleep(2) continue else: add_error_logs(&#34;crawl_error&#34;, &#34;501&#34;, key) 获取到页面内容后，我们先来分析页面结构（图 1），我们需要的数据大概是以这样的格式存在的：tr 标签表示股票某一天的行情，tr 标签下的 td 标签表示当前行情的详细数据：">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-07-10T12:53:37+08:00">
    <meta property="article:modified_time" content="2022-07-10T12:53:37+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python实现股票数据接口</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><a href="https://download.csdn.net/download/sheziqiong/85984124">资源下载地址</a>：https://download.csdn.net/download/sheziqiong/85984124<br> <a href="https://download.csdn.net/download/sheziqiong/85984124">资源下载地址</a>：https://download.csdn.net/download/sheziqiong/85984124<br> 这篇文章主要介绍如何使用 Python 股票数据和实现数据接口。</p> 
<p>1.定时抓取和解析数据</p> 
<p>2.存储数据到 MongoDB</p> 
<p>3.缓存数据到 Redis</p> 
<p>4.配置 Nginx 和数据接口</p> 
<h3><a id="1_12"></a>1.定时抓和解析数据</h3> 
<p>按照链接的格式，我们拼接好股票代码、年份和季度：</p> 
<pre><code>url = "http://quotes.money.163.com/trade/lsjysj_" + key + ".html?year=" + year + "&amp;season=" + season
</code></pre> 
<p>拼接好链接后，使用 requests 库获取页面的内容：</p> 
<pre><code>requests.get(url)
self.parse_pager(content.content, item["code"])
</code></pre> 
<p>考虑到网络请求可能会失败，我们在请求失败时设置多次重新请求(最多 8 次)，如果多次请求后仍然失败，则将请求的相关内容存储到 error_logs 中：</p> 
<pre><code># 请求失败后重新请求(最多8次)
  max_try = 8
  for tries in range(max_try):
      try:
          content = requests.get(url)
          self.parse_pager(content.content, item["code"])
          break
      except Exception:
          if tries &lt; (max_try - 1):
              sleep(2)
              continue
          else:
              add_error_logs("crawl_error", "501", key)
</code></pre> 
<p>获取到页面内容后，我们先来分析页面结构（图 1），我们需要的数据大概是以这样的格式存在的：tr 标签表示股票某一天的行情，tr 标签下的 td 标签表示当前行情的详细数据：</p> 
<p><img src="https://images2.imgbox.com/d5/4b/62ibvZR5_o.png" alt="在这里插入图片描述"></p> 
<p>使用 BeautifulSoup 库对页面进行解析，soup.select(“div.inner_box tr”)会以列表的形势返回 div.inner_box 下的所有 tr 标签：</p> 
<pre><code>soup = bs4.BeautifulSoup(content, "lxml")
parse_list = soup.select("div.inner_box tr")
</code></pre> 
<p>[x.string for x in item.select(“td”)]会将 tr 标签下的内容组合成一个数组 data，这个数组就是我们要抓取的数据：</p> 
<pre><code>data = [x.string for x in item.select("td")]
</code></pre> 
<p>每次解析页面时，我们都会从数据库中取出当前股票已经存在的数据，用于判断待插入数据是否已经存在数据库中。这样做可以及时补全数据，并且避免数据重复插入。</p> 
<pre><code>if price["cur_timer"]not in timer_list:
self.dm.add_tk_item(key, price)
</code></pre> 
<p>由于股票数据是频繁变动的，这就要求我们定时对数据进行更新，这里我们编写一个定时器来实现定时更新数据的功能：</p> 
<pre><code>timer = threading.Timer(time_interval, fun_timer) 
timer.start()
</code></pre> 
<p>我们设置每天 16 点更新数据：</p> 
<pre><code>if (hour =="16" or hour =="20")and minute =="00":
 dc = ENDataCrawl()
 dc.start_crawl()
   sleep(time_interval)
 rm = RedisManager()
 rm.update_data()
</code></pre> 
<h3><a id="2_MongoDB_88"></a>2.存储数据到 MongoDB</h3> 
<p>这里我们使用 MongoDB 来存储数据，MongoDB 作为一个面向文档存储的数据库，操作起来相对比较简单和容易。在编写代码之前，我们需要先进行安装 <a href="http://www.runoob.com/mongodb/mongodb-window-install.html" rel="nofollow">MongoDB 安装教程</a>，此外 python 操作 MongoDB 需要用到 pymongo 库，命令行下输入 pip install pymongo 安装即可。</p> 
<p>安装完成后，我们开始编写 MongoDB 相关的代码，新建 DBManager 类用于管理数据库相关操作：</p> 
<pre><code>class DBManager:
    def __init__(self, table_name):
        # 指定端口和地址
        self.client = MongoClient(mod_config.get_config("database", "dbhost"), int(mod_config.get_config("database", "dbport")))
        # 选择数据库
        self.db = self.client[mod_config.get_config("database", "dbname")]
        self.table = self.db[table_name]
</code></pre> 
<p>在 DBManager 类中，我们最常用到的有 add_tk_item 方法，这个方法会根据 tk_code（股票代码），将最新的数据插入到 price_list 中。</p> 
<pre><code>def add_tk_item(self, tk_code, price_item):
  return self.table.update_one({'code': tk_code}, {"$push": {"price_list": price_item}})*
</code></pre> 
<p>以及 find_by_id 方法，这个方法会根据 tk_code 查询相应的股票信息。当我们需要对 Cursor 进行长时间循环遍历时，应该将 no_cursor_timeout 设置为 true。</p> 
<pre><code>def find_by_id(self, tk_code, request={}):
  if tk_code:
request["code"] = tk_code
 return self.table.find_one(request)
else:
 # 数据量较大时避免CursorNotFoundException
    return self.table.find({}, no_cursor_timeout=True)*
</code></pre> 
<h3><a id="3_Redis_123"></a>3.缓存数据到 Redis</h3> 
<p>为了提升响应速度，我们使用 Redis 对数据进行缓存，Redis 作为一个 key-value 存储系统，具有极高的性能。跟之前一样我们需要先安装 Redis<a href="http://www.runoob.com/redis/redis-intro.html" rel="nofollow">Redis 安装教程</a>，然后为 python 安装 Redis 库，使用 pip install Redis 命令。</p> 
<p>接下来我们创建 RedisManager 类用于管理 Redis 的相关操作：</p> 
<pre><code>class RedisManager:
def __init__(self):
     self.pool = redis.ConnectionPool(host=mod_config.get_config("redis", "redis_host"), port=mod_config.get_config("redis", "redis_port"), decode_responses=True)
self.r = redis.Redis(connection_pool=self.pool)
</code></pre> 
<p>update_data 方法用于将 MongoDB 的数据同步到 Redis，每次系统执行完爬取业务后都会调用该方法：</p> 
<pre><code>    def update_data(self):
        # 将mongodb中的数据同步到redis中
        dm = DBManager("tk_details")
        code_list = dm.find_by_id("")
        for item in code_list:
            try:
                code = item["code"][:6]
                _result = dm.find_by_id(item["code"])
                sorted_result = sorted(_result["price_list"], cmp=cmp_datetime, key=operator.itemgetter("cur_timer"))
                self.r.set(code, sorted_result)
            except Exception:
                add_error_logs("redis_error", "501", item["code"])
                continue
</code></pre> 
<h3><a id="4_Nginx__154"></a>4.配置 Nginx 和数据接口</h3> 
<p>由于我们只有一个简单的数据接口，所以选择使用 Nginx，Nginx 作为一个高性能的 Web 和反向代理服务器，具有简洁高效，占用资源少等优点。考虑到很多开发者习惯在 Windows 下调试代码，我们先在 Windows 系统中安装 Nginx<a href="https://www.cnblogs.com/saysmy/p/6609796.html" rel="nofollow">windows 下安装 nginx</a>（Windows 下 Nginx 是以应用的形式运行的，这可能也是很多人不愿意在 Windows 下运行 Nginx 的原因）。</p> 
<p>配置好 Nginx 后我们开始编写数据接口，start_api_tkdata 方法会开启一个监听，用于响应 Nginx 的请求：</p> 
<pre><code>def start_api_tkdata(): 
WSGIServer(myapp, bindAddress=(mod_config.get_config("server", "server_host"), int(mod_config.get_config("server", "tk_data_port")))).run()
</code></pre> 
<p>myapp 方法每次收到请求时，都会对请求的格式和参数进行校验，校验通过后则从 Redis 中获取数据以 JSON 格式返回。</p> 
<pre><code>start_response('200 OK', [('Content-Type', 'text/plain')])
result_json["data"] =str(result).replace("u'", "'")
result_json["tk_code"] =str(list_query[i +1])
return [json.dumps(result_json)]
</code></pre> 
<p>编写完数据接口后，我们在本机启动 Nginx，在浏览器中输入 <a href="http://127.0.0.1:9002/tkdata?code=600008" rel="nofollow">http://127.0.0.1/tkdata?code=600008</a>，可以看到如下结果（图 2）：</p> 
<p><img src="https://images2.imgbox.com/38/e6/SvMqriKA_o.png" alt="在这里插入图片描述"></p> 
<p>到此为止，我们的股票爬虫和数据接口就已经完成了，我们还可以在现有的基础上做一些优化，例如：</p> 
<p>1.爬取数据时使用多线程和多进程。</p> 
<p>2.添加更多的数据接口，添加均线、Macd、Boll 等指标数据，这些数据可以由收盘价计算得到。</p> 
<p>3.添加数据检测和日志管理模块，如果你打算将这套系统用在生产环境中，这些模块是必须要有的。</p> 
<p><a href="https://download.csdn.net/download/sheziqiong/85984124">资源下载地址</a>：https://download.csdn.net/download/sheziqiong/85984124<br> <a href="https://download.csdn.net/download/sheziqiong/85984124">资源下载地址</a>：https://download.csdn.net/download/sheziqiong/85984124</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/63336d1fcaed2cd6f1e95bebcfae4124/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue的组件之间方法以及变量的调用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e7198c1652ea8e9e15e0c61426ea267c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">存储系统Cache（知识点&#43;例题）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>