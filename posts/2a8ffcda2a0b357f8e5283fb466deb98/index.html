<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【大模型】大型模型飞跃升级—文档图像识别领域迎来技术巨变 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/2a8ffcda2a0b357f8e5283fb466deb98/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="【大模型】大型模型飞跃升级—文档图像识别领域迎来技术巨变">
  <meta property="og:description" content="写在前面 2023年12月31日，第十九届中国图象图形学学会青年科学家会议在广州举行，由中国图象图形学学会主办。
该会议的目标是促进青年科学家之间的交流与合作，以提升我国在图像图形领域的科研水平和创新能力。
由中国图象图形学学会和上海合合信息（INTSIG）联合承办的《垂直领域大模型论坛》中，专注于探讨大语言模型时代下以ChatGPT为代表的大模型技术对图像图形学领域研究方向或落地应用的潜在价值。包括合合信息丁凯博士在内的多位业内专家对大模型时代文档与图像识别领域的新探索进行了详细介绍。
一、技术难题&amp;amp;挑战 文档图像分析识别与理解是计算机视觉和自然语言处理领域的一个复杂问题，涉及到从图像中提取文本信息、理解文档结构、识别语义等多个层面。下面是一些相关的技术难题：
具体问题如下：
场景以及版式多样性： 文档可能以不同的场景和版式出现，例如室内、室外、手写、打印等。每种场景和版式都可能导致不同的光照、视角、失真等问题。采集设备不稳定性：文档图像可能由不同的设备捕获，如摄像头、扫描仪等，这些设备的性能和参数可能存在差异，导致图像质量不稳定。用户需求多样性： 用户的需求可能各不相同，有的用户可能更关注文本内容的准确性，而另一些用户可能更注重图像的布局和格式。文档图形质量退化严重性： 文档图像可能因为老化、损坏、印刷质量差等原因而质量下降，导致文本和图像的清晰度减弱。文字检测及排版分析困难： 文字可能以不同的字体、大小、方向等形式出现，且可能与其他图像元素重叠或相似，使得文字检测和排版分析变得复杂。非限定条件文字识别率低：在非受限条件下，即不受特定规范或格式的限制，文字识别的难度增加，因为文本可能出现在任何位置、方向和形式。结构化智能理解能力差：对文档结构进行深入理解，包括标题、段落、表格等，是一个复杂的任务，尤其是在处理非结构化文档时。 二、ChatGPT-4模型 最新的版本GPT-4已经在多项测试中超越了其前身，获得了更高的评分。
它是一款高级的人工智能聊天机器人技术，它被训练得对各种问题和场景有深入的理解，并且可以生成富有事实性的响应。
它的主要特点可以归纳为以下几点：
大规模和高参数：GPT-4拥有超过1000亿个参数，是GPT-3的3倍，是GPT-2的300倍，是GPT-1的3000倍。这个庞大的规模远超过其他的语言模型，如谷歌的Gemini（300亿参数）、微软的ProphetNet（230亿参数）、百度的ERNIE-GEN（190亿参数）等。数据丰富和覆盖广泛：GPT-4的训练数据集非常庞大，包含了大量的自然语言文本，涵盖了多种语言和领域。高精度和高准确性：GPT-4在图像描述、翻译、生成代码和解答问题等方面的性能都有显著提高。然而，尽管其能力强大，GPT-4仍有可能出现错误或提供不完全准确的回答。多模态能力：GPT-4不仅可以处理文本信息，还可以处理图像内容。这使得GPT-4能够理解和解释图像内容，并将这些信息转化为自然语言。 2.1 在图像领域的优势 GPT-4在图像识别领域的优势主要体现在以下几个方面：
强大的识图能力：GPT-4拥有卓越的图像理解能力，可以接受图像和文本输入。这不仅使其在处理更复杂的任务时更具优势，也使其在理解和解释图像内容方面具有更高的精确度。零样本效果突出：在四个场景下，GPT-4的零样本效果超过了之前的GPT系列模型，这进一步证明了其在图像识别领域的优越性。回答准确性显著提高：与前一代模型相比，GPT-4在回答问题的准确性上有显著的提高，这对于图像识别任务来说是非常重要的。更强的创造力和灵活性：当任务的复杂性达到一定的阈值时，GPT-4表现出更可靠的性能，并且能够处理更细微的指令。这一特性使得GPT-4在处理一些需要精细分析和创新解决方案的图像识别任务时具有优势。更高的输入文字限制：GPT-4将文字输入限制提升至2.5万字，这意味着它可以处理更为复杂和详细的图像识别任务。 2.2 在图像领域的不足 尽管GPT-4V的水平达到了相当高的程度，但它并未完全解决OCR文档识别领域的所有挑战。
在测试中，它显露出一些明显的短板，首当其冲的是对中文的识别。无论是手写还是印刷文字，GPT-4V在识别后输出了大量与实际文章无关的内容。此外，对于一些简单的手写公式，GPT-4V也无法完美地进行识别。
对于长文档，仍然有文档解析和识别的前置依赖，ChatGPT调用了开源的PyPDF2，而该插件效果一般，且输出不支持表格结构、不支持扫描件、不支持处理复杂版式、不支持定位到原文
总结一下其不足主要有：
图像配准算法选择限制： 在图像配准时，缺乏指定算法的情况下，优先选择常见算法，如Threshold。这可能导致对于复杂和特殊图像任务处理能力的限制。
时间关系推理困难： GPT-4在理解和解释图像内容方面表现出色，但在推理多个图像之间的时间关系方面存在困难。
视觉依赖型问题的限制： GPT-4V在视觉问题类型中，对视觉依赖型问题的回答完全依赖于图像内容。缺乏图像信息时，其回答可能变得不确定或无法确切回答。
领域泛化能力与“编造”事实问题： 尽管GPT-4具有出色的领域泛化能力，但在测试中可能出现“编造”事实的情况。这可能影响其在特定领域的可靠性和准确性。
三、垂直大模型 通过对GPT-4V和文档识别领域的深入分析和思考，为OCR文档识别领域的研究开辟了新的方向。需求不断增长的背景下，提高识别精度和处理效率成为了迫切需要满足的新应用标准。在这一背景下，出现了：
素级OCR统一模型OCR大一统模型文档识别分析&#43;LLM（Language Model） 等应用的新方向。
3.1 素级OCR统一模型 素级OCR统一模型，即UPOCR（Unified Pixel-Level OCR）模型，是一种文档图像像素级多任务处理的统一模型。该模型是由合合信息与华南理工大学联合实验室（以下简称“实验室”）研发的研究项目之一。
UPOCR模型的主要特点是引入了可学习的文本检测和识别模块，可以同时完成多个任务，包括文本检测、文本识别、端到端OCR等。这一特性使得UPOCR模型在处理文档图像时具有较高的效率和准确性。
此外，UPOCR模型还具有较好的通用性，不仅可以处理中英文文档，还可以处理包含公式、表格等复杂结构的文档。
在实际应用中，UPOCR的通用性在文本擦除、文本分割和篡改文本检测任务中经过广泛验证。
UPOCR采用ViTEraser[1]作为其主干网络，通过统一训练联合处理文本擦除、文本分割和篡改文本检测等三种不同任务的提示词。一经完成模型训练，即可无需专门的下游任务精调，直接用于各类下游任务。
总的来说，素级OCR统一模型UPOCR在文档图像预处理统一模型方面展现出了强大的实力和广泛的应用前景。
3.2 OCR大一统模型-SPTS v3 OCR大一统模型是一种创新的端到端文本检测和识别方法，是实验室正在研究的项目之一，也称为SPTS（Simultaneous Processing of Text Spotting and Recognition）。
这种方法颠覆了传统的文本检测和识别流程。在传统的方法中，文本检测和识别被视为两个独立的任务，这导致处理流程复杂且冗余。然而，SPTS将这两个任务融为一体，实现了从文本检测到识别的统一处理。
将文档图像识别分析的各项任务以序列预测的方式进行定义，包括对文本、段落、版面、表格、公式等内容的分析。通过采用不同的提示（prompt）来引导模型执行不同的OCR任务，实现了多任务处理的灵活性。该系统支持篇章级的文档图像识别分析，能够输出标准格式的文本，如Markdown、HTML等。
其中，通过引入LLM（Language Model）来处理文档理解相关的工作，进一步提高了系统对于文档结构和内容的理解能力。这种设计使得系统在处理多样性的文档图像时能够更全面、准确地进行分析，并以标准格式输出，为用户提供更便捷的文档处理和理解服务。
SPTS v3 介绍 多任务序列预测： SPTS v3通过将多种OCR任务抽象为序列预测问题，实现了对文本、段落、版面、表格、公式等不同元素的有序识别和分析。这种设计使得模型能够以一种统一的方式处理各种OCR任务。Prompt引导： SPTS v3采用了不同的prompt来引导模型完成不同的OCR任务。每个任务对应一个独特的提示，这样的引导机制使得模型在学习过程中能够专注于不同的目标，提高了系统的灵活性和适应性。模型架构： SPTS v3延续了SPTS的模型结构，包括了CNN（卷积神经网络）用于图像特征提取，以及Transformer Encoder和Transformer Decoder用于实现图像到序列的转换。这种结构旨在充分捕捉图像中的语义信息，同时具有较强的序列生成能力。任务通用性： 由于采用了序列预测的形式，SPTS v3在任务通用性上表现出色。这使得模型在不同场景和不同任务的OCR挑战中都能够取得良好的性能。 SPTSv3的任务定义，目前主要关注以下任务：端到端检测识别、表格结构识别、手写数学公式识别">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-01-09T10:35:38+08:00">
    <meta property="article:modified_time" content="2024-01-09T10:35:38+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【大模型】大型模型飞跃升级—文档图像识别领域迎来技术巨变</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div> 
 <div style="text-align:center;"> 
  <img alt="" src="https://images2.imgbox.com/37/3f/aTVKm2rM_o.gif"> 
 </div> 
 <div> 
  <h2 style="margin-left:0pt;text-align:left;"><strong><span style="color:#1a1a1a;">写在前面</span></strong></h2> 
  <div> 
   <blockquote> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">2023年12月31日，第十九届中国图象图形学学会青年科学家会议在广州举行，由中国图象图形学学会主办。</span></p> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">该会议的目标是促进青年科学家之间的交流与合作，以提升我国在图像图形领域的科研水平和创新能力。</span></p> 
   </blockquote> 
  </div> 
  <blockquote> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">由中国图象图形学学会和上海合合信息（INTSIG）联合承办的《垂直领域大模型论坛》中，专注于探讨大语言模型时代下以ChatGPT为代表的大模型技术对图像图形学领域研究方向或落地应用的潜在价值。包括合合信息丁凯</span><span style="color:#333333;">博士</span><span style="color:#333333;">在内的多位业内专家对大模型时代文档与图像识别领域的新探索进行了详细介绍。</span></p> 
  </blockquote> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="1200" src="https://images2.imgbox.com/b2/19/Gh8WDOab_o.png" width="1200"></p> 
  <h2 style="text-align:left;"><strong><span style="color:#1a1a1a;">一、技术难题&amp;挑战</span></strong></h2> 
  <p style="margin-left:0;text-align:left;"></p> 
  <div> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">文档图像分析识别与理解是计算机视觉和自然语言处理领域的一个复杂问题，涉及到从图像中提取文本信息、理解文档结构、识别语义等多个层面。下面是一些相关的技术难题：</span></p> 
  </div> 
  <p></p> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="736" src="https://images2.imgbox.com/7d/ad/u3PhpjTL_o.png" width="1200"></p> 
  <p style="margin-left:0;text-align:left;"></p> 
  <div> 
   <blockquote> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">具体问题如下：</span></p> 
    <ol><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">场景以及版式多样性</span></span><span style="color:#333333;">： 文档可能以不同的场景和版式出现，例如室内、室外、手写、打印等。每种场景和版式都可能导致不同的光照、视角、失真等问题。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">采集设备不稳定性</span></span><span style="color:#333333;">：文档图像可能由不同的设备捕获，如摄像头、扫描仪等，这些设备的性能和参数可能存在差异，导致图像质量不稳定。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">用户需求多样性</span></span><span style="color:#333333;">： 用户的需求可能各不相同，有的用户可能更关注文本内容的准确性，而另一些用户可能更注重图像的布局和格式。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">文档图形质量退化严重性</span></span><span style="color:#333333;">： 文档图像可能因为老化、损坏、印刷质量差等原因而质量下降，导致文本和图像的清晰度减弱。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">文字检测及排版分析困难</span></span><span style="color:#333333;">： 文字可能以不同的字体、大小、方向等形式出现，且可能与其他图像元素重叠或相似，使得文字检测和排版分析变得复杂。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">非限定条件文字识别率低</span></span><span style="color:#333333;">：在非受限条件下，即不受特定规范或格式的限制，文字识别的难度增加，因为文本可能出现在任何位置、方向和形式。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">结构化智能理解能力差</span></span><span style="color:#333333;">：对文档结构进行深入理解，包括标题、段落、表格等，是一个复杂的任务，尤其是在处理非结构化文档时。</span></li></ol> 
   </blockquote> 
  </div> 
  <p></p> 
  <p style="margin-left:0;text-align:left;"></p> 
  <h2 style="text-align:left;"><strong><span style="color:#1a1a1a;">二、ChatGPT-4模型</span></strong></h2> 
  <p style="margin-left:0;text-align:left;"></p> 
  <div> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">最新的版本GPT-4已经在多项测试中超越了其前身，获得了更高的评分。</span></p> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">它是一款高级的人工智能聊天机器人技术，它被训练得对各种问题和场景有深入的理解，并且可以生成富有事实性的响应。</span></p> 
  </div> 
  <p></p> 
  <blockquote> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">它的主要特点可以归纳为以下几点：</span></p> 
   <ol><li style="text-align:left;"><span style="background-color:#ffff00;"><span style="color:#333333;">大规模和高参数</span></span><span style="color:#333333;">：GPT-4拥有超过1000亿个参数，是GPT-3的3倍，是GPT-2的300倍，是GPT-1的3000倍。这个庞大的规模远超过其他的语言模型，如谷歌的Gemini（300亿参数）、微软的ProphetNet（230亿参数）、百度的ERNIE-GEN（190亿参数）等。</span></li><li style="text-align:left;"><span style="background-color:#ffff00;"><span style="color:#333333;">数据丰富和覆盖广泛</span></span><span style="color:#333333;">：GPT-4的训练数据集非常庞大，包含了大量的自然语言文本，涵盖了多种语言和领域。</span></li><li style="text-align:left;"><span style="background-color:#ffff00;"><span style="color:#333333;">高精度和高准确性</span></span><span style="color:#333333;">：GPT-4在图像描述、翻译、生成代码和解答问题等方面的性能都有显著提高。然而，尽管其能力强大，GPT-4仍有可能出现错误或提供不完全准确的回答。</span></li><li style="text-align:left;"><span style="background-color:#ffff00;"><span style="color:#333333;">多模态能力</span></span><span style="color:#333333;">：GPT-4不仅可以处理文本信息，还可以处理图像内容。这使得GPT-4能够理解和解释图像内容，并将这些信息转化为自然语言。</span></li></ol> 
  </blockquote> 
  <h3 style="margin-left:0pt;text-align:left;"><strong><span style="color:#1a1a1a;">2.1 在图像领域的优势</span></strong></h3> 
  <blockquote> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">GPT-4在图像识别领域的优势主要体现在以下几个方面：</span></p> 
   <ul><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">强大的识图能力</span></span><span style="color:#333333;">：GPT-4拥有卓越的图像理解能力，可以接受图像和文本输入。这不仅使其在处理更复杂的任务时更具优势，也使其在理解和解释图像内容方面具有更高的精确度。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">零样本效果突出</span></span><span style="color:#333333;">：在四个场景下，GPT-4的零样本效果超过了之前的GPT系列模型，这进一步证明了其在图像识别领域的优越性。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">回答准确性显著提高</span></span><span style="color:#333333;">：与前一代模型相比，GPT-4在回答问题的准确性上有显著的提高，这对于图像识别任务来说是非常重要的。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">更强的创造力和灵活性</span></span><span style="color:#333333;">：当任务的复杂性达到一定的阈值时，GPT-4表现出更可靠的性能，并且能够处理更细微的指令。这一特性使得GPT-4在处理一些需要精细分析和创新解决方案的图像识别任务时具有优势。</span></li><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">更高的输入文字限制</span></span><span style="color:#333333;">：GPT-4将文字输入限制提升至2.5万字，这意味着它可以处理更为复杂和详细的图像识别任务。</span></li></ul> 
  </blockquote> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="860" src="https://images2.imgbox.com/38/8a/QqIlYG9G_o.png" width="1200"></p> 
  <h3 style="margin-left:0pt;text-align:left;"><strong><span style="color:#1a1a1a;">2.2 在图像领域的不足</span></strong></h3> 
  <p style="margin-left:0;text-align:left;"></p> 
  <div> 
   <blockquote> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">尽管GPT-4V的水平达到了相当高的程度，但它并未完全解决OCR文档识别领域的所有挑战。</span></p> 
   </blockquote> 
  </div> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">在测试中，它显露出一些明显的短板，首当其冲的是对中文的识别。无论是手写还是印刷文字，GPT-4V在识别后输出了大量与实际文章无关的内容。此外，对于一些简单的手写公式，GPT-4V也无法完美地进行识别。</span></p> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="668" src="https://images2.imgbox.com/29/3a/VARajqSx_o.png" width="1200"></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">对于长文档，仍然有文档解析和识别的前置依赖，ChatGPT调用了开源的PyPDF2，而该插件效果一般，且输出不支持表格结构、不支持扫描件、不支持处理复杂版式、不支持定位到原文</span></p> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="760" src="https://images2.imgbox.com/16/5d/wkSYlqdn_o.png" width="1200"></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">总结一下其不足主要有：</span></p> 
  <ul><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">图像配准算法选择限制</span></span><span style="color:#333333;">：</span></li></ul> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">在图像配准时，缺乏指定算法的情况下，优先选择常见算法，如Threshold。这可能导致对于复杂和特殊图像任务处理能力的限制。</span></p> 
  <ul><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">时间关系推理困难</span></span><span style="color:#333333;">：</span></li></ul> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">GPT-4在理解和解释图像内容方面表现出色，但在推理多个图像之间的时间关系方面存在困难。</span></p> 
  <ul><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">视觉依赖型问题的限制</span></span><span style="color:#333333;">：</span></li></ul> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">GPT-4V在视觉问题类型中，对视觉依赖型问题的回答完全依赖于图像内容。缺乏图像信息时，其回答可能变得不确定或无法确切回答。</span></p> 
  <ul><li style="text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">领域泛化能力与“编造”事实问题</span></span><span style="color:#333333;">：</span></li></ul> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">尽管GPT-4具有出色的领域泛化能力，但在测试中可能出现“编造”事实的情况。这可能影响其在特定领域的可靠性和准确性。</span></p> 
  <h2 style="text-align:left;"><strong><span style="color:#1a1a1a;">三、垂直大模型</span></strong></h2> 
  <p style="margin-left:0;text-align:left;"></p> 
  <div> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">通过对GPT-4V和文档识别领域的深入分析和思考，为OCR文档识别领域的研究开辟了新的方向。需求不断增长的背景下，提高识别精度和处理效率成为了迫切需要满足的新应用标准。在这一背景下，出现了：</span></p> 
   <ul><li style="text-align:left;"><span style="color:#333333;">素级OCR统一模型</span></li><li style="text-align:left;"><span style="color:#333333;">OCR大一统模型</span></li><li style="text-align:left;"><span style="color:#333333;">文档识别分析+LLM（Language Model）</span></li></ul> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">等应用的新方向。</span></p> 
  </div> 
  <p></p> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="574" src="https://images2.imgbox.com/c4/4b/GIJGqQqg_o.png" width="1200"></p> 
  <h3 style="margin-left:0pt;text-align:left;"><strong><span style="color:#1a1a1a;">3.1 素级OCR统一模型</span></strong></h3> 
  <p style="margin-left:0;text-align:left;"></p> 
  <div> 
   <blockquote> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">素级OCR统一模型，即</span><span style="background-color:#92d050;"><span style="color:#333333;">UPOCR（Unified Pixel-Level OCR）</span></span><span style="color:#333333;">模型，是一种</span><span style="background-color:#ffc9c7;"><span style="color:#333333;">文档图像像素级多任务处理的统一模型</span></span><span style="color:#333333;">。该模型是由</span><span style="background-color:#99beff;"><span style="color:#333333;">合合信息与华南理工大学</span></span><span style="color:#333333;">联合实验室（以下简称“实验室”）研发的研究项目之一。</span></p> 
   </blockquote> 
  </div> 
  <p></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">UPOCR模型的主要特点是引入了可学习的文本检测和识别模块，可以同时完成多个任务，包括文本检测、文本识别、端到端OCR等。这一特性使得UPOCR模型在处理文档图像时具有较高的效率和准确性。</span></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">此外，UPOCR模型还具有较好的通用性，不仅可以处理中英文文档，还可以处理包含公式、表格等复杂结构的文档。</span></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">在实际应用中，UPOCR的通用性在文本擦除、文本分割和篡改文本检测任务中经过广泛验证。</span></p> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="593" src="https://images2.imgbox.com/52/08/3Zp33CQm_o.png" width="1200"></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">UPOCR采用ViTEraser[1]作为其主干网络，通过统一训练联合处理文本擦除、文本分割和篡改文本检测等三种不同任务的提示词。一经完成模型训练，即可无需专门的下游任务精调，直接用于各类下游任务。</span></p> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="586" src="https://images2.imgbox.com/8a/41/sNAxYTcG_o.png" width="1200"></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">总的来说，素级OCR统一模型UPOCR在文档图像预处理统一模型方面展现出了强大的实力和广泛的应用前景。</span></p> 
  <p style="margin-left:0;text-align:left;"></p> 
  <h3 style="margin-left:0pt;text-align:left;"><strong><span style="color:#1a1a1a;">3.2 OCR大一统模型-</span></strong><strong><span style="color:#1a1a1a;">SPTS v3</span></strong></h3> 
  <div> 
   <blockquote> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">OCR大一统模型是一种创新的端到端文本检测和识别方法，是实验室正在研究的项目之一，也称为SPTS（Simultaneous Processing of Text Spotting and Recognition）。</span></p> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">这种方法颠覆了传统的文本检测和识别流程。在传统的方法中，文本检测和识别被视为两个独立的任务，这导致处理流程复杂且冗余。然而，SPTS将这两个任务融为一体，实现了从文本检测到识别的统一处理。</span></p> 
   </blockquote> 
  </div> 
  <p></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">将文档图像识别分析的各项任务以序列预测的方式进行定义，包括对文本、段落、版面、表格、公式等内容的分析。通过采用不同的提示（prompt）来引导模型执行不同的OCR任务，实现了多任务处理的灵活性。该系统支持篇章级的文档图像识别分析，能够输出标准格式的文本，如Markdown、HTML等。</span></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">其中，通过引入LLM（Language Model）来处理文档理解相关的工作，进一步提高了系统对于文档结构和内容的理解能力。这种设计使得系统在处理多样性的文档图像时能够更全面、准确地进行分析，并以标准格式输出，为用户提供更便捷的文档处理和理解服务。</span></p> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="412" src="https://images2.imgbox.com/cd/da/Ar0CLpUJ_o.png" width="1200"></p> 
  <h4 style="margin-left:0pt;text-align:left;"><strong><span style="color:#1a1a1a;">SPTS v3 介绍</span></strong></h4> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="434" src="https://images2.imgbox.com/5f/27/EcGR8L4Y_o.png" width="1200"></p> 
  <ul><li style="text-align:left;"><span style="background-color:#ffc9c7;"><span style="color:#333333;">多任务序列预测</span></span><span style="color:#333333;">： SPTS v3通过将多种OCR任务抽象为序列预测问题，实现了对文本、段落、版面、表格、公式等不同元素的有序识别和分析。这种设计使得模型能够以一种统一的方式处理各种OCR任务。</span></li><li style="text-align:left;"><span style="background-color:#ffc9c7;"><span style="color:#333333;">Prompt引导</span></span><span style="color:#333333;">： SPTS v3采用了不同的prompt来引导模型完成不同的OCR任务。每个任务对应一个独特的提示，这样的引导机制使得模型在学习过程中能够专注于不同的目标，提高了系统的灵活性和适应性。</span></li><li style="text-align:left;"><span style="background-color:#ffc9c7;"><span style="color:#333333;">模型架构</span></span><span style="color:#333333;">： SPTS v3延续了SPTS的模型结构，包括了CNN（卷积神经网络）用于图像特征提取，以及Transformer Encoder和Transformer Decoder用于实现图像到序列的转换。这种结构旨在充分捕捉图像中的语义信息，同时具有较强的序列生成能力。</span></li><li style="text-align:left;"><span style="background-color:#ffc9c7;"><span style="color:#333333;">任务通用性</span></span><span style="color:#333333;">： 由于采用了序列预测的形式，SPTS v3在任务通用性上表现出色。这使得模型在不同场景和不同任务的OCR挑战中都能够取得良好的性能。</span></li></ul> 
  <p style="margin-left:0;text-align:left;"></p> 
  <blockquote> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">SPTSv3的任务定义，</span><span style="color:#333333;">目前主要关注以下任务：端到端检测识别、表格结构识别、手写数学公式识别</span></p> 
  </blockquote> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="740" src="https://images2.imgbox.com/6e/3d/z3gH25FI_o.png" width="1200"></p> 
  <p style="margin-left:0;text-align:left;"></p> 
  <h3 style="margin-left:0pt;text-align:left;"></h3> 
  <h3 style="margin-left:0pt;text-align:left;"><strong><span style="color:#1a1a1a;">3.3 文档识别分析+LLM</span></strong></h3> 
  <p style="margin-left:0;text-align:left;"></p> 
  <div> 
   <blockquote> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">文档识别分析与LLM（Large Language Model，大型语言模型）的结合是一种新兴的研究方向。LLM是一种基于深度学习的自然语言处理技术，旨在训练能够处理和生成自然语言文本的大型模型。其核心能力大致分为：生成、总结、提取、分类、检索与改写六部分。</span></p> 
   </blockquote> 
  </div> 
  <blockquote> 
   <p style="margin-left:0;text-align:left;"><span style="color:#333333;">在文档识别分析领域与LLM应用相结合方面，实验室提出了如下技术框架：首先，通过文档识别与版面分析技术，系统能够获取输入文档图像的关键信息。随后，对文档进行切分和召回操作，以便更精准地定位和检索所需信息。最终，利用LLM（Language Model）进行问答，进一步加强对文档内容的理解与交互。这一综合性的技术流程旨在提升文档识别与理解的整体效能</span></p> 
  </blockquote> 
  <p style="margin-left:0;text-align:left;"></p> 
  <p style="margin-left:0;text-align:left;"><img alt="" height="820" src="https://images2.imgbox.com/af/08/R96idHft_o.png" width="1200"></p> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">将文档识别技术与大型语言模型（LLM）相融合，为许多有前途的领域打开了大门，涉及到多个可能的应用和思考方向。以下是其中一些潜在的方向</span></p> 
  <ul><li style="text-align:left;"><span style="background-color:#ffc9c7;"><span style="color:#333333;">智能文档搜索与检索</span></span><span style="color:#333333;">：</span></li></ul> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">结合文档识别技术和LLM，可以建立更智能、语义理解的文档搜索引擎。用户可以通过自然语言提出问题，系统能够理解问题背后的语境并返回相关文档、段落或答案，提高文档检索的精度和效率。</span></p> 
  <ul><li style="text-align:left;"><span style="background-color:#ffc9c7;"><span style="color:#333333;">自动文档摘要生成：</span></span></li></ul> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;"> 利用LLM的文本生成能力，结合文档识别技术，可以实现自动文档摘要的生成。系统可以从文档中抽取关键信息，生成简明扼要的摘要，为用户提供更便捷的文档浏览和理解方式。</span></p> 
  <ul><li style="text-align:left;"><span style="background-color:#ffc9c7;"><span style="color:#333333;">多模态文档理解</span></span><span style="color:#333333;">： </span></li></ul> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;">结合文档识别技术和LLM，可以实现多模态文档理解，不仅包括文本信息的处理，还包括图像、表格等多种形式的内容。这样的系统可以更全面地理解和分析复杂的文档结构。</span></p> 
  <ul><li style="text-align:left;"><span style="background-color:#ffc9c7;"><span style="color:#333333;">定制化文档生成</span></span><span style="color:#333333;">：</span></li></ul> 
  <p style="margin-left:0;text-align:left;"><span style="color:#333333;"> 利用LLM的生成能力，结合文档识别技术，可以实现根据用户需求自动定制化文档的生成。系统可以从大量文档中筛选、整合信息，生成满足用户需求的文档，提高文档生成的效率和质量。</span></p> 
  <p style="margin-left:0;text-align:left;"></p> 
  <div> 
   <blockquote> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">这些方向展示了文档识别技术与LLM应用相结合的广阔前景，涉及到信息检索、自然语言理解、问答系统等多个领域，为提升文档处理和理解的智能化水平提供了丰富的可能性。</span></p> 
   </blockquote> 
  </div> 
  <p></p> 
  <p style="margin-left:0;text-align:left;"></p> 
  <h2 style="background-color:transparent;margin-left:0pt;text-align:left;"><strong><span style="color:#1a1a1a;">四、总结</span></strong></h2> 
  <div> 
   <blockquote> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">总的来说，以GPT-4V为代表的多模态大模型技术在文档识别与分析领域推动了技术的巨大进步，同时也为传统的智能文档处理（IDP）技术提出了一系列挑战。</span></p> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">尽管大模型在某些方面取得了显著的成就，</span><span style="background-color:#92d050;"><span style="color:#333333;">但并没有完全解决IDP领域所面临的所有问题</span></span><span style="color:#333333;">。</span></p> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">因此，我们</span><span style="background-color:#99ddff;"><span style="color:#333333;">需要深入研究，探索如何更好地结合大模型的能力来解决IDP的问题</span></span><span style="color:#333333;">。这个领域仍然值得我们做更多的思考和探索，以找到创新性的解决方案。</span></p> 
   </blockquote> 
   <blockquote> 
    <ul><li style="margin-left:0px;text-align:left;"><span style="color:#333333;">合合信息</span></li></ul> 
    <p style="margin-left:0;text-align:left;"><span style="background-color:#92d050;"><span style="color:#333333;">合合信息专注于智能文档处理领域的技术研究</span></span><span style="color:#333333;">，这些研究成果已经集成到合合 TextIn 智能文字识别产品中。</span></p> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">合合信息基于智能文档处理技术，向全球用户和企业提供他们的研究成果。如果有需要可以访问 </span><u><span style="color:#1e6fff;"><a class="link-info" href="https://www.textin.com/" rel="nofollow" title="textin.com">textin.com</a></span></u><span style="color:#333333;">，轻松体验一站式的智能文字识别服务。</span></p> 
   </blockquote> 
   <blockquote> 
    <ul><li style="margin-left:0px;text-align:left;"><span style="color:#333333;">问卷抽奖</span></li></ul> 
    <p style="margin-left:0;text-align:left;"><span style="color:#333333;">另外，</span><span style="color:#4d4d4d;">大家可填写下方问卷参与抽奖，合合信息将抽 10 人送出 50 元京东卡（12 号开奖）。<a class="link-info" href="https://qywx.wjx.cn/vm/exOhu6f.aspx" rel="nofollow" title="问卷抽奖">问卷抽奖</a></span></p> 
   </blockquote> 
  </div> 
  <p></p> 
  <p style="margin-left:0;text-align:left;"></p> 
 </div> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8e1f7e952ba645f36902f35d0ba94d1f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">贝叶斯算法的故事丨机器学习一文解读</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1f1f34b504499e9a46e760266b5b80ac/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">解决 rasa 中 slot 不能为中文的问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>