<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用Batch Normalization折叠来加速模型推理 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/1ff21010ca259a82d7759e1536097bcb/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="使用Batch Normalization折叠来加速模型推理">
  <meta property="og:description" content="作者：Nathan Hubens
编译：ronghuaiyang
来自：AI公园
导读
如何去掉batch normalization层来加速神经网络。
介绍 Batch Normalization是将各层的输入进行归一化，使训练过程更快、更稳定的一种技术。在实践中，它是一个额外的层，我们通常添加在计算层之后，在非线性之前。它包括两个步骤：
首先减去其平均值，然后除以其标准差
进一步通过γ缩放，通过β偏移，这些是batch normalization层的参数，当网络不需要数据的时候，均值为0、标准差为1。
Batch normalization在神经网络的训练中具有较高的效率，因此得到了广泛的应用。但它在推理的时候有多少用处呢？
一旦训练结束，每个Batch normalization层都拥有一组特定的γ和β，还有μ和σ，后者在训练过程中使用指数加权平均值进行计算。这意味着在推理过程中，Batch normalization就像是对上一层（通常是卷积）的结果进行简单的线性转换。
由于卷积也是一个线性变换，这也意味着这两个操作可以合并成一个单一的线性变换！这将删除一些不必要的参数，但也会减少推理时要执行的操作数量。
在实践中怎么做？ 用一点数学知识，我们可以很容易地重新对卷积进行排列来处理batch normalization。提醒一下，对一个输入x进行卷积之后再进行batch normalization的运算可以表示为：
那么，如果我们重新排列卷积的W和b，考虑batch normalization的参数，如下：
我们可以去掉batch normalization层，仍然得到相同的结果！
注意：通常，在batch normalization层之前的层中是没有bias的，因为这是无用的，也是对参数的浪费，因为任何常数都会被batch normalization抵消掉。
这样做的效果怎样？ 我们将尝试两种常见的架构：
使用batch norm的VGG16
ResNet50
为了演示，我们使用ImageNet dataset和PyTorch。两个网络都将训练5个epoch，看看参数数量和推理时间的变化。
1. VGG16 我们从训练VGG16 5个epoch开始(最终的准确性并不重要)：
参数的数量：
单个图像的初始推理时间为：
如果使用了batch normalization折叠，我们有：
以及：
8448个参数被去掉了，更好的是，几乎快了0.4毫秒！最重要的是，这是完全无损的，在性能方面绝对没有变化：
让我们看看它在Resnet50的情况下是怎么样的！
2. Resnet50 同样的，我们开始训练它5个epochs：
初始参数量为：
推理时间为：
使用batch normalization折叠后，有：
和：
现在，我们有26,560的参数被移除，更惊讶的hi，推理时间减少了1.5ms，性能一点也没降。
英文原文：https://towardsdatascience.com/speed-up-inference-with-batch-normalization-folding-8a45a83a89d8
下载一：中文版！学习TensorFlow、PyTorch、机器学习、深度学习和数据结构五件套！ 后台回复【五件套】 下载二：南大模式识别PPT 后台回复【南大模式识别】 说个正事哈
由于微信平台算法改版，公号内容将不再以时间排序展示，如果大家想第一时间看到我们的推送，强烈建议星标我们和给我们多点点【在看】。星标具体步骤为：
（1）点击页面最上方“深度学习自然语言处理”，进入公众号主页。
（2）点击右上角的小点点，在弹出页面点击“设为星标”，就可以啦。
感谢支持，比心。
投稿或交流学习，备注：昵称-学校（公司）-方向，进入DL&amp;amp;NLP交流群。
方向有很多：机器学习、深度学习，python，情感分析、意见挖掘、句法分析、机器翻译、人机对话、知识图谱、语音识别等。">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2020-11-16T21:15:10+08:00">
    <meta property="article:modified_time" content="2020-11-16T21:15:10+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用Batch Normalization折叠来加速模型推理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p>作者：Nathan Hubens</p> 
 <p>编译：ronghuaiyang</p> 
 <p>来自：AI公园<br></p> 
 <p><strong>导读</strong></p> 
 <p>如何去掉batch normalization层来加速神经网络。</p> 
 <h3><strong>介绍</strong></h3> 
 <p>Batch Normalization是将各层的输入进行归一化，使训练过程更快、更稳定的一种技术。在实践中，它是一个额外的层，我们通常添加在计算层之后，在非线性之前。它包括两个步骤：</p> 
 <ul><li><p>首先减去其平均值，然后除以其标准差</p></li><li><p>进一步通过γ缩放，通过β偏移，这些是batch normalization层的参数，当网络不需要数据的时候，均值为0、标准差为1。</p></li></ul> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/14/d3/Nnaz0O8s_o.png"></p> 
 <p>Batch normalization在神经网络的训练中具有较高的效率，因此得到了广泛的应用。但它在推理的时候有多少用处呢？<br></p> 
 <p>一旦训练结束，每个Batch normalization层都拥有一组特定的γ和β，还有μ和σ，后者在训练过程中使用指数加权平均值进行计算。这意味着在推理过程中，Batch normalization就像是对上一层（通常是卷积）的结果进行简单的线性转换。</p> 
 <p>由于卷积也是一个线性变换，这也意味着这两个操作可以合并成一个单一的线性变换！这将删除一些不必要的参数，但也会减少推理时要执行的操作数量。</p> 
 <h3>在实践中怎么做？</h3> 
 <p>用一点数学知识，我们可以很容易地重新对卷积进行排列来处理batch normalization。提醒一下，对一个输入<em>x</em>进行卷积之后再进行batch normalization的运算可以表示为：</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/dc/88/O7QDuZps_o.png"></p> 
 <p>那么，如果我们重新排列卷积的<strong>W</strong>和<strong>b</strong>，考虑batch normalization的参数，如下：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/b6/52/sRtVlgsQ_o.png"></p> 
 <p>我们可以去掉batch normalization层，仍然得到相同的结果！<br></p> 
 <blockquote> 
  <p><strong>注意</strong>：通常，在batch normalization层之前的层中是没有bias的，因为这是无用的，也是对参数的浪费，因为任何常数都会被batch normalization抵消掉。</p> 
 </blockquote> 
 <h3>这样做的效果怎样？</h3> 
 <p>我们将尝试两种常见的架构：</p> 
 <ul><li><p>使用batch norm的VGG16</p></li><li><p>ResNet50</p></li></ul> 
 <p>为了演示，我们使用ImageNet dataset和PyTorch。两个网络都将训练5个epoch，看看参数数量和推理时间的变化。</p> 
 <h3>1. VGG16</h3> 
 <p>我们从训练VGG16 5个epoch开始(最终的准确性并不重要)：</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/3c/9c/YCQlVLCY_o.png"></p> 
 <p>参数的数量：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/4b/8d/4eTcWQoQ_o.png"></p> 
 <p>单个图像的初始推理时间为：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/c3/10/nNnfBy0m_o.png"></p> 
 <p>如果使用了batch normalization折叠，我们有：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/d4/ba/JsQ5B7sV_o.png"></p> 
 <p>以及：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/aa/1a/R65usZEa_o.png"></p> 
 <p>8448个参数被去掉了，更好的是，几乎快了0.4毫秒！最重要的是，这是完全无损的，在性能方面绝对没有变化：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/68/0e/PosQxb6S_o.png"></p> 
 <p>让我们看看它在Resnet50的情况下是怎么样的！<br></p> 
 <h3>2. Resnet50</h3> 
 <p>同样的，我们开始训练它5个epochs：</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/30/90/2rfuSbI4_o.png"></p> 
 <p>初始参数量为：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/94/04/eZEipOHj_o.png"></p> 
 <p>推理时间为：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/a5/98/cduZgkUT_o.png"></p> 
 <p>使用batch normalization折叠后，有：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/75/e3/VKFmjHPC_o.png"></p> 
 <p>和：<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/e1/9a/qA27OvzF_o.png"></p> 
 <p>现在，我们有26,560的参数被移除，更惊讶的hi，推理时间减少了1.5ms，性能一点也没降。<br></p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/5d/5d/Mi2i3aSP_o.png"></p> 
 <p>英文原文：https://towardsdatascience.com/speed-up-inference-with-batch-normalization-folding-8a45a83a89d8</p> 
 <pre class="has"><code class="language-php">下载一：中文版！学习TensorFlow、PyTorch、机器学习、深度学习和数据结构五件套！

后台回复【五件套】
下载二：南大模式识别PPT

后台回复【南大模式识别】
</code></pre> 
 <p><strong>说个正事哈</strong><br></p> 
 <p>由于微信平台算法改版，公号内容将不再以时间排序展示，如果大家想第一时间看到我们的推送，强烈建议星标我们和给我们多点点【在看】。星标具体步骤为：</p> 
 <p>（1）点击页面<strong>最上方</strong>“<strong>深度学习自然语言处理</strong>”，进入公众号主页。</p> 
 <p>（2）点击<strong>右上角的小点点</strong>，在弹出页面点击“<strong>设为星标</strong>”，就可以啦。<br></p> 
 <p>感谢支持，比心<img src="https://images2.imgbox.com/93/e2/3a0UvrYG_o.png">。</p> 
 <p>投稿或交流学习，备注：<strong>昵称-学校（公司）-方向</strong>，进入DL&amp;NLP交流群。<br></p> 
 <p>方向有很多：机器学习、深度学习，python，情感分析、意见挖掘、句法分析、机器翻译、人机对话、知识图谱、语音识别等。</p> 
 <p><img src="https://images2.imgbox.com/91/53/cNslPMPB_o.png"></p> 
 <p>记得备注呦</p> 
 <p><strong>推荐两个专辑给大家：</strong><strong></strong></p> 
 <p>专辑 | <a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzI3ODgwODA2MA%3D%3D&amp;action=getalbum&amp;album_id=1409615437079642112#wechat_redirect" rel="nofollow">李宏毅人类语言处理2020笔记</a></p> 
 <p><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzI3ODgwODA2MA%3D%3D&amp;action=getalbum&amp;album_id=1339674581853962241#wechat_redirect" rel="nofollow">专辑 | NLP论文解读</a></p> 
 <p><a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzI3ODgwODA2MA%3D%3D&amp;action=getalbum&amp;album_id=1431373214395269120#wechat_redirect" rel="nofollow">专辑 | 情感分析</a><br></p> 
 <pre class="has"><code class="language-php">
整理不易，还望给个在看！
</code></pre> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fa58d13250a2649f173a0ae1ca0b3955/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">8个常用的wx小程序 UI 组件库，社区、电商、工具各类都有！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b8c37b59d0f104013432885391ea0a5b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">电脑无线网络显示红叉_电脑连不上网络了 网络出现一个红叉，怎么办？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>