<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习算法（2）—— 线性回归算法 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/9fb82b2295923b0a4bcf833a0c84932a/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="机器学习算法（2）—— 线性回归算法">
  <meta property="og:description" content="线性回归算法 1 线性回归简介2 线性回归的初步使用3 损失函数4 优化算法4.1 正规方程4.2 梯度下降4.3 优化方法比较4.4 线性回归api再介绍 5 欠拟合与过拟合5.1 欠拟合5.2 过拟合 6 正则化线性模型6.1 Ridge Regression 岭回归6.2 Lasso 回归6.3 Elastic Net 弹性网络6.4 Early stopping 7 模型的保存与加载 1 线性回归简介 线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。
h代表学习算法的解决方案或函数，也称为假设（hypothesis），h(x)代表预测的值 注意：
只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型。线性回归当中主要有两种模型，一种是线性关系，另一种是非线性关系 2 线性回归的初步使用 线性回归API
sklearn.linear_model.LinearRegression()
LinearRegression.coef_：回归系数 &#39;&#39;&#39;导入模块&#39;&#39;&#39; from sklearn.linear_model import LinearRegression &#39;&#39;&#39;构造数据集&#39;&#39;&#39; x = [[80, 86], [82, 80], [85, 78], [90, 90], [86, 82], [82, 90], [78, 80], [92, 94]] y = [84.2, 80.6, 80.1, 90, 83.">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-12-15T00:06:17+08:00">
    <meta property="article:modified_time" content="2022-12-15T00:06:17+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习算法（2）—— 线性回归算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>线性回归算法</h4> 
 <ul><li><a href="#1__1" rel="nofollow">1 线性回归简介</a></li><li><a href="#2__14" rel="nofollow">2 线性回归的初步使用</a></li><li><a href="#3__48" rel="nofollow">3 损失函数</a></li><li><a href="#4__65" rel="nofollow">4 优化算法</a></li><li><ul><li><a href="#41__71" rel="nofollow">4.1 正规方程</a></li><li><a href="#42__93" rel="nofollow">4.2 梯度下降</a></li><li><a href="#43__175" rel="nofollow">4.3 优化方法比较</a></li><li><a href="#44_api_192" rel="nofollow">4.4 线性回归api再介绍</a></li></ul> 
  </li><li><a href="#5__290" rel="nofollow">5 欠拟合与过拟合</a></li><li><ul><li><a href="#51__292" rel="nofollow">5.1 欠拟合</a></li><li><a href="#52__301" rel="nofollow">5.2 过拟合</a></li></ul> 
  </li><li><a href="#6__330" rel="nofollow">6 正则化线性模型</a></li><li><ul><li><a href="#61_Ridge_Regression__331" rel="nofollow">6.1 Ridge Regression 岭回归</a></li><li><a href="#62_Lasso__388" rel="nofollow">6.2 Lasso 回归</a></li><li><a href="#63_Elastic_Net__396" rel="nofollow">6.3 Elastic Net 弹性网络</a></li><li><a href="#64_Early_stopping_409" rel="nofollow">6.4 Early stopping</a></li></ul> 
  </li><li><a href="#7__412" rel="nofollow">7 模型的保存与加载</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1__1"></a>1 线性回归简介</h2> 
<p>线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间关系进行建模的一种分析方式。</p> 
<p><img src="https://images2.imgbox.com/d8/cf/viVmcl51_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <ul><li>h代表学习算法的解决方案或函数，也称为假设（hypothesis），h(x)代表预测的值</li></ul> 
</blockquote> 
<blockquote> 
 <p>注意：</p> 
 <ul><li>只有一个自变量的情况称为<strong>单变量回归</strong>，多于一个自变量情况的叫做<strong>多元回归</strong></li><li>特征值与目标值之间建立了一个关系，这个关系可以理解为<strong>线性模型</strong>。</li><li>线性回归当中主要有两种模型，一种是<strong>线性关系</strong>，另一种是<strong>非线性关系</strong></li></ul> 
</blockquote> 
<h2><a id="2__14"></a>2 线性回归的初步使用</h2> 
<p>线性回归API</p> 
<p><code>sklearn.linear_model.LinearRegression()</code></p> 
<ul><li><code>LinearRegression.coef_</code>：回归系数</li></ul> 
<p><img src="https://images2.imgbox.com/91/fc/SmWUrkcQ_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">'''导入模块'''</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression

<span class="token triple-quoted-string string">'''构造数据集'''</span>
x <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">86</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
	 <span class="token punctuation">[</span><span class="token number">82</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
	 <span class="token punctuation">[</span><span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">78</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
	 <span class="token punctuation">[</span><span class="token number">90</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
	 <span class="token punctuation">[</span><span class="token number">86</span><span class="token punctuation">,</span> <span class="token number">82</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
	 <span class="token punctuation">[</span><span class="token number">82</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
	 <span class="token punctuation">[</span><span class="token number">78</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
	 <span class="token punctuation">[</span><span class="token number">92</span><span class="token punctuation">,</span> <span class="token number">94</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">84.2</span><span class="token punctuation">,</span> <span class="token number">80.6</span><span class="token punctuation">,</span> <span class="token number">80.1</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">,</span> <span class="token number">83.2</span><span class="token punctuation">,</span> <span class="token number">87.6</span><span class="token punctuation">,</span> <span class="token number">79.4</span><span class="token punctuation">,</span> <span class="token number">93.4</span><span class="token punctuation">]</span>

<span class="token triple-quoted-string string">'''模型训练'''</span>
<span class="token comment"># 实例化一个估计器</span>
estimator <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 使用fit方法进行训练</span>
estimator<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
<span class="token comment"># 查看回归系数值</span>
coef <span class="token operator">=</span> estimator<span class="token punctuation">.</span>coef_
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"系数是：\n"</span><span class="token punctuation">,</span>coef<span class="token punctuation">)</span> <span class="token comment"># [0.3 0.7]</span>
<span class="token comment"># 预测值</span>
prediction <span class="token operator">=</span> estimator<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测值是：\n"</span><span class="token punctuation">,</span>prediction<span class="token punctuation">)</span> <span class="token comment"># [86.]</span>
</code></pre> 
<h2><a id="3__48"></a>3 损失函数</h2> 
<p>为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。<strong>损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数</strong></p> 
<p><img src="https://images2.imgbox.com/3c/3f/aYGJ6cha_o.png" alt="在这里插入图片描述"><br> 如上图所示，真实结果与我们预测的结果之间存在一定的误差，而这个误差（损失）可以计算出来：</p> 
<p><img src="https://images2.imgbox.com/e6/c2/JwnJZ5AG_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>注意：</p> 
 <ul><li> <p>yi为第i个训练样本的真实值</p> </li><li> <p>h(xi)为第i个训练样本特征值组合预测函数</p> </li><li> <p>又称<strong>最小二乘法</strong></p> </li><li> <p>损失函数（Loss Function）度量单样本预测的错误程度，损失函数值越小，模型就越好。</p> </li><li> <p>代价函数（Cost Function）度量全部样本集的平均误差。</p> </li><li> <p>目标函数（Object Function）代价函数和正则化函数，最终要优化的函数。</p> </li></ul> 
</blockquote> 
<h2><a id="4__65"></a>4 优化算法</h2> 
<p>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</p> 
<p>线性回归经常使用的两种优化算法</p> 
<ul><li>正规方程</li><li>梯度下降法</li></ul> 
<h3><a id="41__71"></a>4.1 正规方程</h3> 
<ol><li> <p>什么是正规方程<br> <img src="https://images2.imgbox.com/b2/10/vubSi7nm_o.png" alt="在这里插入图片描述"></p> </li><li> <p>正规矩阵求解</p> </li></ol> 
<p>把损失函数转换成矩阵写法：<br> <img src="https://images2.imgbox.com/c6/bd/gjn6HltL_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>其中y是真实值矩阵，X是特征值矩阵，w是权重矩阵</p> 
</blockquote> 
<p>对其求解关于w（<strong>w为自变量</strong>）的最小值，导数为零的位置，即为损失的最小值<br> <img src="https://images2.imgbox.com/ce/e5/IKCuEckT_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>注意：</p> 
 <ul><li>式(1)到式(2)推导过程中,X是一个m行n列的矩阵，并不能保证其有逆矩阵，但是<strong>右乘X的转置XT把其变成一个方阵</strong>，<strong>保证其有逆矩阵</strong>。式（5）到式（6）推导过程中，和上类似。（面试官可能让你手推公式）</li></ul> 
</blockquote> 
<p>即正规方程为：<br> <img src="https://images2.imgbox.com/46/ba/z85p1Osh_o.png" alt="在这里插入图片描述"></p> 
<ul><li>X为特征值矩阵，y为目标值矩阵。直接求到最好的结果</li><li>当特征过多过复杂时，求解速度太慢并且得不到结果</li></ul> 
<h3><a id="42__93"></a>4.2 梯度下降</h3> 
<ol><li>梯度下降</li></ol> 
<p>梯度下降法的基本思想可以类比为一个下山的过程。</p> 
<p>假设这样一个场景：一个人被困在山上，需要从山上下来(找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，（同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走）。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</p> 
<p><img src="https://images2.imgbox.com/49/6e/G4NtgKhZ_o.png" alt="在这里插入图片描述"></p> 
<p>梯度是微积分中一个很重要的概念</p> 
<ul><li>​ 在<strong>单变量</strong>的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的<strong>切线</strong>的斜率。</li><li>​ 在<strong>多变量</strong>函数中，梯度是一个<strong>向量</strong>，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着<strong>梯度的反方向</strong>（α为负的原因）一直走，就能走到局部的最低点！</li></ul> 
<ol start="2"><li>梯度下降公式</li></ol> 
<p>梯度下降公式（Gradient Descent）<br> <img src="https://images2.imgbox.com/e5/7f/OrcIGuaE_o.png" alt="在这里插入图片描述"><br> <strong>注意：</strong><br> α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点。</p> 
<p>所以有了梯度下降这样一个优化算法，回归就有了"<strong>自动学习</strong>"的能力</p> 
<ol start="3"><li>梯度下降举例</li></ol> 
<p>（1） 单变量函数的梯度下降</p> 
<p>我们假设有一个单变量的函数 :J(θ) = θ²<br> 函数的微分:J’(θ) = 2θ<br> 初始化，起点为： θº = 1<br> 学习率：α = 0.4<br> 我们开始进行梯度下降的迭代计算过程:<br> <img src="https://images2.imgbox.com/11/c1/WN1poUGy_o.png" alt="在这里插入图片描述"></p> 
<p>如下图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底</p> 
<p><img src="https://images2.imgbox.com/ff/04/mAiKVgfX_o.png" alt="在这里插入图片描述"><br> （2）多变量函数的梯度下降</p> 
<p>我们假设有一个目标函数 ：:J(θ) = θ₁² + θ₂²<br> 现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下 来，我们会从梯度下降算法开始一步步计算到这个最小值! 我们假设初始的起点为: θº = (1, 3)<br> 初始的学习率为:α = 0.1<br> 函数的梯度为:▽:J(θ) =&lt; 2θ₁ ,2θ₂&gt;</p> 
<p>进行多次迭代:<br> <img src="https://images2.imgbox.com/cb/9a/UADy26Rq_o.png" alt="在这里插入图片描述"><br> 我们发现，已经基本靠近函数的最小值点<br> <img src="https://images2.imgbox.com/84/06/IWKlj40W_o.png" alt="在这里插入图片描述"><br> 4. 梯度下降算法</p> 
<p>常见的梯度下降算法有：</p> 
<ul><li>全梯度下降算法(Full gradient descent），</li><li>随机梯度下降算法（Stochastic gradient descent），</li><li>随机平均梯度下降算法（Stochastic average gradient descent）</li><li>小批量梯度下降算法（Mini-batch gradient descent）,</li></ul> 
<p>它们都是为了正确地调节权重向量，通过为每个权重计算一个梯度，从而更新权值，使目标函数尽可能最小化。其差别在于样本的使用方式不同。</p> 
<p>（1）全梯度下降算法（FG）</p> 
<p><img src="https://images2.imgbox.com/34/b6/oU2n8cD6_o.png" alt="在这里插入图片描述"><br> （2）随机梯度下降算法（SG）</p> 
<p><img src="https://images2.imgbox.com/a5/c5/RU5y1OaV_o.png" alt="在这里插入图片描述"><br> （3）小批量梯度下降算法（mini-bantch）</p> 
<p><img src="https://images2.imgbox.com/3f/19/Gz7ZQv1F_o.png" alt="在这里插入图片描述"><br> （4）随机平均梯度下降算法（SAG）<br> <img src="https://images2.imgbox.com/dd/f5/GDs43Sed_o.png" alt="在这里插入图片描述"><br> （5）算法比较</p> 
<p>以下6幅图反映了模型优化过程中四种梯度算法的性能差异。<br> <img src="https://images2.imgbox.com/e6/71/As4SAFPH_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/58/d3/JsGf5cRI_o.png" alt="在这里插入图片描述"><br> 结论：</p> 
<ul><li>FG方法由于它每轮更新都要使用全体数据集，故花费的时间成本最多，内存存储最大。</li><li>SAG在训练初期表现不佳，优化速度较慢。这是因为我们常将初始梯度设为0，而SAG每轮梯度更新都结合了上一轮梯度值。</li><li>综合考虑迭代次数和运行时间，SG表现性能都很好，能在训练初期快速摆脱初始梯度值，快速将平均损失函数降到很低。但要注意，在使用SG方法时要慎重选择步长，否则容易错过最优解。</li><li>mini-batch结合了SG的“胆大”和FG的“心细”，从6幅图像来看，它的表现也正好居于SG和FG二者之间。在目前的机器学习领域，mini-batch是使用最多的梯度下降算法，正是因为它避开了FG运算效率低成本大和SG收敛效果不稳定的缺点。</li></ul> 
<h3><a id="43__175"></a>4.3 优化方法比较</h3> 
<p><img src="https://images2.imgbox.com/f7/81/jnUpPgem_o.png" alt="在这里插入图片描述"></p> 
<ol><li>梯度下降要设置α并不保证一次能获得最优的α，正规方程不用考虑α。</li><li>梯度下降要迭代多次，正规方程不用。（所以，遇到比较简单的情况，可用正规方程）</li><li>梯度下降最后总能得到一个最优结果，正规方程不一定。因为<strong>正规方程要求X的转置乘X的结果可逆</strong>。</li><li>当特征数量很多的时候，正规方程计算不方便，不如梯度下降。</li></ol> 
<p>算法选择依据：</p> 
<ul><li>小规模数据： 
  <ul><li>正规方程：LinearRegression(不能解决拟合问题)</li><li>岭回归</li></ul> </li><li>大规模数据： 
  <ul><li>梯度下降法： SGDRegressor</li></ul> </li></ul> 
<h3><a id="44_api_192"></a>4.4 线性回归api再介绍</h3> 
<p>数据集介绍</p> 
<p><img src="https://images2.imgbox.com/ba/29/X8XFqcLE_o.png" alt="在这里插入图片描述"></p> 
<p><strong>（1）线性回归:正规方程</strong></p> 
<p><code>sklearn.linear_model.LinearRegression(fit_intercept=True)</code></p> 
<ul><li><code>fit_intercept</code>：是否计算偏置</li><li><code>LinearRegression.coef_</code>：回归系数（y=kx+b中的 k）</li><li><code>LinearRegression.intercept_</code>：偏置（y=kx+b中的 b）</li></ul> 
<p><strong>回归模型评估</strong></p> 
<p><img src="https://images2.imgbox.com/03/51/ohtBEe7d_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_boston
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error

<span class="token triple-quoted-string string">'''获取数据集'''</span>
data <span class="token operator">=</span> load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''划分数据集'''</span>
x_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">.</span>data<span class="token punctuation">,</span> data<span class="token punctuation">.</span>target<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''特征工程：数据标准化'''</span>
transfer <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train <span class="token operator">=</span> transfer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
x_test <span class="token operator">=</span> transfer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''机器学习：线性回归（正规方程）'''</span>
estimator <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
estimator<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''模型评估'''</span>
y_predict <span class="token operator">=</span> estimator<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测值为："</span><span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"系数值为："</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"偏置值为："</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>intercept_<span class="token punctuation">)</span>
error <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"均方误差为："</span><span class="token punctuation">,</span> error<span class="token punctuation">)</span>
</code></pre> 
<p><strong>（2）线性回归:梯度下降法</strong></p> 
<p>SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。</p> 
<p><code>sklearn.linear_model.SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)</code></p> 
<p>参数：</p> 
<ul><li><code>loss</code>:损失类型 
  <ul><li><code>loss=”squared_loss”</code>: 普通最小二乘法</li></ul> </li><li><code>fit_intercept</code>：是否计算偏置</li><li><code>learning_rate</code> : 学习率填充， string类型，optional 
  <ul><li><code>'constant'</code>: eta = eta0 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。</li><li><code>'optimal'</code>: eta = 1.0 / (alpha * (t + t0)) [default]</li><li><code>'invscaling'</code>: eta = eta0 / pow(t, power_t)</li></ul> </li></ul> 
<p>属性：</p> 
<ul><li><code>SGDRegressor.coef_</code>：回归系数</li><li><code>SGDRegressor.intercept_</code>：偏置</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_boston
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error

<span class="token triple-quoted-string string">'''获取数据集'''</span>
data <span class="token operator">=</span> load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''划分数据集'''</span>
x_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">.</span>data<span class="token punctuation">,</span> data<span class="token punctuation">.</span>target<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''特征工程：数据标准化'''</span>
transfer <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train <span class="token operator">=</span> transfer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
x_test <span class="token operator">=</span> transfer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''机器学习-线性回归(特征方程)'''</span>
estimator <span class="token operator">=</span> SGDRegressor<span class="token punctuation">(</span>max_iter<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>
estimator<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''模型评估'''</span>
y_predict <span class="token operator">=</span> estimator<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测值为:\n"</span><span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"模型中的系数为:\n"</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"模型中的偏置为:\n"</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>intercept_<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''评价'''</span>
error <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"均方误差为:\n"</span><span class="token punctuation">,</span> error<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="5__290"></a>5 欠拟合与过拟合</h2> 
<p><img src="https://images2.imgbox.com/6f/f4/dN9Y4cz1_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="51__292"></a>5.1 欠拟合</h3> 
<p><strong>概念</strong>：一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</p> 
<p><strong>原因</strong>：学习到数据的特征过少</p> 
<p><strong>解决办法</strong>：</p> 
<ul><li>添加其他特征项</li><li>添加多项式特征</li></ul> 
<h3><a id="52__301"></a>5.2 过拟合</h3> 
<p><strong>概念</strong>：一个假设在训练数据上能够获得更好的拟合， 但是在测试数据集上却不能很好地拟合数据，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</p> 
<p><strong>原因</strong>：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点</p> 
<p><strong>解决办法</strong>：</p> 
<ul><li>重新清洗数据</li><li>增大数据的训练量</li><li><strong>正则化</strong></li><li>减少特征维度，防止<strong>维灾难</strong></li></ul> 
<p><strong>正则化</strong><br> <img src="https://images2.imgbox.com/2f/54/qhAXpr0Q_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/0b/b5/TwJmBBWI_o.png" alt="在这里插入图片描述"><br> 在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是<strong>正则化</strong></p> 
<p>L1正则化</p> 
<ul><li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li><li>LASSO回归</li></ul> 
<p>L2正则化</p> 
<ul><li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li><li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li><li>Ridge回归</li></ul> 
<p><strong>维灾难</strong><br> <img src="https://images2.imgbox.com/7d/cd/sDuE0oSN_o.png" alt="在这里插入图片描述"><br> 随着维度的增加，分类器性能逐步上升，到达某点之后，其性能便逐渐下降。</p> 
<h2><a id="6__330"></a>6 正则化线性模型</h2> 
<h3><a id="61_Ridge_Regression__331"></a>6.1 Ridge Regression 岭回归</h3> 
<p>岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项：<img src="https://images2.imgbox.com/95/83/2hdYeTVU_o.png" alt="在这里插入图片描述"><br> 以达到在拟合数据的同时，使模型权重尽可能小的目的,岭回归代价函数:<br> <img src="https://images2.imgbox.com/cf/55/804P6fbZ_o.png" alt="在这里插入图片描述"><br> <strong>注意：</strong></p> 
<ul><li>α=0时，岭回归退化为线性回归</li><li>就是把系数添加平方项，然后限制系数值的大小，α值越小，系数值越大，α越大，系数值越小</li></ul> 
<p><code>sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)</code></p> 
<ul><li>具有l2正则化的线性回归</li></ul> 
<p>参数：</p> 
<ul><li><code>alpha</code>:正则化力度，也叫 λ，λ取值：0~1 或者 1~10（正则化力度越大，权重系数越小，反之，越大）</li><li><code>solver</code>:会根据数据自动选择优化方法，sag:如果数据集、特征都比较大，选择该随机梯度下降优化</li><li><code>normalize</code>:数据是否进行标准化，normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</li></ul> 
<p>属性：</p> 
<ul><li><code>Ridge.coef_</code>:回归权重</li><li><code>Ridge.intercept_</code>:回归偏置</li></ul> 
<p>Ridge方法相当于SGDRegressor(penalty=‘l2’, loss=“squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</p> 
<p><code>sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)</code></p> 
<ul><li>具有l2正则化的线性回归，可以进行交叉验证</li><li><code>coef_</code>:回归系数</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_boston
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> Ridge<span class="token punctuation">,</span> RidgeCV
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error
<span class="token keyword">import</span> joblib

<span class="token triple-quoted-string string">'''获取数据集'''</span>
data <span class="token operator">=</span> load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''划分数据集'''</span>
x_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">.</span>data<span class="token punctuation">,</span> data<span class="token punctuation">.</span>target<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''特征工程：数据标准化'''</span>
transfer <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
x_train <span class="token operator">=</span> transfer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
x_test <span class="token operator">=</span> transfer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''机器学习：岭回归'''</span>
<span class="token comment"># estimator = Ridge()</span>
estimator <span class="token operator">=</span> RidgeCV<span class="token punctuation">(</span>alphas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
estimator<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">'''模型评估'''</span>
y_predict <span class="token operator">=</span> estimator<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测值为："</span><span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"系数值为："</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"偏置值为："</span><span class="token punctuation">,</span> estimator<span class="token punctuation">.</span>intercept_<span class="token punctuation">)</span>
error <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_predict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"均方误差为："</span><span class="token punctuation">,</span> error<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="62_Lasso__388"></a>6.2 Lasso 回归</h3> 
<p>Lasso回归的代价函数 ：<br> <img src="https://images2.imgbox.com/da/2b/ZthuSIsO_o.png" alt="在这里插入图片描述">Lasso Regression 的代价函数在 θi=0处不可导，可以在θi=0处用一个<strong>次梯度向量</strong>(subgradient vector)代替梯度：<br> <img src="https://images2.imgbox.com/36/5a/pVuMDkSz_o.png" alt="在这里插入图片描述"><br> <strong>注意：</strong></p> 
<ul><li>Lasso Regression 的代价函数在 θi=0处是不可导的</li><li>倾向于完全消除不重要的权重</li><li>对系数值进行绝对值处理，由于绝对值在顶点处不可导，所以进行计算的过程中产生很多0，最后得到结果为：稀疏矩阵</li></ul> 
<h3><a id="63_Elastic_Net__396"></a>6.3 Elastic Net 弹性网络</h3> 
<p>弹性网络的代价函数 ：<br> <img src="https://images2.imgbox.com/9b/45/TDFG45xt_o.png" alt="在这里插入图片描述"><br> 弹性网络在岭回归和Lasso回归中进行了折中，通过 混合比(mix ratio) r 进行控制：</p> 
<ul><li>r=0：弹性网络变为岭回归</li><li>r=1：弹性网络便为Lasso回归</li></ul> 
<p><strong>注意：</strong></p> 
<ul><li>常用：岭回归</li><li>假设只有少部分特征是有用的： 
  <ul><li>弹性网络</li><li>Lasso</li><li>一般来说，弹性网络的使用更为广泛。因为在特征维度高于训练样本数，或者特征是强相关的情况下，Lasso回归的表现不太稳定。</li></ul> </li></ul> 
<h3><a id="64_Early_stopping_409"></a>6.4 Early stopping</h3> 
<p>Early Stopping 也是正则化迭代学习的方法之一。<br> 其做法为：在验证错误率达到最小值的时候停止训练。</p> 
<h2><a id="7__412"></a>7 模型的保存与加载</h2> 
<p><code>from sklearn.externals import joblib</code></p> 
<ul><li>保存：<code>joblib.dump(estimator, 'test.pkl')</code></li><li>加载：<code>estimator = joblib.load('test.pkl')</code></li></ul> 
<p>注意：</p> 
<ul><li>保存文件，后缀名是**.pkl</li><li>加载模型是需要通过一个变量进行承接</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5ec93325166f407d6f03981bfa566ebc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C语言入门第七讲（应用二：水仙花数）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c767ea3e030930d19a259c7c48755450/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Flutter跳转到第三方地图</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>