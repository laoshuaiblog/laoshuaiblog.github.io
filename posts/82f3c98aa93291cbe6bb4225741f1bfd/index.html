<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【论文&#43;综述&#43;视觉换衣】视觉虚拟换衣调研：StableVITON、OutfitAnyone、TryOnDiffusion、HR-VITON - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/82f3c98aa93291cbe6bb4225741f1bfd/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="【论文&#43;综述&#43;视觉换衣】视觉虚拟换衣调研：StableVITON、OutfitAnyone、TryOnDiffusion、HR-VITON">
  <meta property="og:description" content="23.12.StableVITON (韩国科学技术院 KAIST)：基于潜在扩散模型的虚拟试穿语义对应学习：Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On
23.12.OutfitAnyone（阿里未开源）: 超高质量的虚拟试穿，适合任何服装和任何人： Outfit Anyone: Ultra-high quality virtual try-on for Any Clothing and Any Person 23.06.TryOnDiffusion: （Google未开源）基于diffusion的试穿模型： 2个Unet网络的故事 (阿里着重参考)
A Tale of Two UNets：项目地址 | 论文
22.11 Paint by Example: 将换衣作为去除衣服后的填充方法 Exemplar-based Image Editing with Diffusion Models
22.06 HR-VITON：(韩国科学技术院开源) High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions | code
21.03 VITON-HD: (韩国科学技术院开源) High-Resolution Virtual Try-On via Misalignment-Aware Normalization | code">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-01T19:00:31+08:00">
    <meta property="article:modified_time" content="2024-02-01T19:00:31+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文&#43;综述&#43;视觉换衣】视觉虚拟换衣调研：StableVITON、OutfitAnyone、TryOnDiffusion、HR-VITON</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/97/b7/3kwqHCRF_o.png" alt="在这里插入图片描述"></p> 
<p>23.12.<code>StableVITON</code> (韩国科学技术院 KAIST)：基于潜在扩散模型的虚拟试穿语义对应学习：<a href="https://rlawjdghek.github.io/StableVITON/" rel="nofollow">Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On</a><br> 23.12.<code>OutfitAnyone</code>（阿里未开源）: 超高质量的虚拟试穿，适合任何服装和任何人： <a href="https://humanaigc.github.io/outfit-anyone/" rel="nofollow">Outfit Anyone: Ultra-high quality virtual try-on for Any Clothing and Any Person </a><br> 23.06.<code>TryOnDiffusion</code>: （Google未开源）基于diffusion的试穿模型： 2个Unet网络的故事 (阿里<strong>着重参考</strong>)<br> A Tale of Two UNets：<a href="https://tryondiffusion.github.io/" rel="nofollow">项目地址</a> | <a href="https://readpaper.com/paper/1798206297393674240" rel="nofollow">论文</a><br> 22.11 <code>Paint by Example</code>: 将换衣作为去除衣服后的填充方法 <a href="https://github.com/Fantasy-Studio/Paint-by-Example">Exemplar-based Image Editing with Diffusion Models</a><br> 22.06 <code>HR-VITON</code>：(韩国科学技术院<strong>开源</strong>) <a href="https://koo616.github.io/HR-VITON/" rel="nofollow">High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions</a> | <a href="https://github.com/sangyun884/HR-VITON">code</a><br> 21.03 <code>VITON-HD</code>: (韩国科学技术院<strong>开源</strong>) <a href="https://psh01087.github.io/VITON-HD/" rel="nofollow">High-Resolution Virtual Try-On via Misalignment-Aware Normalization</a> | <a href="https://github.com/shadow2496/VITON-HD">code</a></p> 
<p>VITON-HD<code>数据集</code>下载：<a href="https://opendatalab.com/OpenDataLab/VITON-HD" rel="nofollow">https://opendatalab.com/OpenDataLab/VITON-HD</a><br> 产品—沃尔玛的虚拟衣服试穿：<a href="https://www.walmart.com/cp/virtual-try-on/4879497" rel="nofollow">https://www.walmart.com/cp/virtual-try-on/4879497</a></p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_17" rel="nofollow">从最新的论文到旧的，更能理解一些参数意义与发展</a></li><li><ul><li><a href="#2312_StableVITON__22" rel="nofollow">23.12 StableVITON (半开源，模型需要申请，没有训练代码)</a></li><li><ul><li><a href="#_26" rel="nofollow">简介</a></li><li><a href="#_33" rel="nofollow">方法</a></li></ul> 
   </li><li><a href="#2312_OutfitAnyone_httpshumanaigcgithubiooutfitanyone_40" rel="nofollow">23.12 OutfitAnyone ([未开源、无论文):](https://humanaigc.github.io/outfit-anyone/)</a></li><li><ul><li><a href="#outfitanyone__55" rel="nofollow">outfit-anyone 部分方法概述</a></li><li><a href="#__63" rel="nofollow">实测 （可上传图片，自动提取到衣服）</a></li><li><ul><li><a href="#_67" rel="nofollow">有遮挡的衣服测试(无法还原衣服)</a></li></ul> 
   </li></ul> 
   </li><li><a href="#2306_TryOnDiffusionhttpstryondiffusiongithubio__httpsreadpapercompaper1798206297393674240_72" rel="nofollow">2306. TryOnDiffusion（未开源）[项目地址](https://tryondiffusion.github.io/) | [论文](https://readpaper.com/paper/1798206297393674240)</a></li><li><ul><li><a href="#_78" rel="nofollow">方法</a></li></ul> 
   </li><li><a href="#2206__HRVITON_88" rel="nofollow">22.06 `HR-VITON`：错位和遮挡条件下高分辨率虚拟试穿</a></li><li><ul><li><a href="#_93" rel="nofollow">方法概述</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_17"></a>从最新的论文到旧的，更能理解一些参数意义与发展</h2> 
<p>23.12stable-Viton与其他方法的对比<br> <img src="https://images2.imgbox.com/cc/2a/hRK6l6JL_o.png" alt="在这里插入图片描述"><br> 23.06 TryOnDiffusion与其他效果对比<br> <img src="https://images2.imgbox.com/cc/6c/uzNrfLEe_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2312_StableVITON__22"></a>23.12 StableVITON (半开源，模型需要申请，没有训练代码)</h3> 
<p><a href="https://rlawjdghek.github.io/StableVITON/" rel="nofollow">项目主页</a> | <a href="https://arxiv.org/pdf/2312.01725.pdf" rel="nofollow">论文</a> | 代码<br> Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On</p> 
<h4><a id="_26"></a>简介</h4> 
<p>换衣结果：<br> <a href="https://opendatalab.com/OpenDataLab/VITON-HD" rel="nofollow">VITON-HD 数据集</a>中的图片(第一行（以换衣后人物为一行）)、<br> SHHQ-1.0(第二行的前两幅图像)和<strong>网络爬取图像</strong>(第二行的最后两个图像)。<br> 所有结果都是使用在<code>VITON-HD数据集</code>上训练的StableVITON生成的。<br> <img src="https://images2.imgbox.com/ee/d3/ro7b3Q2d_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_33"></a>方法</h4> 
<p>本文将虚拟试穿作为一个基于范例(exemplar)的<code>图像修复</code>（image inpainting）问题（用需要换的衣服来填充去掉衣服的人）, 灵感来源于： <a href="https://readpaper.com/paper/4693335992405344257" rel="nofollow">22.11 Paint by Example: Exemplar-based Image Editing with Diffusion Models</a></p> 
<p>对于虚拟试穿任务，<code>StableVITON</code>还采用三个条件（通过预处理得到）：不可知衣服的人（agnostic map(消除需要换装人的任何服装信息)）、不可知掩码（mask）和密集分割图（dense pose），作为预训练U-Net的输入，U-Net用作交叉注意力的查询向量（Q）。服装的特征图被用作交叉注意力的关键点（K）和值（V），并以UNet为条件，如（b）所示。</p> 
<p>为了保持服装的精细细节，我们引入了空间编码器（spatial encoder,图中可训练的SD encoder），对服装进行编码，并通过zero-cross-attention 作为条件与预训练preprain相连<br> <img src="https://images2.imgbox.com/11/1b/N78aNSOM_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2312_OutfitAnyone_httpshumanaigcgithubiooutfitanyone_40"></a>23.12 OutfitAnyone (<a href="https://humanaigc.github.io/outfit-anyone/" rel="nofollow">未开源、无论文):</a></h3> 
<p><code>在线运行</code>：<a href="https://modelscope.cn/studios/DAMOXR/OutfitAnyone/summary" rel="nofollow">https://modelscope.cn/studios/DAMOXR/OutfitAnyone/summary</a><br> <img src="https://images2.imgbox.com/54/33/7qEzd1OE_o.gif" alt="在这里插入图片描述"></p> 
<p>Outfit Anyone: Ultra-high quality virtual try-on for Any Clothing and Any Person<br> 超高质量的虚拟试穿，适合任何服装和任何人</p> 
<p>现有方法往往难以生成高保真(high-fidelity)和细节一致的（detail-consistent）结果。</p> 
<p>扩散模型已经证明了其生成高质量逼真图像的能力，但在虚拟试穿等条件生成场景中，它们在实现控制和一致性（control and consistency）方面仍面临挑战。</p> 
<p>Outfit Anyone 利用双流条件扩散模型(two-stream conditional diffusion model)解决了这些局限性，使其能够巧妙地处理服装变形(garment deformation)，从而获得更逼真的效果。</p> 
<p>它的与众不同之处在于可扩展性–可调节<code>姿势和体形</code>等因素（modulating factors such as pose and body shape）–以及广泛的适用性，从动漫到现实场景图像</p> 
<h4><a id="outfitanyone__55"></a>outfit-anyone 部分方法概述</h4> 
<p>条件扩散模型是本方法的核心，使用服装图像（garment image）作为控制因素，处理模特、服装和附带文本提示的图像。在内部，该网络分为<code>两个流</code>，用于独立处理模特（试穿者，model）和服装数据。这些流汇聚在一个融合网络中，该融合网络有助于将服装细节嵌入到模特（试穿者）的特征表示中。<br> <img src="https://images2.imgbox.com/92/13/xzCwai6n_o.png" alt="在这里插入图片描述"><br> 训练完成后的推理<br> <img src="https://images2.imgbox.com/1d/4f/kQ5bJ1Is_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="__63"></a>实测 （可上传图片，自动提取到衣服）</h4> 
<p>在线测试：<a href="https://modelscope.cn/studios/DAMOXR/OutfitAnyone/summary" rel="nofollow">https://modelscope.cn/studios/DAMOXR/OutfitAnyone/summary</a><br> <img src="https://images2.imgbox.com/da/c3/zpwjkwpI_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="_67"></a>有遮挡的衣服测试(无法还原衣服)</h5> 
<p><img src="https://images2.imgbox.com/26/99/PgvOsQNz_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2306_TryOnDiffusionhttpstryondiffusiongithubio__httpsreadpapercompaper1798206297393674240_72"></a>2306. TryOnDiffusion（未开源）<a href="https://tryondiffusion.github.io/" rel="nofollow">项目地址</a> | <a href="https://readpaper.com/paper/1798206297393674240" rel="nofollow">论文</a></h3> 
<p>生成具有<code>显着身体形状和姿势变化、</code>(body shape and pose )、大遮挡（large occlusions）的服装试穿结果，同时保留 <code>1024×1024</code> 分辨率的服装细节。<br> 输入显示在生成结构右上角（需要换装的人，已经穿着衣服的另一个人）<br> 并在姿势和身材变化大时，对衣服的图片和纹理褶皱进行相应变化（当姿势或身体形状差异很大时，服装需要以根据新的形状或遮挡创建或展平皱纹的方式扭曲。）<br> <img src="https://images2.imgbox.com/8e/ee/7HPef6Yu_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_78"></a>方法</h4> 
<p>在<code>400万的图像对</code>进行训练。<br> 采用了2个并行Unet（<code>Parallel-UNet</code>）网络：通过交叉注意进行通信<br> 服装通过<code>交叉注意机制</code>隐式（implicitly）变形（以适应新的身体和角度），<br> 目标人和源服装是通过<code>多尺度特征</code>交叉注意它们的来实现的，这允许建立远程对应。</p> 
<ol><li>在<code>预处理步骤</code>中，目标人物（去除原始衣服后）处理成 “<code>服装不可知RGB”图像</code>（“clothing agnostic RGB” ）目标<code>服装</code>从服装图像中<code>分割</code>出来，并对原始换衣人和服装图像计算姿态。</li><li>这些输入被视为128×128 Parallel-UNet(注意力机制中的key)，以创建128×128试穿图像，这些图像进一步作为输入发送到256×256 Parallel-UNet，以及试穿条件输入。</li><li>256×256 Parallel-UNet的输出被发送到<code>标准超分辨率扩散模型</code>以创建1024×1024图像(为了在1024×1024分辨率下生成高质量的我们遵循<code>Imagen</code>:<a href="https://readpaper.com/paper/688890531776978944" rel="nofollow">22.03.Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a>)</li></ol> 
<p><img src="https://images2.imgbox.com/04/b5/gYGJkXJy_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2206__HRVITON_88"></a>22.06 <code>HR-VITON</code>：错位和遮挡条件下高分辨率虚拟试穿</h3> 
<p>(韩国科学技术院<strong>开源</strong>) <a href="https://koo616.github.io/HR-VITON/" rel="nofollow">High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions</a> | <a href="https://github.com/sangyun884/HR-VITON">code</a></p> 
<p><img src="https://images2.imgbox.com/97/bf/yukrpFar_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="_93"></a>方法概述</h4> 
<p>总框架<br> <img src="https://images2.imgbox.com/ac/db/cPDEnw5y_o.png" alt="在这里插入图片描述"></p> 
<p>生成器详细的结构<br> <img src="https://images2.imgbox.com/1e/06/q3fjJD2r_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/849a8c2ac5594e6ccc1087bc9dc3d66d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">常用十大爬虫软件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dfbcfaf1914adf54d76e1ef3b260ec7d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">关于STM32 HAL库 （I2C/IIC）问题的解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>