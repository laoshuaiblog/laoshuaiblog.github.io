<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人工智能基础 作业5 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/4e6820c5b53e3ddb5175fb2f25872505/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="人工智能基础 作业5">
  <meta property="og:description" content="For循环版本：手工实现 卷积-池化-激活 import numpy as np x = np.array([[-1, -1, -1, -1, -1, -1, -1, -1, -1], [-1, 1, -1, -1, -1, -1, -1, 1, -1], [-1, -1, 1, -1, -1, -1, 1, -1, -1], [-1, -1, -1, 1, -1, 1, -1, -1, -1], [-1, -1, -1, -1, 1, -1, -1, -1, -1], [-1, -1, -1, 1, -1, 1, -1, -1, -1], [-1, -1, 1, -1, -1, -1, 1, -1, -1], [-1, 1, -1, -1, -1, -1, -1, 1, -1], [-1, -1, -1, -1, -1, -1, -1, -1, -1]]) print(&#34;">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-05-22T00:31:43+08:00">
    <meta property="article:modified_time" content="2022-05-22T00:31:43+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人工智能基础 作业5</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>For循环版本：手工实现 卷积-池化-激活</h2> 
<pre><code class="language-python">import numpy as np
 
x = np.array([[-1, -1, -1, -1, -1, -1, -1, -1, -1],
              [-1, 1, -1, -1, -1, -1, -1, 1, -1],
              [-1, -1, 1, -1, -1, -1, 1, -1, -1],
              [-1, -1, -1, 1, -1, 1, -1, -1, -1],
              [-1, -1, -1, -1, 1, -1, -1, -1, -1],
              [-1, -1, -1, 1, -1, 1, -1, -1, -1],
              [-1, -1, 1, -1, -1, -1, 1, -1, -1],
              [-1, 1, -1, -1, -1, -1, -1, 1, -1],
              [-1, -1, -1, -1, -1, -1, -1, -1, -1]])
print("x=\n", x)
# 初始化 三个 卷积核
Kernel = [[0 for i in range(0, 3)] for j in range(0, 3)]
Kernel[0] = np.array([[1, -1, -1],
                      [-1, 1, -1],
                      [-1, -1, 1]])
Kernel[1] = np.array([[1, -1, 1],
                      [-1, 1, -1],
                      [1, -1, 1]])
Kernel[2] = np.array([[-1, -1, 1],
                      [-1, 1, -1],
                      [1, -1, -1]])
 
# --------------- 卷积  ---------------
stride = 1  # 步长
feature_map_h = 7  # 特征图的高
feature_map_w = 7  # 特征图的宽
feature_map = [0 for i in range(0, 3)]  # 初始化3个特征图
for i in range(0, 3):
    feature_map[i] = np.zeros((feature_map_h, feature_map_w))  # 初始化特征图
for h in range(feature_map_h):  # 向下滑动，得到卷积后的固定行
    for w in range(feature_map_w):  # 向右滑动，得到卷积后的固定行的列
        v_start = h * stride  # 滑动窗口的起始行（高）
        v_end = v_start + 3  # 滑动窗口的结束行（高）
        h_start = w * stride  # 滑动窗口的起始列（宽）
        h_end = h_start + 3  # 滑动窗口的结束列（宽）
        window = x[v_start:v_end, h_start:h_end]  # 从图切出一个滑动窗口
        for i in range(0, 3):
            feature_map[i][h, w] = np.divide(np.sum(np.multiply(window, Kernel[i][:, :])), 9)
print("feature_map:\n", np.around(feature_map, decimals=2))
 
# --------------- 池化  ---------------
pooling_stride = 2  # 步长
pooling_h = 4  # 特征图的高
pooling_w = 4  # 特征图的宽
feature_map_pad_0 = [[0 for i in range(0, 8)] for j in range(0, 8)]
for i in range(0, 3):  # 特征图 补 0 ，行 列 都要加 1 (因为上一层是奇数，池化窗口用的偶数)
    feature_map_pad_0[i] = np.pad(feature_map[i], ((0, 1), (0, 1)), 'constant', constant_values=(0, 0))
# print("feature_map_pad_0 0:\n", np.around(feature_map_pad_0[0], decimals=2))
 
pooling = [0 for i in range(0, 3)]
for i in range(0, 3):
    pooling[i] = np.zeros((pooling_h, pooling_w))  # 初始化特征图
for h in range(pooling_h):  # 向下滑动，得到卷积后的固定行
    for w in range(pooling_w):  # 向右滑动，得到卷积后的固定行的列
        v_start = h * pooling_stride  # 滑动窗口的起始行（高）
        v_end = v_start + 2  # 滑动窗口的结束行（高）
        h_start = w * pooling_stride  # 滑动窗口的起始列（宽）
        h_end = h_start + 2  # 滑动窗口的结束列（宽）
        for i in range(0, 3):
            pooling[i][h, w] = np.max(feature_map_pad_0[i][v_start:v_end, h_start:h_end])
print("pooling:\n", np.around(pooling[0], decimals=2))
print("pooling:\n", np.around(pooling[1], decimals=2))
print("pooling:\n", np.around(pooling[2], decimals=2))
 
 
# --------------- 激活  ---------------
def relu(x):
    return (abs(x) + x) / 2
 
 
relu_map_h = 7  # 特征图的高
relu_map_w = 7  # 特征图的宽
relu_map = [0 for i in range(0, 3)]  # 初始化3个特征图
for i in range(0, 3):
    relu_map[i] = np.zeros((relu_map_h, relu_map_w))  # 初始化特征图
 
for i in range(0, 3):
    relu_map[i] = relu(feature_map[i])
 
print("relu map :\n",np.around(relu_map[0], decimals=2))
print("relu map :\n",np.around(relu_map[1], decimals=2))
print("relu map :\n",np.around(relu_map[2], decimals=2))</code></pre> 
<p>输出</p> 
<blockquote> 
 <p>x=<br>  [[-1 -1 -1 -1 -1 -1 -1 -1 -1]<br>  [-1  1 -1 -1 -1 -1 -1  1 -1]<br>  [-1 -1  1 -1 -1 -1  1 -1 -1]<br>  [-1 -1 -1  1 -1  1 -1 -1 -1]<br>  [-1 -1 -1 -1  1 -1 -1 -1 -1]<br>  [-1 -1 -1  1 -1  1 -1 -1 -1]<br>  [-1 -1  1 -1 -1 -1  1 -1 -1]<br>  [-1  1 -1 -1 -1 -1 -1  1 -1]<br>  [-1 -1 -1 -1 -1 -1 -1 -1 -1]]<br> feature_map:<br>  [[[ 0.78 -0.11  0.11  0.33  0.56 -0.11  0.33]  [-0.11  1.   -0.11  0.33 -0.11  0.11 -0.11] <br>   [ 0.11 -0.11  1.   -0.33  0.11 -0.11  0.56] <br>   [ 0.33  0.33 -0.33  0.56 -0.33  0.33  0.33] <br>   [ 0.56 -0.11  0.11 -0.33  1.   -0.11  0.11] <br>   [-0.11  0.11 -0.11  0.33 -0.11  1.   -0.11] <br>   [ 0.33 -0.11  0.56  0.33  0.11 -0.11  0.78]]</p> 
 <p> [[ 0.33 -0.56  0.11 -0.11  0.11 -0.56  0.33] <br>   [-0.56  0.56 -0.56  0.33 -0.56  0.56 -0.56] <br>   [ 0.11 -0.56  0.56 -0.78  0.56 -0.56  0.11] <br>   [-0.11  0.33 -0.78  1.   -0.78  0.33 -0.11] <br>   [ 0.11 -0.56  0.56 -0.78  0.56 -0.56  0.11] <br>   [-0.56  0.56 -0.56  0.33 -0.56  0.56 -0.56] <br>   [ 0.33 -0.56  0.11 -0.11  0.11 -0.56  0.33]]</p> 
 <p> [[ 0.33 -0.11  0.56  0.33  0.11 -0.11  0.78] <br>   [-0.11  0.11 -0.11  0.33 -0.11  1.   -0.11] <br>   [ 0.56 -0.11  0.11 -0.33  1.   -0.11  0.11] <br>   [ 0.33  0.33 -0.33  0.56 -0.33  0.33  0.33] <br>   [ 0.11 -0.11  1.   -0.33  0.11 -0.11  0.56] <br>   [-0.11  1.   -0.11  0.33 -0.11  0.11 -0.11] <br>   [ 0.78 -0.11  0.11  0.33  0.56 -0.11  0.33]]]<br> pooling:<br>  [[1.   0.33 0.56 0.33]<br>  [0.33 1.   0.33 0.56]<br>  [0.56 0.33 1.   0.11]<br>  [0.33 0.56 0.11 0.78]]<br> pooling:<br>  [[0.56 0.33 0.56 0.33]<br>  [0.33 1.   0.56 0.11]<br>  [0.56 0.56 0.56 0.11]<br>  [0.33 0.11 0.11 0.33]]<br> pooling:<br>  [[0.33 0.56 1.   0.78]<br>  [0.56 0.56 1.   0.33]<br>  [1.   1.   0.11 0.56]<br>  [0.78 0.33 0.56 0.33]]<br> relu map :<br>  [[0.78 0.   0.11 0.33 0.56 0.   0.33]<br>  [0.   1.   0.   0.33 0.   0.11 0.  ]<br>  [0.11 0.   1.   0.   0.11 0.   0.56]<br>  [0.33 0.33 0.   0.56 0.   0.33 0.33]<br>  [0.56 0.   0.11 0.   1.   0.   0.11]<br>  [0.   0.11 0.   0.33 0.   1.   0.  ]<br>  [0.33 0.   0.56 0.33 0.11 0.   0.78]]<br> relu map :<br>  [[0.33 0.   0.11 0.   0.11 0.   0.33]<br>  [0.   0.56 0.   0.33 0.   0.56 0.  ]<br>  [0.11 0.   0.56 0.   0.56 0.   0.11]<br>  [0.   0.33 0.   1.   0.   0.33 0.  ]<br>  [0.11 0.   0.56 0.   0.56 0.   0.11]<br>  [0.   0.56 0.   0.33 0.   0.56 0.  ]<br>  [0.33 0.   0.11 0.   0.11 0.   0.33]]<br> relu map :<br>  [[0.33 0.   0.56 0.33 0.11 0.   0.78]<br>  [0.   0.11 0.   0.33 0.   1.   0.  ]<br>  [0.56 0.   0.11 0.   1.   0.   0.11]<br>  [0.33 0.33 0.   0.56 0.   0.33 0.33]<br>  [0.11 0.   1.   0.   0.11 0.   0.56]<br>  [0.   1.   0.   0.33 0.   0.11 0.  ]<br>  [0.78 0.   0.11 0.33 0.56 0.   0.33]]</p> 
 <p> </p> 
</blockquote> 
<h2>Pytorch版本：调用函数完成 卷积-池化-激活</h2> 
<pre><code class="language-python"># https://blog.csdn.net/qq_26369907/article/details/88366147
# https://zhuanlan.zhihu.com/p/405242579
import numpy as np
import torch
import torch.nn as nn
 
x = torch.tensor([[[[-1, -1, -1, -1, -1, -1, -1, -1, -1],
                    [-1, 1, -1, -1, -1, -1, -1, 1, -1],
                    [-1, -1, 1, -1, -1, -1, 1, -1, -1],
                    [-1, -1, -1, 1, -1, 1, -1, -1, -1],
                    [-1, -1, -1, -1, 1, -1, -1, -1, -1],
                    [-1, -1, -1, 1, -1, 1, -1, -1, -1],
                    [-1, -1, 1, -1, -1, -1, 1, -1, -1],
                    [-1, 1, -1, -1, -1, -1, -1, 1, -1],
                    [-1, -1, -1, -1, -1, -1, -1, -1, -1]]]], dtype=torch.float)
print(x.shape)
print(x)
 
print("--------------- 卷积  ---------------")
conv1 = nn.Conv2d(1, 1, (3, 3), 1)  # in_channel , out_channel , kennel_size , stride
conv1.weight.data = torch.Tensor([[[[1, -1, -1],
                                    [-1, 1, -1],
                                    [-1, -1, 1]]
                                   ]])
conv2 = nn.Conv2d(1, 1, (3, 3), 1)  # in_channel , out_channel , kennel_size , stride
conv2.weight.data = torch.Tensor([[[[1, -1, 1],
                                    [-1, 1, -1],
                                    [1, -1, 1]]
                                   ]])
conv3 = nn.Conv2d(1, 1, (3, 3), 1)  # in_channel , out_channel , kennel_size , stride
conv3.weight.data = torch.Tensor([[[[-1, -1, 1],
                                    [-1, 1, -1],
                                    [1, -1, -1]]
                                   ]])
 
feature_map1 = conv1(x)
feature_map2 = conv2(x)
feature_map3 = conv3(x)
 
print(feature_map1 / 9)
print(feature_map2 / 9)
print(feature_map3 / 9)
 
print("--------------- 池化  ---------------")
max_pool = nn.MaxPool2d(2, padding=0, stride=2)  # Pooling
zeroPad = nn.ZeroPad2d(padding=(0, 1, 0, 1))  # pad 0 , Left Right Up Down
 
feature_map_pad_0_1 = zeroPad(feature_map1)
feature_pool_1 = max_pool(feature_map_pad_0_1)
feature_map_pad_0_2 = zeroPad(feature_map2)
feature_pool_2 = max_pool(feature_map_pad_0_2)
feature_map_pad_0_3 = zeroPad(feature_map3)
feature_pool_3 = max_pool(feature_map_pad_0_3)
 
print(feature_pool_1.size())
print(feature_pool_1 / 9)
print(feature_pool_2 / 9)
print(feature_pool_3 / 9)
 
print("--------------- 激活  ---------------")
activation_function = nn.ReLU()
 
feature_relu1 = activation_function(feature_map1)
feature_relu2 = activation_function(feature_map2)
feature_relu3 = activation_function(feature_map3)
print(feature_relu1 / 9)
print(feature_relu2 / 9)
print(feature_relu3 / 9)
 </code></pre> 
<p>输出 </p> 
<blockquote> 
 <p>torch.Size([1, 1, 9, 9])<br> tensor([[[[-1., -1., -1., -1., -1., -1., -1., -1.<br>           [-1.,  1., -1., -1., -1., -1., -1.,  1.<br>           [-1., -1.,  1., -1., -1., -1.,  1., -1.<br>           [-1., -1., -1.,  1., -1.,  1., -1., -1.<br>           [-1., -1., -1., -1.,  1., -1., -1., -1.<br>           [-1., -1., -1.,  1., -1.,  1., -1., -1.<br>           [-1., -1.,  1., -1., -1., -1.,  1., -1.<br>           [-1.,  1., -1., -1., -1., -1., -1.,  1.<br>           [-1., -1., -1., -1., -1., -1., -1., -1.<br> --------------- 卷积  ---------------<br> tensor([[[[ 0.7886, -0.1003,  0.1219,  0.3441,  0<br>           [-0.1003,  1.0108, -0.1003,  0.3441, -0<br>           [ 0.1219, -0.1003,  1.0108, -0.3225,  0<br>           [ 0.3441,  0.3441, -0.3225,  0.5664, -0<br>           [ 0.5664, -0.1003,  0.1219, -0.3225,  1<br>           [-0.1003,  0.1219, -0.1003,  0.3441, -0<br>           [ 0.3441, -0.1003,  0.5664,  0.3441,  0<br>        grad_fn=&lt;DivBackward0&gt;)<br> tensor([[[[ 0.3346, -0.5543,  0.1124, -0.1099,  0<br>           [-0.5543,  0.5568, -0.5543,  0.3346, -0<br>           [ 0.1124, -0.5543,  0.5568, -0.7765,  0<br>           [-0.1099,  0.3346, -0.7765,  1.0013, -0<br>           [ 0.1124, -0.5543,  0.5568, -0.7765,  0<br>           [-0.5543,  0.5568, -0.5543,  0.3346, -0<br>           [ 0.3346, -0.5543,  0.1124, -0.1099,  0<br>        grad_fn=&lt;DivBackward0&gt;)<br> tensor([[[[ 0.3137, -0.1308,  0.5359,  0.3137,  0<br>           [-0.1308,  0.0914, -0.1308,  0.3137, -0<br>           [ 0.5359, -0.1308,  0.0914, -0.3530,  0<br>           [ 0.3137,  0.3137, -0.3530,  0.5359, -0<br>           [ 0.0914, -0.1308,  0.9803, -0.3530,  0<br>           [-0.1308,  0.9803, -0.1308,  0.3137, -0<br>           [ 0.7581, -0.1308,  0.0914,  0.3137,  0<br>        grad_fn=&lt;DivBackward0&gt;)<br> --------------- 池化  ---------------<br> torch.Size([1, 1, 4, 4])<br> tensor([[[[1.0108, 0.3441, 0.5664, 0.3441],<br>           [0.3441, 1.0108, 0.3441, 0.5664],<br>           [0.5664, 0.3441, 1.0108, 0.1219],<br>           [0.3441, 0.0000, 0.5664, 0.3441, 0.1219, 0.0000, 0.7886]]]],  <br>        grad_fn=&lt;DivBackward0&gt;)<br> tensor([[[[0.3346, 0.0000, 0.1124, 0.0000, 0.1124, 0.0000, 0.3346],     <br>           [0.0000, 0.5568, 0.0000, 0.3346, 0.0000, 0.5568, 0.0000],     <br>           [0.1124, 0.0000, 0.5568, 0.0000, 0.5568, 0.0000, 0.1124],     <br>           [0.0000, 0.3346, 0.0000, 1.0013, 0.0000, 0.3346, 0.0000],     <br>           [0.1124, 0.0000, 0.5568, 0.0000, 0.5568, 0.0000, 0.1124],     <br>           [0.0000, 0.5568, 0.0000, 0.3346, 0.0000, 0.5568, 0.0000],     <br>           [0.3346, 0.0000, 0.1124, 0.0000, 0.1124, 0.0000, 0.3346]]]],  <br>        grad_fn=&lt;DivBackward0&gt;)<br> tensor([[[[0.3137, 0.0000, 0.5359, 0.3137, 0.0914, 0.0000, 0.7581],     <br>           [0.0000, 0.0914, 0.0000, 0.3137, 0.0000, 0.9803, 0.0000],     <br>           [0.5359, 0.0000, 0.0914, 0.0000, 0.9803, 0.0000, 0.0914],     <br>           [0.3137, 0.3137, 0.0000, 0.5359, 0.0000, 0.3137, 0.3137],     <br>           [0.0914, 0.0000, 0.9803, 0.0000, 0.0914, 0.0000, 0.5359],     <br>           [0.0000, 0.9803, 0.0000, 0.3137, 0.0000, 0.0914, 0.0000],     <br>           [0.7581, 0.0000, 0.0914, 0.3137, 0.5359, 0.0000, 0.3137]]]],  <br>        grad_fn=&lt;DivBackward0&gt;)<br>  </p> 
</blockquote> 
<h2>可视化：了解数字与图像之间的关系</h2> 
<pre><code class="language-python"># https://blog.csdn.net/qq_26369907/article/details/88366147
# https://zhuanlan.zhihu.com/p/405242579
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False #用来正常显示负号 #有中文出现的情况，需要u'内容
x = torch.tensor([[[[-1, -1, -1, -1, -1, -1, -1, -1, -1],
                    [-1, 1, -1, -1, -1, -1, -1, 1, -1],
                    [-1, -1, 1, -1, -1, -1, 1, -1, -1],
                    [-1, -1, -1, 1, -1, 1, -1, -1, -1],
                    [-1, -1, -1, -1, 1, -1, -1, -1, -1],
                    [-1, -1, -1, 1, -1, 1, -1, -1, -1],
                    [-1, -1, 1, -1, -1, -1, 1, -1, -1],
                    [-1, 1, -1, -1, -1, -1, -1, 1, -1],
                    [-1, -1, -1, -1, -1, -1, -1, -1, -1]]]], dtype=torch.float)
print(x.shape)
print(x)
img = x.data.squeeze().numpy()  # 将输出转换为图片的格式
plt.imshow(img, cmap='gray')
plt.title('原图')
plt.show()
 
print("--------------- 卷积  ---------------")
conv1 = nn.Conv2d(1, 1, (3, 3), 1)  # in_channel , out_channel , kennel_size , stride
conv1.weight.data = torch.Tensor([[[[1, -1, -1],
                                    [-1, 1, -1],
                                    [-1, -1, 1]]
                                   ]])
img = conv1.weight.data.squeeze().numpy()  # 将输出转换为图片的格式
plt.imshow(img, cmap='gray')
plt.title('Kernel 1')
plt.show()
conv2 = nn.Conv2d(1, 1, (3, 3), 1)  # in_channel , out_channel , kennel_size , stride
conv2.weight.data = torch.Tensor([[[[1, -1, 1],
                                    [-1, 1, -1],
                                    [1, -1, 1]]
                                   ]])
img = conv2.weight.data.squeeze().numpy()  # 将输出转换为图片的格式
plt.imshow(img, cmap='gray')
plt.title('Kernel 2')
plt.show()
conv3 = nn.Conv2d(1, 1, (3, 3), 1)  # in_channel , out_channel , kennel_size , stride
conv3.weight.data = torch.Tensor([[[[-1, -1, 1],
                                    [-1, 1, -1],
                                    [1, -1, -1]]
                                   ]])
img = conv3.weight.data.squeeze().numpy()  # 将输出转换为图片的格式
plt.imshow(img, cmap='gray')
plt.title('Kernel 3')
plt.show()
 
feature_map1 = conv1(x)
feature_map2 = conv2(x)
feature_map3 = conv3(x)
 
print(feature_map1 / 9)
print(feature_map2 / 9)
print(feature_map3 / 9)
 
img = feature_map1.data.squeeze().numpy()  # 将输出转换为图片的格式
plt.imshow(img, cmap='gray')
plt.title('卷积后的特征图1')
plt.show()
 
print("--------------- 池化  ---------------")
max_pool = nn.MaxPool2d(2, padding=0, stride=2)  # Pooling
zeroPad = nn.ZeroPad2d(padding=(0, 1, 0, 1))  # pad 0 , Left Right Up Down
 
feature_map_pad_0_1 = zeroPad(feature_map1)
feature_pool_1 = max_pool(feature_map_pad_0_1)
feature_map_pad_0_2 = zeroPad(feature_map2)
feature_pool_2 = max_pool(feature_map_pad_0_2)
feature_map_pad_0_3 = zeroPad(feature_map3)
feature_pool_3 = max_pool(feature_map_pad_0_3)
 
print(feature_pool_1.size())
print(feature_pool_1 / 9)
print(feature_pool_2 / 9)
print(feature_pool_3 / 9)
img = feature_pool_1.data.squeeze().numpy()  # 将输出转换为图片的格式
plt.imshow(img, cmap='gray')
plt.title('卷积池化后的特征图1')
plt.show()
 
print("--------------- 激活  ---------------")
activation_function = nn.ReLU()
 
feature_relu1 = activation_function(feature_map1)
feature_relu2 = activation_function(feature_map2)
feature_relu3 = activation_function(feature_map3)
print(feature_relu1 / 9)
print(feature_relu2 / 9)
print(feature_relu3 / 9)
img = feature_relu1.data.squeeze().numpy()  # 将输出转换为图片的格式
plt.imshow(img, cmap='gray')
plt.title('卷积 + relu 后的特征图1')
plt.show()
 </code></pre> 
<p>输出</p> 
<blockquote> 
 <p> torch.Size([1, 1, 9, 9])<br> tensor([[[[-1., -1., -1., -1., -1., -1., -1., -1., -1.],   <br>           [-1.,  1., -1., -1., -1., -1., -1.,  1., -1.],   <br>           [-1., -1.,  1., -1., -1., -1.,  1., -1., -1.],   <br>           [-1., -1., -1.,  1., -1.,  1., -1., -1., -1.],   <br>           [-1., -1., -1., -1.,  1., -1., -1., -1., -1.],   <br>           [-1., -1., -1.,  1., -1.,  1., -1., -1., -1.],   <br>           [-1., -1.,  1., -1., -1., -1.,  1., -1., -1.],   <br>           [-1.,  1., -1., -1., -1., -1., -1.,  1., -1.],   <br>           [-1., -1., -1., -1., -1., -1., -1., -1., -1.]]]])<br> --------------- 卷积  ---------------<br> tensor([[[[ 0.7492, -0.1396,  0.0826,  0.3048,  0.5270, -0.1396,  0.3048],<br>           [-0.1396,  0.9715, -0.1396,  0.3048, -0.1396,  0.0826, -0.1396],<br>           [ 0.0826, -0.1396,  0.9715, -0.3619,  0.0826, -0.1396,  0.5270],<br>           [ 0.3048,  0.3048, -0.3619,  0.5270, -0.3619,  0.3048,  0.3048],<br>           [ 0.5270, -0.1396,  0.0826, -0.3619,  0.9715, -0.1396,  0.0826],<br>           [-0.1396,  0.0826, -0.1396,  0.3048, -0.1396,  0.9715, -0.1396],<br>           [ 0.3048, -0.1396,  0.5270,  0.3048,  0.0826, -0.1396,  0.7492]]]],<br>        grad_fn=&lt;DivBackward0&gt;)<br> tensor([[[[ 0.3337, -0.5552,  0.1115, -0.1108,  0.1115, -0.5552,  0.3337],<br>           [-0.5552,  0.5559, -0.5552,  0.3337, -0.5552,  0.5559, -0.5552],<br>           [ 0.1115, -0.5552,  0.5559, -0.7774,  0.5559, -0.5552,  0.1115],<br>           [-0.1108,  0.3337, -0.7774,  1.0004, -0.7774,  0.3337, -0.1108],<br>           [ 0.1115, -0.5552,  0.5559, -0.7774,  0.5559, -0.5552,  0.1115],<br>           [-0.5552,  0.5559, -0.5552,  0.3337, -0.5552,  0.5559, -0.5552],<br>           [ 0.3337, -0.5552,  0.1115, -0.1108,  0.1115, -0.5552,  0.3337]]]],<br>        grad_fn=&lt;DivBackward0&gt;)<br> tensor([[[[ 0.3622, -0.0822,  0.5844,  0.3622,  0.1400, -0.0822,  0.8067],<br>           [-0.0822,  0.1400, -0.0822,  0.3622, -0.0822,  1.0289, -0.0822],<br>           [ 0.5844, -0.0822,  0.1400, -0.3044,  1.0289, -0.0822,  0.1400],<br>           [ 0.3622,  0.3622, -0.3044,  0.5844, -0.3044,  0.3622,  0.3622],<br>           [ 0.1400, -0.0822,  1.0289, -0.3044,  0.1400, -0.0822,  0.5844],<br>           [-0.0822,  1.0289, -0.0822,  0.3622, -0.0822,  0.1400, -0.0822],<br>           [ 0.8067, -0.0822,  0.1400,  0.3622,  0.5844, -0.0822,  0.3622]]]],<br>        grad_fn=&lt;DivBackward0&gt;)<br> --------------- 池化  ---------------<br> torch.Size([1, 1, 4, 4])<br> tensor([[[[0.9715, 0.3048, 0.5270, 0.3048],<br>           [0.3048, 0.9715, 0.3048, 0.5270],<br>           [0.5270, 0.3048, 0.9715, 0.0826],<br>           [0.3048, 0.5270, 0.0826, 0.7492]]]], grad_fn=&lt;DivBackward0&gt;)  <br> tensor([[[[0.5559, 0.3337, 0.5559, 0.3337],<br>           [0.3337, 1.0004, 0.5559, 0.1115],<br>           [0.5559, 0.5559, 0.5559, 0.1115],<br>           [0.3337, 0.1115, 0.1115, 0.3337]]]], grad_fn=&lt;DivBackward0&gt;)  <br> tensor([[[[0.3622, 0.5844, 1.0289, 0.8067],<br>           [0.5844, 0.5844, 1.0289, 0.3622],<br>           [1.0289, 1.0289, 0.1400, 0.5844],<br>           [0.8067, 0.3622, 0.5844, 0.3622]]]], grad_fn=&lt;DivBackward0&gt;)  <br> --------------- 激活  ---------------<br> tensor([[[[0.7492, 0.0000, 0.0826, 0.3048, 0.5270, 0.0000, 0.3048],     <br>           [0.0000, 0.9715, 0.0000, 0.3048, 0.0000, 0.0826, 0.0000],     <br>           [0.0826, 0.0000, 0.9715, 0.0000, 0.0826, 0.0000, 0.5270],     <br>           [0.3048, 0.3048, 0.0000, 0.5270, 0.0000, 0.3048, 0.3048],     <br>           [0.5270, 0.0000, 0.0826, 0.0000, 0.9715, 0.0000, 0.0826],     <br>           [0.0000, 0.0826, 0.0000, 0.3048, 0.0000, 0.9715, 0.0000],     <br>           [0.3048, 0.0000, 0.5270, 0.3048, 0.0826, 0.0000, 0.7492]]]],  <br>        grad_fn=&lt;DivBackward0&gt;)<br> tensor([[[[0.3337, 0.0000, 0.1115, 0.0000, 0.1115, 0.0000, 0.3337],     <br>           [0.0000, 0.5559, 0.0000, 0.3337, 0.0000, 0.5559, 0.0000],     <br>           [0.1115, 0.0000, 0.5559, 0.0000, 0.5559, 0.0000, 0.1115],     <br>           [0.0000, 0.3337, 0.0000, 1.0004, 0.0000, 0.3337, 0.0000],     <br>           [0.1115, 0.0000, 0.5559, 0.0000, 0.5559, 0.0000, 0.1115],     <br>           [0.0000, 0.5559, 0.0000, 0.3337, 0.0000, 0.5559, 0.0000],     <br>           [0.3337, 0.0000, 0.1115, 0.0000, 0.1115, 0.0000, 0.3337]]]],  <br>        grad_fn=&lt;DivBackward0&gt;)<br> tensor([[[[0.3622, 0.0000, 0.5844, 0.3622, 0.1400, 0.0000, 0.8067],<br>           [0.0000, 0.1400, 0.0000, 0.3622, 0.0000, 1.0289, 0.0000],     <br>           [0.5844, 0.0000, 0.1400, 0.0000, 1.0289, 0.0000, 0.1400],     <br>           [0.3622, 0.3622, 0.0000, 0.5844, 0.0000, 0.3622, 0.3622],     <br>           [0.1400, 0.0000, 1.0289, 0.0000, 0.1400, 0.0000, 0.5844],     <br>           [0.0000, 1.0289, 0.0000, 0.3622, 0.0000, 0.1400, 0.0000],     <br>           [0.8067, 0.0000, 0.1400, 0.3622, 0.5844, 0.0000, 0.3622]]]],  <br>        grad_fn=&lt;DivBackward0&gt;)</p> 
</blockquote> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/a4/c4/pESsqLuj_o.png" width="1200"> </p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/02e146d5a594d54fff36bd6fd49e17fa/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">scatter_ 做 one-hot的一些要注意的点</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/242b41481993407f6a763cfb43981f3b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python https请求报错：SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED]</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>