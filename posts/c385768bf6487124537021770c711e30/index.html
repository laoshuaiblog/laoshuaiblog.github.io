<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习（五）logistic回归 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/c385768bf6487124537021770c711e30/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="机器学习（五）logistic回归">
  <meta property="og:description" content="目录
1.Logistic回归概述
1.1 Sigmoid函数
1.2 基于最优化方法的最佳回归系数确定
1.2.1 极大似然估计
1.2.2 梯度上升法
1.2.3 梯度下降算法
2.Logistic实例分析
2.1准备数据
2.2使用梯度上升算法进行分类
3.实验总结 Logistics回归模型通常被用于处理二分类问题，它是一种用于分析各个影响因素（x1,x2,...xn）与分类结果y之间关系的有监督学习方法。虽然它的名字是“回归”，但实际却是一种分类学习方法。这里的“回归”一词源于最佳拟合，表示要找到最佳拟合参数集，因此，logistic训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化方法。
1.Logistic回归概述 我们都知道使用线性模型可以进行回归学习，但若要做的是分类任务改如何处理？只需要找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。
考虑二分类任务，其输出标记y的取值为0和1，而线性回归模型产生的预测值是实值，于是需将实值z转换为0/1值。通过Sigmoid函数引入非线性因素，可以实现实值z转换为0/1值，处理二分类问题。
1.1 Sigmoid函数 首先我们介绍一下Sigmoid函数，也称为逻辑函数：
其函数曲线如下：
从上图可以看出sigmoid函数是一个s形的曲线，它的取值是[0,1]之间，在远离0的地方函数的值会很快接近0或者1。它的这个特性对于解决二分类问题十分重要。
逻辑回归的假设函数形式如下：
所以：
其中x是我们的输入，w和b为我们要求取的参数
若将y视为样本x作为正例的可能性，则1-y是其反例可能性，两者的比值为称为“几率”，反映了x作为正例的相对可能性
样本作为正例的相对可能性的对数
因此有： 上面两个式子分别表示y=1和y=0的概率。上述过程我们通过sigmoid函数将z值映射到0到1之间，获得数值之后就可以进行分类。如定义阈值为0.5，y*为分类结果，则，实际应用时特定的情况可以选择不同的阈值。接下来我们要解决的问题就是获得最佳回归系数，即求解w和b得值。
1.2 基于最优化方法的最佳回归系数确定 1.2.1 极大似然估计 极大似然估计的方法步骤：
确定待求解的未知参数，如均值、方差或特定分布函数的参数等；计算每个样本的概率密度为；根据样本的概率密度累乘构造似然函数：通过似然函数最大化（求导为0），求解未知参数θ，为了降低计算难度，可采用对数加法替换概率乘法，通过导数为0/极大值来求解未知参数。 我们可通过“极大似然法”来估计w和b，给定数据集,最大化样本属于其真实标记的概率，等同于最大化对数似然函数：
为便于讨论，令则可简化为
令,则上述式中的似然项可重写为 根据sigmoid函数，似然函数可重写为：
最大化对数似然函数等价于最小化,是关于的高阶可导连续凸函数，经典的数值优化算法如梯度下降法可求得其最优解
1.2.2 梯度上升法 梯度上升法基本的思想是：要找到某函数的 最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为∇，则函数f(x,y)的梯度由 下式表示：
这个梯度意味着要沿x的方向移动 ，沿y的方向移动 。其中，函数f(x,y) 必须要在待计算的点上有定义并且可微。如下图：
梯度上升算法到达每个点后都会重新估计移动的方向。从P0开始，计算完该点的梯度，函数就根据梯度移动到下一点P1。在P1点，梯度再次被重新计算，并沿新的梯度方向移动到P2。如此循环迭代，直到满足停止条件。迭代的过程中，梯度算子总是保证我们能选取到最佳的移动方向。我们知道了移动的反向，那移动量的大小是多少。该量值称为步长，记做α。用向量来表示的话，梯度上升算法的迭代公式如下：
该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或算法达到某个可以允许的误差范围。
1.2.3 梯度下降算法 梯度下降算法，它与上述的梯度上升算法是一样的，只是公式中的加法需要变成减法。因此，对应的公式可以写成：
梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。
2.Logistic实例分析 利用Logistic回归模型根据身高、体重和肺活量预测性别
2.1准备数据 数据部分截图：
2.2使用梯度上升算法进行分类 sigmoid函数：
# sigmoid函数 def sigmoid(inX): return 1.">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-12-05T00:28:46+08:00">
    <meta property="article:modified_time" content="2022-12-05T00:28:46+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习（五）logistic回归</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1.Logistic%E5%9B%9E%E5%BD%92%E6%A6%82%E8%BF%B0-toc" style="margin-left:0px;"><a href="#1.Logistic%E5%9B%9E%E5%BD%92%E6%A6%82%E8%BF%B0" rel="nofollow">1.Logistic回归概述</a></p> 
<p id="1.1%20Sigmoid%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#1.1%20Sigmoid%E5%87%BD%E6%95%B0" rel="nofollow">1.1 Sigmoid函数</a></p> 
<p id="1.2%C2%A0%E5%9F%BA%E4%BA%8E%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%9A%84%E6%9C%80%E4%BD%B3%E5%9B%9E%E5%BD%92%E7%B3%BB%E6%95%B0%E7%A1%AE%E5%AE%9A-toc" style="margin-left:40px;"><a href="#1.2%C2%A0%E5%9F%BA%E4%BA%8E%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%9A%84%E6%9C%80%E4%BD%B3%E5%9B%9E%E5%BD%92%E7%B3%BB%E6%95%B0%E7%A1%AE%E5%AE%9A" rel="nofollow">1.2 基于最优化方法的最佳回归系数确定</a></p> 
<p id="1.2.1%20%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-toc" style="margin-left:80px;"><a href="#1.2.1%20%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" rel="nofollow">1.2.1 极大似然估计</a></p> 
<p id="1.2.2%20%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E6%B3%95-toc" style="margin-left:80px;"><a href="#1.2.2%20%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E6%B3%95" rel="nofollow">1.2.2 梯度上升法</a></p> 
<p id="1.2.3%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95-toc" style="margin-left:80px;"><a href="#1.2.3%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95" rel="nofollow">1.2.3 梯度下降算法</a></p> 
<p id="%C2%A02.Logistic%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90-toc" style="margin-left:0px;"><a href="#%C2%A02.Logistic%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90" rel="nofollow"> 2.Logistic实例分析</a></p> 
<p id="2.1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE-toc" style="margin-left:40px;"><a href="#2.1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE" rel="nofollow">2.1准备数据</a></p> 
<p id="2.2%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E7%AE%97%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB-toc" style="margin-left:40px;"><a href="#2.2%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E7%AE%97%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB" rel="nofollow">2.2使用梯度上升算法进行分类</a></p> 
<p id="3.%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93%C2%A0-toc" style="margin-left:0px;"><a href="#3.%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93%C2%A0" rel="nofollow">3.实验总结 </a></p> 
<p>Logistics回归模型通常被用于处理二分类问题，它是一种用于分析各个影响因素（x1,x2,...xn）与分类结果y之间关系的有监督学习方法。虽然它的名字是“回归”，但实际却是一种分类学习方法。这里的“回归”一词源于最佳拟合，表示要找到最佳拟合参数集，因此，logistic训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化方法。</p> 
<h2 id="1.Logistic%E5%9B%9E%E5%BD%92%E6%A6%82%E8%BF%B0">1.Logistic回归概述</h2> 
<p>我们都知道使用线性模型可以进行回归学习，但若要做的是分类任务改如何处理？只需要找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。</p> 
<p>考虑二分类任务，其输出标记y的取值为0和1，而线性回归模型产生的预测值<img alt="z = w^Tx+b" class="mathcode" src="https://images2.imgbox.com/21/ed/JAE1H9nA_o.png">是实值，于是需将实值z转换为0/1值。通过Sigmoid函数引入非线性因素，可以实现实值z转换为0/1值，处理二分类问题。</p> 
<h3 id="1.1%20Sigmoid%E5%87%BD%E6%95%B0">1.1 Sigmoid函数</h3> 
<p>首先我们介绍一下Sigmoid函数，也称为逻辑函数：</p> 
<p>                <img alt="g(z) = \frac{1}{1+e^{-z}}" class="mathcode" src="https://images2.imgbox.com/bb/d7/tyLxpqi7_o.png"></p> 
<p>其函数曲线如下：</p> 
<p>                <img alt="" height="258" src="https://images2.imgbox.com/d1/8f/cORqEx4A_o.png" width="358"></p> 
<p>从上图可以看出sigmoid函数是一个s形的曲线，它的取值是[0,1]之间，在远离0的地方函数的值会很快接近0或者1。它的这个特性对于解决二分类问题十分重要。</p> 
<p>逻辑回归的假设函数形式如下：</p> 
<p>        <img alt="y = g(w^Tx + b)" class="mathcode" src="https://images2.imgbox.com/04/61/r9IJWIIh_o.png">           <img alt="g(z) = \frac{1}{1+e^{-z}}" class="mathcode" src="https://images2.imgbox.com/07/f2/VlHhLtHo_o.png"></p> 
<p>所以：</p> 
<p>                        <img alt="" height="52" src="https://images2.imgbox.com/49/26/NWduJvnz_o.png" width="319"></p> 
<p>其中x是我们的输入，w和b为我们要求取的参数</p> 
<p>若将y视为样本x作为正例的可能性，则1-y是其反例可能性，两者的比值为<img alt="\frac{y}{1-y}" class="mathcode" src="https://images2.imgbox.com/7c/83/Xnl5fYq5_o.png">称为“几率”，反映了x作为正例的相对可能性</p> 
<p>样本作为正例的相对可能性的对数</p> 
<p>               <img alt="" height="53" src="https://images2.imgbox.com/a4/6e/xucXS3hX_o.png" width="95"> <img alt="" height="55" src="https://images2.imgbox.com/d8/04/0cQVe9lU_o.png" width="296"></p> 
<p>因此有： </p> 
<p>                                       <img alt="" height="137" src="https://images2.imgbox.com/a6/cd/gGAzqLFo_o.png" width="290"></p> 
<p> 上面两个式子分别表示y=1和y=0的概率。上述过程我们通过sigmoid函数将z值映射到0到1之间，获得数值之后就可以进行分类。如定义阈值为0.5，y*为分类结果，则<img alt="y^* = 1,if P(y=1|x)&gt;0.5,y^* = 0,if P(y=1|x)\leq 0.5" class="mathcode" src="https://images2.imgbox.com/51/fd/dOqOfetB_o.png">，实际应用时特定的情况可以选择不同的阈值。接下来我们要解决的问题就是获得最佳回归系数，即求解w和b得值。</p> 
<h3 id="1.2%C2%A0%E5%9F%BA%E4%BA%8E%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%9A%84%E6%9C%80%E4%BD%B3%E5%9B%9E%E5%BD%92%E7%B3%BB%E6%95%B0%E7%A1%AE%E5%AE%9A">1.2 基于最优化方法的最佳回归系数确定</h3> 
<h4 id="1.2.1%20%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1">1.2.1 极大似然估计</h4> 
<blockquote> 
 <p>极大似然估计的方法步骤：</p> 
 <ul><li>确定待求解的未知参数<img alt="" height="15" src="https://images2.imgbox.com/13/00/pnX2pel4_o.png" width="81">，如均值、方差或特定分布函数的参数等；</li><li>计算每个样本<img alt="" height="18" src="https://images2.imgbox.com/db/91/0ti9ol5K_o.png" width="93">的概率密度为<img alt="" height="18" src="https://images2.imgbox.com/da/4d/pxwgxeQO_o.png" width="109">；</li><li>根据样本的概率密度累乘构造似然函数：<img alt="" height="20" src="https://images2.imgbox.com/68/1a/oBTCzBiP_o.png" width="224"></li><li>通过似然函数最大化（求导为0），求解未知参数θ，为了降低计算难度，可采用对数加法替换概率乘法，通过导数为0/极大值来求解未知参数。</li></ul> 
</blockquote> 
<p>我们可通过“极大似然法”来估计w和b，给定数据集<img alt="\left \{ \left ( x_i,y_i \right )\right \}_{i=1}^{m}" class="mathcode" src="https://images2.imgbox.com/be/31/vJtzV0dI_o.png">,最大化样本属于其真实标记的概率，等同于最大化对数似然函数：</p> 
<p>                                        <img alt="" height="67" src="https://images2.imgbox.com/4b/22/kA863eva_o.png" width="249"></p> 
<p>为便于讨论，令<img alt="\beta = (w;b),\hat{x} = (x;1)," class="mathcode" src="https://images2.imgbox.com/34/6a/f0lO16um_o.png">则<img alt="w^Tx+b" class="mathcode" src="https://images2.imgbox.com/5c/31/aAMh5bVw_o.png">可简化为<img alt="\beta ^T\hat{x}" class="mathcode" src="https://images2.imgbox.com/49/77/wzQgwJ0n_o.png"></p> 
<p>令<img alt="p_1(\hat{x};\beta ) = p(y=1|\hat{x};\beta),p_0(\hat{x};\beta ) = p(y=0|\hat{x};\beta) = 1-p_1(\hat{x};\beta )" class="mathcode" src="https://images2.imgbox.com/26/5b/yAb7PdvK_o.png">,则上述式中的似然项可重写为    </p> 
<p>                       <img alt="p(y_i|x_i;w,b) = y_ip_1(\hat{x};\beta ) +(1-y_i)p_0(\hat{x};\beta )" class="mathcode" src="https://images2.imgbox.com/82/32/RG9rkzH2_o.png"></p> 
<p>根据sigmoid函数，似然函数可重写为：</p> 
<p>                             <img alt="" height="54" src="https://images2.imgbox.com/10/f3/FZ7Qru7Z_o.png" width="286"></p> 
<p>最大化对数似然函数等价于最小化<img alt="l(\beta )" class="mathcode" src="https://images2.imgbox.com/de/2e/dGIiV3xW_o.png">,<img alt="l(\beta )" class="mathcode" src="https://images2.imgbox.com/a5/9a/9Ao97O83_o.png">是关于<img alt="\beta" class="mathcode" src="https://images2.imgbox.com/89/91/4LM9Npwl_o.png">的高阶可导连续凸函数，<span style="color:#000000;">经典的数值优化算法如梯度下降法可求得其最优解</span></p> 
<h4 id="1.2.2%20%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E6%B3%95"><span style="color:#000000;">1.2.2 梯度上升法</span></h4> 
<p> 梯度上升法基本的思想是：要找到某函数的 最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为∇，则函数f(x,y)的梯度由 下式表示：</p> 
<p>                                <img alt="" height="91" src="https://images2.imgbox.com/a3/c5/sCRYFfZR_o.png" width="168"></p> 
<p>这个梯度意味着要沿x的方向移动 <img alt="" height="24" src="https://images2.imgbox.com/ab/55/TmnGp1fr_o.png" width="37">，沿y的方向移动<img alt="" height="29" src="https://images2.imgbox.com/58/81/nRbQcdc3_o.png" width="40"> 。其中，函数f(x,y) 必须要在待计算的点上有定义并且可微。如下图：</p> 
<p>                        <img alt="" height="294" src="https://images2.imgbox.com/e0/36/6zOwVHDV_o.png" width="367"></p> 
<p> 梯度上升算法到达每个点后都会重新估计移动的方向。从P0开始，计算完该点的梯度，函数就根据梯度移动到下一点P1。在P1点，梯度再次被重新计算，并沿新的梯度方向移动到P2。如此循环迭代，直到满足停止条件。迭代的过程中，梯度算子总是保证我们能选取到最佳的移动方向。我们知道了移动的反向，那移动量的大小是多少。该量值称为步长，记做α。用向量来表示的话，梯度上升算法的迭代公式如下：</p> 
<p>                                <img alt="" src="https://images2.imgbox.com/3b/55/XVAxelD7_o.png"></p> 
<p>该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或算法达到某个可以允许的误差范围。</p> 
<h4 id="1.2.3%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95">1.2.3 梯度下降算法</h4> 
<p>梯度下降算法，它与上述的梯度上升算法是一样的，只是公式中的加法需要变成减法。因此，对应的公式可以写成：</p> 
<p>                                <img alt="" src="https://images2.imgbox.com/2d/2d/txU5dJgs_o.png"></p> 
<p>梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。</p> 
<h2 id="%C2%A02.Logistic%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90"> 2.Logistic实例分析</h2> 
<p>利用Logistic回归模型根据身高、体重和肺活量预测性别</p> 
<h3 id="2.1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE">2.1准备数据</h3> 
<p>数据部分截图：</p> 
<p>                <img alt="" height="441" src="https://images2.imgbox.com/19/53/BRr2X4W8_o.png" width="466"></p> 
<h3 id="2.2%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E7%AE%97%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB">2.2使用梯度上升算法进行分类</h3> 
<p><img alt="" src="https://images2.imgbox.com/6c/c5/3hoMxeoG_o.png"></p> 
<p>sigmoid函数：</p> 
<pre><code class="language-python"># sigmoid函数
def sigmoid(inX):
    return 1.0 / (1 + exp(-inX))</code></pre> 
<p>梯度上升算法：</p> 
<pre><code class="language-python"># 梯度上升算法
def gradAscent(dataMatIn, classLabels):                            # dataMatIn数据集、classLabels数据标签
    dataMatrix = mat(dataMatIn)                                    # 转换为NumPy矩阵
    labelMat = mat(classLabels).transpose()                        # 转换为NumPy矩阵，并且矩阵转置
    m, n = shape(dataMatrix)                                       # 获取数据集矩阵的大小，m为行数，n为列数
    alpha = 0.001                                                  # 目标移动的步长
    maxCycles = 500                                                # 迭代次数
    weights = ones((n, 1))                                         # 权重初始化为1
    for k in range(maxCycles):                                     # 重复矩阵运算
        h = sigmoid(dataMatrix * weights)                          # 矩阵相乘，计算sigmoid函数
        error = (labelMat - h)                                     # 计算误差
        weights = weights + alpha * dataMatrix.transpose() * error # 矩阵相乘，更新权重
    return weights</code></pre> 
<p>使用Logistic 回归方法进行分类并不需要做很多工作，所需做的只是把测试集上每个特征向量乘以最优化方法得来的回归系数，再将该乘积结果求和，最后输入到Sigmoid函数中即可。如果对应的Sigmoid值大于0.5就预测类别标签为1，否则为0。 </p> 
<pre><code class="language-python"># 分类函数
def classifyVector(inX, weights):
    prob = sigmoid(sum(inX * weights))   # 计算sigmoid值
    if prob &gt; 0.5:                       # 概率大于0.5，返回分类结果1.0
        return 1.0
    else:                                # 概率小于等于0.5，返回分类结果0.0
        return 0.0</code></pre> 
<pre><code class="language-python">def colicTest1():
    # 读取测试集和训练集,并对数据进行格式化处理
    frTrain = open("D:\syy\MachineLearning\data\dataTrain.txt")     # 读取训练集文件
    frTest = open('D:\syy\MachineLearning\data\dataTest.txt')           # 读取测试集文件
    trainingSet = []                              # 创建数据列表
    trainingLabels = []                           # 创建标签列表
    for line in frTrain.readlines():              # 按行读取
        currLine = line.strip().split('\t')       # 分隔
        lineArr = []
        for i in range(3):
            lineArr.append(float(currLine[i]))
        trainingSet.append(lineArr)
        trainingLabels.append(float(currLine[3]))
 
    # 使用改进的随即上升梯度训练
    trainWeights =  gradAscent(array(trainingSet), trainingLabels)
    errorCount = 0                                # 错误数
    numTestVec = 0.0
    for line in frTest.readlines():               # 遍历每行数据
        numTestVec += 1.0                         # 测试集数量加1
        currLine = line.strip().split('\t')
        lineArr = []
        for i in range(3):
            lineArr.append(float(currLine[i]))
        if int(classifyVector(array(lineArr), trainWeights)) != int(currLine[3]):
            errorCount += 1                        # 预测结果与真值不一致，错误数加1
    errorRate = (float(errorCount) / numTestVec)   # 计算错误率
    print("测试的错误率为: %f" % errorRate)
    return errorRate</code></pre> 
<pre><code class="language-python"># 求结果的平均值
def multiTest():
    numTests = 10
    errorSum = 0.0
    for k in range(numTests):
        errorSum += colicTest1()
    print("在 %d 迭代之后， 平均错误率为: %f" % (numTests, errorSum / float(numTests)))</code></pre> 
<p>运行结果如图：</p> 
<p><img alt="" height="197" src="https://images2.imgbox.com/da/9e/mLzkLUtG_o.png" width="603"></p> 
<p>从上述实验结果来看，使用逻辑回归进行性别分类的错误率较高</p> 
<h2 id="3.%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93%C2%A0">3.实验总结 </h2> 
<ul><li>逻辑回归的优点：直接对分类的可能性建模，无需事先假设数据分布，避免了假设分布不准确带来的问题，不仅预测出类别，还可得到近似概率预测</li><li>缺点：容易欠拟合，分类精度不高，数据特征有缺失或特征空间很大时效果不好</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/53303d57b0e4592c87fea337ca4b3ce3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【数据结构与算法】一套链表 OJ 带你轻松玩转链表</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b3561bb77007c56716ce740398e10a4b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Service层代码单元测试以及单元测试如何Mock</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>