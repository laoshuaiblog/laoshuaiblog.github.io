<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【LLMs&#43;小羊驼】23.03.Vicuna: 类似GPT4的开源聊天机器人（ 90%* ChatGPT Quality） - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/5d54f96ad7a9f29e6adfddee9278027b/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="【LLMs&#43;小羊驼】23.03.Vicuna: 类似GPT4的开源聊天机器人（ 90%* ChatGPT Quality）">
  <meta property="og:description" content="官方在线demo: https://chat.lmsys.org/
Github项目代码：https://github.com/lm-sys/FastChat
官方博客：Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality
模型下载: https://huggingface.co/lmsys/vicuna-7b-v1.5 | 所有的模型
解读：量子位科技报道 | | 知乎陈城南 || GPT的一生
相关-斯坦福羊驼模型 Alpaca: A Strong, Replicable Instruction-Following Model
文章目录 一、简介1.1 什么是Vicuna(小羊驼)? （类似GPT4的开源聊天机器人）Vicuna1.5（LLaMA2上微调的）1.1.2 性能对比 1.2 GPT相关概念 ?1.2.1 GPT的4个阶段：1.2.2 什么是token？ （字符切分的最小单位，1 token ~= 0.75 of word） 二 、本地部署 （linux服务器）本机环境：cuda12.1 &#43; 3090ti模型和项目下载下载相关模型 安装依赖启动方式1：纯命令端 启动（不推荐）方式2：gradio ui对话 (启动3个服务 server 、model、gradio) 一、简介 1.1 什么是Vicuna(小羊驼)? （类似GPT4的开源聊天机器人） Vicuna（音标 vɪˈkjuːnə ,小羊驼、骆马）
是 基于LLaMA的指令**微调**模型 （类似GPT的文本生成模型）
LLaMA: 是基础大语言模型，用大量质量一般的互联网文本数据训练，与GPT3 、PaLM类似">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-03-15T16:37:43+08:00">
    <meta property="article:modified_time" content="2024-03-15T16:37:43+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【LLMs&#43;小羊驼】23.03.Vicuna: 类似GPT4的开源聊天机器人（ 90%* ChatGPT Quality）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>官方<code>在线</code>demo: <a href="https://chat.lmsys.org/" rel="nofollow">https://chat.lmsys.org/</a><br> Github项目<code>代码</code>：<a href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a><br> <strong>官方</strong>博客：<a href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="nofollow">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality</a><br> <strong>模型</strong>下载: <a href="https://huggingface.co/lmsys/vicuna-7b-v1.5" rel="nofollow">https://huggingface.co/lmsys/vicuna-7b-v1.5</a> | <a href="https://huggingface.co/lmsys" rel="nofollow">所有的模型</a><br> <strong>解读</strong>：<a href="https://mp.weixin.qq.com/s/OWPQtFaww1vAqLoQuTxI6Q" rel="nofollow">量子位科技报道</a> | | <a href="https://zhuanlan.zhihu.com/p/639848185" rel="nofollow">知乎陈城南</a> || <a href="https://yinguobing.com/the-lifetime-of-gpt/" rel="nofollow">GPT的一生</a><br> 相关-斯坦福羊驼模型 <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" rel="nofollow">Alpaca: A Strong, Replicable Instruction-Following Model</a></p> 
<p><img src="https://images2.imgbox.com/0d/5b/kmdVIJrl_o.png" alt="在这里插入图片描述"><br> </p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_9" rel="nofollow">一、简介</a></li><li><ul><li><a href="#11_Vicuna__GPT4_11" rel="nofollow">1.1 什么是Vicuna(小羊驼)? （类似GPT4的开源聊天机器人）</a></li><li><ul><li><a href="#Vicuna15LLaMA2_20" rel="nofollow">Vicuna1.5（LLaMA2上微调的）</a></li><li><a href="#112__29" rel="nofollow">1.1.2 性能对比</a></li></ul> 
   </li><li><a href="#12_GPT__34" rel="nofollow">1.2 GPT相关概念 ?</a></li><li><ul><li><a href="#121_GPT4_36" rel="nofollow">1.2.1 GPT的4个阶段：</a></li><li><a href="#122__token_1_token__075_of_word_44" rel="nofollow">1.2.2 什么是token？ （字符切分的最小单位，1 token ~= 0.75 of word）</a></li></ul> 
  </li></ul> 
  </li><li><a href="#__linux_48" rel="nofollow">二 、本地部署 （linux服务器）</a></li><li><ul><li><a href="#cuda121__3090ti_50" rel="nofollow">本机环境：cuda12.1 + 3090ti</a></li><li><a href="#_55" rel="nofollow">模型和项目下载</a></li><li><ul><li><a href="#_60" rel="nofollow">下载相关模型</a></li></ul> 
   </li><li><a href="#_80" rel="nofollow">安装依赖</a></li><li><a href="#_88" rel="nofollow">启动</a></li><li><ul><li><a href="#1__89" rel="nofollow">方式1：纯命令端 启动（不推荐）</a></li><li><a href="#2gradio_ui__3__server_modelgradio_95" rel="nofollow">方式2：gradio ui对话 (启动3个服务 server 、model、gradio)</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_9"></a>一、简介</h2> 
<h3><a id="11_Vicuna__GPT4_11"></a>1.1 什么是Vicuna(小羊驼)? （类似GPT4的开源聊天机器人）</h3> 
<p>Vicuna（音标 vɪˈkjuːnə ,小羊驼、骆马）<br> 是 <strong>基于LLaMA</strong>的<code>指令**微调**模型</code> （类似GPT的文本生成模型）<br> <strong>LLaMA</strong>: 是基础大语言模型，用大量质量一般的互联网文本数据训练，与GPT3 、PaLM类似<br> 与<code>Stanford Alpaca</code> (ælˈpækə，又叫羊驼)的关系: 都是对LLaMa的微调，但是Vicuna数据集质量更高性能更好，参照Alpaca的训练</p> 
<p>Vicuna 用ShareGPT网站的用户分享的7w条ChatGPT对话记录，对 LLaMA进行监督质量微调训练（Supervised Finturning），性能超越了LLaMa和Stanford Alpaca，<strong>达到了与ChatGPT相似的水平。</strong><br> <img src="https://images2.imgbox.com/3e/63/XBT0YxxE_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Vicuna15LLaMA2_20"></a>Vicuna1.5（LLaMA2上微调的）</h4> 
<p><code>Vicuna1.5</code>= LLaMA2 + 125K 对话（ShareGPT.com）</p> 
<blockquote> 
 <p>Vicuna v1.5 is fine-tuned from Llama 2 with supervised instruction fine-tuning. The training data is around 125K conversations collected from ShareGPT.com. See more details in the “Training Details of Vicuna Models” section in the appendix of this paper.</p> 
</blockquote> 
<p>支持中文，但是中文数据只占LLaMA2的<strong>0.13%</strong>，有监督微调占的比例未知。<br> <img src="https://images2.imgbox.com/2f/29/wjC1Ww92_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="112__29"></a>1.1.2 性能对比</h4> 
<p>使用GPT4做裁判，设置问题，进行验证和评分</p> 
<p><img src="https://images2.imgbox.com/cf/7e/DF96lBAC_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="12_GPT__34"></a>1.2 GPT相关概念 ?</h3> 
<p>下面内容<code>来源</code>: <a href="https://karpathy.ai/stateofgpt.pdf" rel="nofollow">https://karpathy.ai/stateofgpt.pdf</a></p> 
<h4><a id="121_GPT4_36"></a>1.2.1 GPT的4个阶段：</h4> 
<p><strong>预训练（Pretraining）：</strong> 基础大语言模型，用大量质量一般的互联网文本数据<code>无监督</code>训练，典型代表是GPT3 、PaLM，LLaMA:<br> <strong>有监督的精调</strong>（<strong>SFT</strong>, Supervised Finetuning）: 人工精心设计问答<br> <strong>奖励建模</strong>（RM，Reward Modeling）<br> <strong>强化学习</strong>（RL，Reinforcement Learning）： 典型代表是chatgpt Claude.</p> 
<p><img src="https://images2.imgbox.com/5e/43/BXTTSsdT_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="122__token_1_token__075_of_word_44"></a>1.2.2 什么是token？ （字符切分的最小单位，1 token ~= 0.75 of word）</h4> 
<p>将单词切分为<br> <img src="https://images2.imgbox.com/4a/1e/p9cqB7KP_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="__linux_48"></a>二 、本地部署 （linux服务器）</h2> 
<p>参考1 ：<a href="https://juejin.cn/post/7341593721100386344" rel="nofollow">https://juejin.cn/post/7341593721100386344</a></p> 
<h3><a id="cuda121__3090ti_50"></a>本机环境：cuda12.1 + 3090ti</h3> 
<p>7B未压缩-占用约13G显存<br> <img src="https://images2.imgbox.com/73/cb/6kBqFBEb_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_55"></a>模型和项目下载</h3> 
<p>下载项目</p> 
<pre><code class="prism language-bash"><span class="token function">git</span> clone https://github.com/lm-sys/FastChat.git
</code></pre> 
<h4><a id="_60"></a>下载相关模型</h4> 
<p>按需求和显存选择模型</p> 
<blockquote> 
 <p>lmsys/vicuna-7b-v1.5<br> lmsys/vicuna-7b-v1.5-16k<br> lmsys/vicuna-13b-v1.5-16k<br> lmsys/vicuna-33b-v1.3</p> 
</blockquote> 
<p>如果下载遇到问题，令export HF_HUB_ENABLE_HF_TRANSFER=0</p> 
<pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com
pip <span class="token function">install</span> <span class="token parameter variable">-U</span> huggingface_hub
pip <span class="token function">install</span> <span class="token parameter variable">-U</span> hf-transfer
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_HUB_ENABLE_HF_TRANSFER</span><span class="token operator">=</span><span class="token number">1</span>
huggingface-cli download --resume-download lmsys/vicuna-7b-v1.5  --local-dir ./weights/vicuna-7b-v1.5
<span class="token comment"># </span>
<span class="token comment"># 或者13b</span>
huggingface-cli download --resume-download lmsys/vicuna-13b-v1.5  --local-dir ./weights/vicuna-13b-v1.5
</code></pre> 
<h3><a id="_80"></a>安装依赖</h3> 
<p>参考官网：<a href="https://github.com/lm-sys/FastChat/blob/main/pyproject.toml">https://github.com/lm-sys/FastChat/blob/main/pyproject.toml</a></p> 
<pre><code class="prism language-bash">conda create <span class="token parameter variable">-n</span> fastchat   <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.10</span> <span class="token parameter variable">-y</span>
conda activate fastchat
pip <span class="token function">install</span> <span class="token string">"fschat[model_worker,webui]"</span>
</code></pre> 
<h3><a id="_88"></a>启动</h3> 
<h4><a id="1__89"></a>方式1：纯命令端 启动（不推荐）</h4> 
<pre><code class="prism language-bash">python <span class="token parameter variable">-m</span> fastchat.serve.cli --model-path weights/vicuna-7b-v1.5
</code></pre> 
<h4><a id="2gradio_ui__3__server_modelgradio_95"></a>方式2：gradio ui对话 (启动3个服务 server 、model、gradio)</h4> 
<p>服务器ip+端口</p> 
<pre><code class="prism language-bash"><span class="token comment"># server 控制器</span>
python3 <span class="token parameter variable">-m</span> fastchat.serve.controller

<span class="token comment"># 模型相关</span>
python <span class="token parameter variable">-m</span> fastchat.serve.model_worker --model-path weights/vicuna-7b-v1.5/
<span class="token comment"># 连接测试（可不选）</span>
python3 <span class="token parameter variable">-m</span> fastchat.serve.test_message --model-name vicuna-7b-v1.5
</code></pre> 
<p><img src="https://images2.imgbox.com/0e/16/wlIEgqg2_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5d043ef8ffc53aeadf6917e67add061b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">服务器数据恢复—服务器硬盘灯显示红色的数据恢复案例</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bf5afd14aafd9f28fa066a313597b3fc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端和后端权限控制【笔记】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>