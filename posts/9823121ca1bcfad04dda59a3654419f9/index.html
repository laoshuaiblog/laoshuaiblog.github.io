<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深入理解批归一化Batch Normalization批标准化 - 老帅的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:url" content="https://laoshuaiblog.github.io/posts/9823121ca1bcfad04dda59a3654419f9/">
  <meta property="og:site_name" content="老帅的博客">
  <meta property="og:title" content="深入理解批归一化Batch Normalization批标准化">
  <meta property="og:description" content="原文地址：
【深度学习】深入理解Batch Normalization批标准化 ： https://www.cnblogs.com/guoyaohua/p/8724433.html
【深度学习】批归一化（Batch Normalization） ：【深度学习】批归一化（Batch Normalization） - Madcola - 博客园
Batch Normalization原理与实战 （ 什么是Internal Covariate Shift）：Batch Normalization原理与实战 - 知乎
#解决问题
解决内部数据分布不稳定问题
#做法
统计mini-batch个样本的期望和方差从而进行归一化，但这样会导致数据表达能力的缺失，所以要引入两个参数从而恢复数据本身的表达能力
#优势
使得网络中每层输入数据的分布相对稳定，加速模型学习速度，允许网络使用饱和性激活函数，缓解梯度消失问题，具有一定的正则化效果
0 Batch Normalization 是训练神经网络模型的一种有效方法。该方法的目标是将特征(每层激活后的输出)归一化为均值为 0，标准差为 1。所以问题在于非零的均值是如何影响模型训练的：
首先，可以理解为非零均值是指数据不围绕 0 值分布，但数据中大多数值大于零或小于零。结合高方差问题，数据变得非常大或非常小。这个问题在训练层数很多的神经网络时很常见。特征没有在稳定区间内分布(由小到大)，这将影响网络的优化过程。众所周知，优化神经网络需要使用导数计算。假设一个简单的层计算公式是 y = (Wx &#43; b)， y 对 w 的导数是:dy = dWx。因此，x 的取值直接影响导数的取值(当然，神经网络模型中梯度的概念并不是那么简单，但从理论上讲，x 会影响导数)。因此，如果 x 带来不稳定的变化，其导数可能太大，也可能太小，导致学习模型不稳定，因此不得不降低网络的学习率。相反当使用 Batch Normalization 时我们可以在训练中使用更高的学习率。
Batch Normalization 可以避免 x 值经过非线性激活函数后趋于饱和的现象。因此，它确保激活值不会过高或过低。这有助于权重的学习，当不使用时有些权重可能永远无法进行学习，而用了之后，基本上都可以学习到。这有助于我们减少对参数初始值的依赖。
Batch Normalization 也是一种正则化形式，有助于最小化过拟合。使用 Batch Normalization，我们不需要使用太多的 dropput，这是有意义的，因为我们不需要担心丢失太多的信息，当我们实际使用的时候，仍然建议结合使用这两种技术。
1、为什么使用BN 使用BN的原因是网络训练中每一层不断改变的参数会导致后续每一层输入的分布发生变化，而学习的过程又要使每一层去适应输入的分布，因此不得不降低网络的学习率，并且要小心得初始化（internal covariant shift）如果仅通过归一化方法使得数据具有零均值和单位方差，则会降低层的表达能力（如使用Sigmoid函数时，只使用线性区域），所以增加两个调节参数（scale和shift），使得网络表达能力增强BN就是在深度神经网络训练过程中使得每一层神经网络的输入保持相近的分布。改善正则化策略：作为正则化的一种形式，轻微减少了对dropout的需求另一个注意点：在arxiv六月份的preprint论文中，有一篇叫做“How Does Batch Normalization Help Optimization?">
  <meta property="og:locale" content="zh-CN">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2022-12-30T22:22:22+08:00">
    <meta property="article:modified_time" content="2022-12-30T22:22:22+08:00">

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
  


</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="老帅的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">老帅的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深入理解批归一化Batch Normalization批标准化</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#f33b45;"><strong>原文地址：</strong></span></p> 
<p><span style="color:#f33b45;"><strong>【深度学习】深入理解Batch Normalization批标准化 ： </strong></span><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" rel="nofollow" title=" https://www.cnblogs.com/guoyaohua/p/8724433.html"> https://www.cnblogs.com/guoyaohua/p/8724433.html</a></p> 
<p><strong><a href="https://www.cnblogs.com/skyfsm/p/8453498.html" rel="nofollow" id="cb_post_title_url" title="【深度学习】批归一化（Batch Normalization）">【深度学习】批归一化（Batch Normalization）</a></strong>   ：<a href="https://www.cnblogs.com/skyfsm/p/8453498.html" rel="nofollow" title="【深度学习】批归一化（Batch Normalization） - Madcola - 博客园">【深度学习】批归一化（Batch Normalization） - Madcola - 博客园</a></p> 
<p>Batch Normalization原理与实战 （ 什么是Internal Covariate Shift）：<a href="https://zhuanlan.zhihu.com/p/34879333" rel="nofollow" title="Batch Normalization原理与实战 - 知乎">Batch Normalization原理与实战 - 知乎</a></p> 
<p><strong>#解决问题</strong><br> 解决内部数据分布不稳定问题<br><strong>#做法</strong><br> 统计mini-batch个样本的期望和方差从而进行归一化，但这样会导致数据表达能力的缺失，所以要引入两个参数从而恢复数据本身的表达能力<br><strong>#优势</strong><br> 使得网络中每层输入数据的分布相对稳定，加速模型学习速度，允许网络使用饱和性激活函数，缓解梯度消失问题，具有一定的正则化效果</p> 
<h4><span style="color:#fe2c24;">0</span></h4> 
<p>Batch Normalization 是训练神经网络模型的一种有效方法。该方法的目标是将特征(每层激活后的输出)归一化为均值为 0，标准差为 1。所以问题在于非零的均值是如何影响模型训练的：</p> 
<ul><li> <p>首先，可以理解为非零均值是指数据不围绕 0 值分布，但数据中大多数值大于零或小于零。结合高方差问题，数据变得非常大或非常小。这个问题在训练层数很多的神经网络时很常见。特征没有在稳定区间内分布(由小到大)，这将影响网络的优化过程。众所周知，优化神经网络需要使用导数计算。假设一个简单的层计算公式是 y = (Wx + b)， y 对 w 的导数是:dy = dWx。因此，x 的取值直接影响导数的取值(当然，神经网络模型中梯度的概念并不是那么简单，但从理论上讲，x 会影响导数)。因此，如果 x 带来不稳定的变化，其导数可能太大，也可能太小，导致学习模型不稳定，因此不得不降低网络的学习率。相反当使用 Batch Normalization 时我们可以在训练中使用更高的学习率。</p> </li><li> <p>Batch Normalization 可以避免 x 值经过非线性激活函数后趋于饱和的现象。因此，它确保激活值不会过高或过低。这有助于权重的学习，当不使用时有些权重可能永远无法进行学习，而用了之后，基本上都可以学习到。这有助于我们减少对参数初始值的依赖。</p> </li><li> <p>Batch Normalization 也是一种正则化形式，有助于最小化过拟合。使用 Batch Normalization，我们不需要使用太多的 dropput，这是有意义的，因为我们不需要担心丢失太多的信息，当我们实际使用的时候，仍然建议结合使用这两种技术。</p> </li></ul> 
<h4><span style="color:#f33b45;"><strong>1、为什么使用BN</strong></span></h4> 
<ul><li><span style="color:#f33b45;">使用BN的原因是网络训练中每一层不断改变的参数会导致后续每一层输入的分布发生变化，而学习的过程又要使每一层去适应输入的分布，因此不得不降低网络的学习率，并且要小心得初始化（internal covariant shift）</span></li><li><span style="color:#f33b45;">如果仅通过归一化方法使得数据具有零均值和单位方差，则会降低层的表达能力（如使用Sigmoid函数时，只使用线性区域），所以</span><span style="color:#fe2c24;">增加两个调节参数（scale和shift），使得网络表达能力增强</span></li><li><span style="color:#f33b45;">BN就是在深度神经网络训练过程中使得每一层神经网络的输入保持相近的分布。</span></li><li><span style="color:#fe2c24;">改善正则化策略：作为正则化的一种形式，轻微减少了对dropout的需求</span></li><li><span style="color:#f33b45;">另一个注意点：在arxiv六月份的preprint论文中，有一篇叫做“How Does Batch Normalization Help Optimization?”的文章，里面提到BN起作用的真正原因和改变输入的分布从而产生稳定性几乎没有什么关系，真正的原因是BN使对应优化问题的landscape变得更加平稳，这就保证了更加predictive的梯度以及可以使用更加大的学习率从而使网络更快收敛，而且不止BN可以产生这种影响，许多正则化技巧都有这种类似影响</span></li></ul> 
<h4><span style="color:#f33b45;"><strong>2、BN训练和测试时的参数是一样的嘛？</strong></span></h4> 
<p>对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。</p> 
<p>但是，在测试过程中使用的均值和方差已经不是某一个batch的了，而是针对整个数据集而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录每一个Batch的均值和方差，以便训练完成之后按照下式计算整体的均值和方差</p> 
<p>在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全量训练数据的均值和方差，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量（类似于移动平均法）</p> 
<h4><span style="color:#f33b45;">3、<strong>BN训练时为什么不用全量训练集的均值和方差呢？</strong></span></h4> 
<p>因为用全量训练集的均值和方差容易过拟合，对于BN，其实就是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上能够增加模型的鲁棒性，也会在一定程度上减少过拟合。</p> 
<p>也正是因此，BN一般要求将训练集完全打乱，并用一个较大的batch值，否则，一个batch的数据无法较好得代表训练集的分布，会影响模型训练的效果。</p> 
<h4><span style="color:#f33b45;">4、BN如何在inference是加速</span></h4> 
<p><span style="color:#f33b45;">    </span>权重合并：可以融合到卷积里，因为test时，bn是一个线性模型，卷积也是线性的</p> 
<p>    因为偏置参数 b 经过 BN 层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个 β 参数作为偏置项，所以 b 这个参数就可以不用了。具体参考下面的链接</p> 
<p>    参考链接： <a href="https://zhuanlan.zhihu.com/p/48005099" rel="nofollow" title="网络inference阶段conv层和BN层的融合">网络inference阶段conv层和BN层的融合</a></p> 
<h4><span style="color:#f33b45;">5、为什么BN层一般用在线性层和卷积层后面，而不是放在非线性单元后？</span></h4> 
<p>       原文中是这样解释的，因为非线性单元的输出分布形状会在训练过程中变化，归一化无法消除他的方差偏移，相反的，全连接和卷积层的输出一般是一个对称,非稀疏的一个分布，更加类似高斯分布，对他们进行归一化会产生更加稳定的分布。其实想想也是的，像relu这样的激活函数，如果你输入的数据是一个高斯分布，经过他变换出来的数据能是一个什么形状？小于0的被抑制了，也就是分布小于0的部分直接变成0了，这样不是很高斯了。</p> 
<p><strong><span style="color:#fe2c24;">5.1 、BN放在激活函数前后有什么区别？</span></strong></p> 
<p><strong>结论：BN 层在激活函数之前。</strong></p> 
<p>BN层的作用机制是通过平滑隐藏层输入的分布，帮助随机梯度下降的进行，缓解随机梯度下降权重更新对后续层的负面影响。因此，实际上，无论是放非线性激活之前，还是之后，也许都能发挥这个作用。只不过，取决于具体激活函数的不同，效果也许有一点差别（比如，对sigmoid和tanh而言，放非线性激活之前，也许顺便还能缓解sigmoid/tanh的梯度衰减问题，而对ReLU而言，这个平滑作用经ReLU“扭曲”之后也许有所衰弱）。</p> 
<p>　　(1)、<strong>sigmoid、tanh 激活函数。</strong>容易出现<strong>梯度衰减</strong>的问题。那么，如果在tanh或sigmoid之前，进行一些normalization处理，就可以<strong>缓解梯度衰减</strong>的问题。我想这可能也是最初的BN论文选择把<strong>BN层放在非线性激活之前</strong>的原因。</p> 
<p>　　(2)、<strong>relu激活函数。</strong>relu使得负半区的卷积值被抑制，正半区的卷积值被保留。而bn的作用是使得输入值的均值为０，方差为１，<strong>也就是说假如relu之前是bn的话，会有接近一半的输入值被抑制，一半的输入值被保留。</strong>所以bn放到relu之前的好处可以这样理解：<strong>bn可以防止某一层的激活值全部都被抑制，从而防止从这一层往前传的梯度全都变成０，也就是防止梯度消失</strong>。（当然也可以防止梯度爆炸）</p> 
<h4><span style="color:#f33b45;"><strong>6、BN防止过拟合的分析</strong></span></h4> 
<p>1、使得输入规范化到有梯度的区间，防止梯度消失。</p> 
<p>2、改变输入数据的均值和方差，起到数据增强的作用，防止过拟合</p> 
<p>BN论文对BN抑制过拟合的解释：</p> 
<p>When training with Batch Normalization, a training example is seen in conjunction with other examples in the mini-batch, and the training network no longer producing deterministic values for a given training example. In our experiments, we found this effect to be advantageous to the generalization of the network. </p> 
<p>大概意思是：在训练中，BN的使用使得一个mini-batch中的所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果。</p> 
<p>        这句话什么意思呢？意思就是同样一个样本的输出不再仅仅取决于样本本身，也取决于跟这个样本属于同一个mini-batch的其它样本。同一个样本跟不同的样本组成一个mini-batch，它们的输出是不同的（仅限于训练阶段，在inference阶段是没有这种情况的）。我把这个理解成一种数据增强：同样一个样本在超平面上被拉扯，每次拉扯的方向的大小均有不同。不同于数据增强的是，这种拉扯是贯穿数据流过神经网络的整个过程的，意味着神经网络每一层的输入都被数据增强处理了。</p> 
<p><span style="color:#fe2c24;"><strong>6.1 BN 是如何影响模型训练的</strong></span></p> 
<p>Batch Normalization 是训练神经网络模型的一种有效方法。该方法的目标是将特征(每层激活后的输出)归一化为均值为 0，标准差为 1。所以问题在于非零的均值是如何影响模型训练的：</p> 
<ul><li> <p>首先，可以理解为非零均值是指数据不围绕 0 值分布，但数据中大多数值大于零或小于零。结合高方差问题，数据变得非常大或非常小。这个问题在训练层数很多的神经网络时很常见。特征没有在稳定区间内分布(由小到大)，这将影响网络的优化过程。众所周知，优化神经网络需要使用导数计算。假设一个简单的层计算公式是 y = (Wx + b)， y 对 w 的导数是:dy = dWx。因此，x 的取值直接影响导数的取值(当然，神经网络模型中梯度的概念并不是那么简单，但从理论上讲，x 会影响导数)。因此，如果 x 带来不稳定的变化，其导数可能太大，也可能太小，导致学习模型不稳定。这也意味着当使用 Batch Normalization 时我们可以在训练中使用更高的学习率。</p> </li><li> <p>Batch Normalization 可以避免 x 值经过非线性激活函数后趋于饱和的现象。因此，它确保激活值不会过高或过低。这有助于权重的学习，当不使用时有些权重可能永远无法进行学习，而用了之后，基本上都可以学习到。这有助于我们减少对参数初始值的依赖。</p> </li><li> <p>Batch Normalization 也是一种正则化形式，有助于最小化过拟合。使用 Batch Normalization，我们不需要使用太多的 dropput，这是有意义的，因为我们不需要担心丢失太多的信息，当我们实际使用的时候，仍然建议结合使用这两种技术</p> </li></ul> 
<h4><span style="color:#fe2c24;">7、BN的优点和缺陷</span></h4> 
<p>我将列举使用batch normalization的一些好处，</p> 
<ul><li>鲁棒的超参数，更快的收敛。</li><li>减轻了对参数初始化的依赖。</li><li>BN一定程度上增加了泛化能力，dropout等技术可以去掉</li></ul> 
<p><span style="color:#fe2c24;"><strong>BN的缺陷</strong></span></p> 
<p>1、batch normalization依赖于batch的大小，当batch值很小时，计算的均值和方差不稳定</p> 
<p>2、rnn，因为它是一个动态的网络结构，同一个batch中训练实例有长有短，导致每一个时间步长必须维持各自的统计量，这使得BN并不能正确的使用。在rnn中，对bn进行改进也非常的困难。不过，困难并不意味着没人做，事实上现在仍然可以使用的，不过这超出了咱们初识境的学习范围。</p> 
<p></p> 
<p>        随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。</p> 
<p><span style="color:#f33b45;"><strong>正文开始：</strong></span></p> 
<p>Batch Normalization作为最近一年来DL的重要成果，已经广泛被证明其有效性和重要性。虽然有些细节处理还解释不清其理论原因，但是实践证明好用才是真的好，别忘了DL从Hinton对深层网络做Pre-Train开始就是一个<strong>经验领先于理论分析</strong>的偏经验的一门学问。本文是对论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》的导读。</p> 
<p>　　机器学习领域有个很重要的假设：<strong>IID独立同分布假设</strong>，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？<strong>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</strong></p> 
<p>　　接下来一步一步的理解什么是BN。</p> 
<p>　　为什么深度神经网络<strong>随着网络深度加深，训练起来越困难，收敛越来越慢？</strong>这是个在DL领域很接近本质的好问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，BN本质上也是解释并从某个不同的角度来解决这个问题的。</p> 
<h3>一、“Internal Covariate Shift”问题</h3> 
<p>　　从论文名字可以看出，BN是用来解决“Internal Covariate Shift”问题的，那么首先得理解什么是“Internal Covariate Shift”？</p> 
<p>　　论文首先说明Mini-Batch SGD相对于One Example SGD的两个优势：梯度更新方向更准确；并行计算速度快；（为什么要说这些？因为BatchNorm是基于Mini-Batch SGD的，所以先夸下Mini-Batch SGD，当然也是大实话）；然后吐槽下SGD训练的缺点：超参数调起来很麻烦。（作者隐含意思是用BN就能解决很多SGD的缺点）</p> 
<p>　　接着引入<strong>covariate shift的概念</strong>：<strong>如果ML系统实例集合&lt;X,Y&gt;中的输入值X的分布老是变，这不符合IID假设</strong>，网络模型很难<strong>稳定的学规律</strong>，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是<strong>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。</strong></p> 
<p>　　然后提出了BatchNorm的基本思想：能不能<strong>让每个隐层节点的激活输入分布固定下来呢</strong>？这样就避免了“Internal Covariate Shift”问题了。</p> 
<p>　　BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓<strong>白化</strong>，<strong>就是对输入数据分布变换到0均值，单位方差的正态分布</strong>——那么神经网络会较快收敛，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，<strong>可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。</strong></p> 
<h3><strong>二、</strong>BatchNorm的本质思想</h3> 
<p>　　BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的<strong>激活输入值</strong>（就是那个x=WU+B，U是输入）<strong>随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这<strong>导致反向传播时低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>，<strong>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布</strong>，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是<strong>这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</strong></p> 
<p>　　THAT’S IT。其实一句话就是：<strong>对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。</p> 
<p>　　上面说得还是显得抽象，下面更形象地表达下这种调整到底代表什么含义。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/bc/96/5mzf24YW_o.png"></p> 
<p>  图1  几个正态分布</p> 
<p>　　假设某个隐层神经元原先的激活输入x取值符合正态分布，正态分布均值是-2，方差是0.5，对应上图中最左端的浅蓝色曲线，通过BN后转换为均值为0，方差是1的正态分布（对应上图中的深蓝色图形），意味着什么，意味着输入x的取值正态分布整体右移2（均值的变化），图形曲线更平缓了（方差增大的变化）。这个图的意思是，BN其实就是把每个隐层神经元的激活输入分布从偏离均值为0方差为1的正态分布通过平移均值压缩或者扩大曲线尖锐程度，调整为均值为0方差为1的正态分布。</p> 
<p>　　那么把激活输入x调整到这个正态分布有什么用？首先我们看下均值为0，方差为1的标准正态分布代表什么含义：</p> 
<p><img alt="" class="has" height="341" src="https://images2.imgbox.com/e8/27/9DfadQ4s_o.png" width="666"></p> 
<p></p> 
<p>图2  均值为0方差为1的标准正态分布图</p> 
<p></p> 
<p>　　这意味着在一个标准差范围内，也就是说64%的概率x其值落在[-1,1]的范围内，在两个标准差范围内，也就是说95%的概率x其值落在了[-2,2]的范围内。那么这又意味着什么？我们知道，激活值x=WU+B,U是真正的输入，x是某个神经元的激活值，假设非线性函数是sigmoid，那么看下sigmoid(x)其图形：</p> 
<p><img alt="" class="has" height="306" src="https://images2.imgbox.com/1b/f8/3NeJCh5D_o.png" width="460"></p> 
<p>图3. Sigmoid(x)</p> 
<p>及sigmoid(x)的导数为：G’=f(x)*(1-f(x))，因为f(x)=sigmoid(x)在0到1之间，所以G’在0到0.25之间，其对应的图如下：</p> 
<p><img alt="" class="has" height="374" src="https://images2.imgbox.com/c9/af/371ryfuD_o.png" width="478"></p> 
<p>图4  Sigmoid(x)导数图</p> 
<p>　　假设没有经过BN调整前x的原先正态分布均值是-6，方差是1，那么意味着95%的值落在了[-8,-4]之间，那么对应的Sigmoid（x）函数的值明显接近于0，这是典型的梯度饱和区，在这个区域里梯度变化很慢，为什么是梯度饱和区？请看下sigmoid(x)如果取值接近0或者接近于1的时候对应导数函数取值，接近于0，意味着梯度变化很小甚至消失。而假设经过BN后，均值是0，方差是1，那么意味着95%的x值落在了[-2,2]区间内，很明显这一段是sigmoid(x)函数接近于线性变换的区域，意味着x的小变化会导致非线性函数值较大的变化，也即是梯度变化较大，对应导数函数图中明显大于0的区域，就是梯度非饱和区。</p> 
<p>　　从上面几个图应该看出来BN在干什么了吧？其实就是把隐层神经元激活输入x=WU+B从变化不拘一格的正态分布通过BN操作拉回到了均值为0，方差为1的正态分布，即原始正态分布中心左移或者右移到以0为均值，拉伸或者缩减形态形成以1为方差的图形。什么意思？就是说<strong>经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。</strong></p> 
<p>　　但是很明显，看到这里，稍微了解神经网络的读者一般会提出一个疑问：如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的<strong>表达能力</strong>下降了，这也意味着深度的意义就没有了。<strong>所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)</strong>，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。当然，这是我的理解，论文作者并未明确这样说。但是很明显这里的scale和shift操作是会有争议的，因为按照论文作者论文里写的理想状态，就会又通过scale和shift操作把变换后的x调整回未变换的状态，那不是饶了一圈又绕回去原始的“Internal Covariate Shift”问题里去了吗，感觉论文作者并未能够清楚地解释scale和shift操作的理论原因。</p> 
<h3>三、训练阶段如何做BatchNorm</h3> 
<p>　　上面是对BN的抽象分析和解释，具体在Mini-Batch SGD下做BN怎么做？其实论文里面这块写得很清楚也容易理解。为了保证这篇文章完整性，这里简单说明下。</p> 
<p>　　假设对于一个深层神经网络来说，其中两层结构如下：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/ba/e6/a4fPdJ0H_o.png"></p> 
<p>  图5  DNN其中两层</p> 
<p>　　要对每个隐层神经元的激活值做BN，可以想象成每个隐层又加上了一层BN操作层，它位于X=WU+B激活值获得之后，非线性函数变换之前，其图示如下：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/13/ad/2QtunWbQ_o.png"></p> 
<p>  图6. BN操作</p> 
<p>　　对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：</p> 
<p><img alt="" class="has" height="91" src="https://images2.imgbox.com/d4/8b/UthRNZDK_o.png" width="224"></p> 
<p>　　要注意，这里t层某个神经元的x(k)不是指原始输入，就是说不是t-1层每个神经元的输出，而是t层这个神经元的线性激活x=WU+B，这里的U才是t-1层神经元的输出。变换的意思是：某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。</p> 
<p>　　上文说过经过这个<strong>变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。</strong><strong>但是这样会导致网络表达能力下降，为了防止这一点，每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作，这其实是变换的反操作：</strong></p> 
<p><img alt="" class="has" height="47" src="https://images2.imgbox.com/a7/82/5kjezp5x_o.png" width="253"></p> 
<p>　　BN其具体操作流程，如论文中描述的一样：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/9c/70/HxLyBO46_o.png"></p> 
<p>　　过程非常清楚，就是上述公式的流程化描述，这里不解释了，直接应该能看懂。</p> 
<p>反向传播过程：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/b4/29/OPV0hUUr_o.png"></p> 
<p></p> 
<h3>四、BatchNorm的推理(Inference)过程</h3> 
<p>　　BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？</p> 
<p>　　既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。</p> 
<p>　　决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量，即：</p> 
<p><img alt="" class="has" height="102" src="https://images2.imgbox.com/eb/47/YExzwLDh_o.png" width="230"></p> 
<p>　　有了均值和方差，每个隐层神经元也已经有对应训练好的Scaling参数和Shift参数，就可以在推导的时候对每个神经元的激活数据计算NB进行变换了，在推理过程中进行BN采取如下方式：</p> 
<p><img alt="" class="has" height="71" src="https://images2.imgbox.com/07/a2/GA7SGOzg_o.png" width="386"></p> 
<p>　　这个公式其实和训练时</p> 
<p><img alt="" class="has" height="44" src="https://images2.imgbox.com/2d/0c/JfFZLBx8_o.png" width="236"></p> 
<p>　　是等价的，通过简单的合并计算推导就可以得出这个结论。那么为啥要写成这个变换形式呢？我猜作者这么写的意思是：在实际运行的时候，按照这种变体形式可以减少计算量，为啥呢？因为对于每个隐层节点来说：</p> 
<p>　　　　　　　　<img alt="" class="has" height="97" src="https://images2.imgbox.com/9e/61/mIhGqJeh_o.png" width="196">　　<img alt="" class="has" height="99" src="https://images2.imgbox.com/b2/a2/GMsxXtc1_o.png" width="183"></p> 
<p>　　都是固定值，这样这两个值可以事先算好存起来，在推理的时候直接用就行了，这样比原始的公式每一步骤都现算少了除法的运算过程，乍一看也没少多少计算量，但是如果隐层节点个数多的话节省的计算量就比较多了。</p> 
<h3>五、BatchNorm的好处</h3> 
<p>　　BatchNorm为什么NB呢，关键还是效果好。<strong>①</strong><strong>不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。</strong>总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8022a75b651309fc7cadb4ed1c9fe2bc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">clickhouse中使用concat()函数无法拼接问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cd688cb99331fffdcce01768e047fdcf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C#实现十六进制与十进制相互转换以及及不同进制转换</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 老帅的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://www.w3counter.com/tracker.js?id=151347"></script>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>